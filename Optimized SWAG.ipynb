{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningSaeid/Grad/blob/main/Optimized%20SWAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import timeit\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Embedding, Dense, concatenate, Dropout, Flatten, Activation\n",
        "from keras import backend as K\n",
        "from keras.utils import get_custom_objects, to_categorical, plot_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.datasets import load_iris, load_digits, load_wine, load_diabetes, load_breast_cancer, fetch_olivetti_faces, load_linnerud\n",
        "\n"
      ],
      "metadata": {
        "id": "wzehVN0lPEpD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set random seed\n",
        "np.random.seed(110)\n",
        "# Defining new activation functions\n",
        "def define_activation_functions():\n",
        "    \"\"\"\n",
        "    Define custom activation functions and update the custom objects dictionary.\n",
        "    \"\"\"\n",
        "    # Define activation functions and their respective names\n",
        "    activation_functions = [\n",
        "        ('X_1', lambda x: K.pow(x, 1)),\n",
        "        ('X_2', lambda x: K.pow(x, 2) / 2),\n",
        "        ('X_2_', lambda x: K.pow(x, 2) / 24),\n",
        "        ('X_2__', lambda x: K.pow(x, 2) / 720),\n",
        "        ('X_2___', lambda x: K.pow(x, 2) / 40320),\n",
        "        ('X_3', lambda x: K.pow(x, 3) / 6),\n",
        "        ('X_4', lambda x: K.pow(x, 4) / 24),\n",
        "        ('X_5', lambda x: K.pow(x, 5) / 120),\n",
        "        ('X_6', lambda x: K.pow(x, 6) / 720),\n",
        "    ]\n",
        "\n",
        "    # Update the custom objects dictionary with the defined activation functions\n",
        "    for name, function in activation_functions:\n",
        "        get_custom_objects().update({name: Activation(function)})\n",
        "\n",
        "# Define and register the custom activation functions\n",
        "define_activation_functions()\n",
        "\n",
        "def create_optimized_model(input_dim,  hidden_dim , output_dim , metrics_ ):\n",
        "    \"\"\"\n",
        "    Create an optimized SWAG model with a custom architecture.\n",
        "\n",
        "    :param input_dim: int, dimension of the input data\n",
        "    :param output_dim: int, dimension of the output data\n",
        "    :param hidden_dim: int, hidden layer dimension, default is 50\n",
        "    :return: Model, a compiled Keras model\n",
        "    \"\"\"\n",
        "\n",
        "    # Define input layer\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "    # First layer with custom activations\n",
        "    layer_1_x1 = Dense(hidden_dim, activation='X_1')(input_layer)\n",
        "    layer_1_x2 = Dense(hidden_dim, activation='X_2')(input_layer)\n",
        "    concat_first_layer = concatenate([layer_1_x1, layer_1_x2])\n",
        "\n",
        "    # Second layer with custom activations\n",
        "    layer_x3_x4 = Dense(hidden_dim, activation='X_2_')(concat_first_layer)\n",
        "\n",
        "    # Third layer with custom activations\n",
        "    layer_x5_x6 = Dense(hidden_dim, activation='X_2__')(layer_x3_x4)\n",
        "\n",
        "\n",
        "\n",
        "    # Concatenate all layers\n",
        "    concat_second_layer = concatenate([layer_1_x1, layer_1_x2, concat_first_layer,\n",
        "                                       layer_x3_x4, layer_x5_x6])\n",
        "\n",
        "    # Output layer for the concatenated layers\n",
        "    output_first_layer = Dense(hidden_dim, activation='linear')(concat_second_layer)\n",
        "\n",
        "    # Final output layer\n",
        "    output_layer = Dense(output_dim, activation='linear')(output_first_layer)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model(input_layer, output_layer)\n",
        "    # Define the optimizer\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    # Compile the model\n",
        "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=metrics_)\n",
        "    # model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xID7aH81K83q"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimized_model = create_optimized_model(784,784, 10)\n",
        "\n",
        "# Load and preprocess MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = (x_train + 10) / 300.0, (x_test + 10) / 300.0\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Create and train the model\n",
        "optimized_model = create_optimized_model(784, 500, 10 , 'accuracy')\n",
        "batch_size = 100\n",
        "epochs = 2\n",
        "start = timeit.default_timer()\n",
        "history = optimized_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
        "end = timeit.default_timer()\n",
        "\n",
        "# Training time\n",
        "print(f\"Training time: {end - start} seconds\")"
      ],
      "metadata": {
        "id": "8aQhT2LoKZDH",
        "outputId": "95600ec6-9a7d-4be5-d035-83891395dae1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "600/600 [==============================] - 5s 5ms/step - loss: 0.0414 - accuracy: 0.8390 - val_loss: 0.0251 - val_accuracy: 0.9128\n",
            "Epoch 2/2\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.0207 - accuracy: 0.9360 - val_loss: 0.0206 - val_accuracy: 0.9266\n",
            "Training time: 12.301850872999239 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "y_train = to_categorical(y_train, 3)\n",
        "y_test = to_categorical(y_test, 3)\n",
        "\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "# Create and train the model\n",
        "model = create_optimized_model(num_features, 10, num_classes, 'accuracy')\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "start = timeit.default_timer()\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
        "end = timeit.default_timer()\n",
        "\n",
        "# Training time\n",
        "print(f\"Training time: {end - start} seconds\")"
      ],
      "metadata": {
        "id": "KXztQpoaiuYu",
        "outputId": "eacc3257-2c3d-4b74-d31d-e859db2cc3ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "12/12 [==============================] - 2s 21ms/step - loss: 0.4406 - accuracy: 0.7667 - val_loss: 0.2354 - val_accuracy: 0.8667\n",
            "Epoch 2/10\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1724 - accuracy: 0.8417 - val_loss: 0.1127 - val_accuracy: 0.8667\n",
            "Epoch 3/10\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1187 - accuracy: 0.8833 - val_loss: 0.0966 - val_accuracy: 0.8667\n",
            "Epoch 4/10\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0987 - accuracy: 0.9000 - val_loss: 0.0889 - val_accuracy: 0.9000\n",
            "Epoch 5/10\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0897 - accuracy: 0.9083 - val_loss: 0.0879 - val_accuracy: 0.9333\n",
            "Epoch 6/10\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0848 - accuracy: 0.9000 - val_loss: 0.0828 - val_accuracy: 0.9333\n",
            "Epoch 7/10\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0806 - accuracy: 0.9167 - val_loss: 0.0776 - val_accuracy: 0.9333\n",
            "Epoch 8/10\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0781 - accuracy: 0.9083 - val_loss: 0.0746 - val_accuracy: 0.9333\n",
            "Epoch 9/10\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0753 - accuracy: 0.9167 - val_loss: 0.0704 - val_accuracy: 0.9667\n",
            "Epoch 10/10\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0723 - accuracy: 0.9167 - val_loss: 0.0687 - val_accuracy: 0.9667\n",
            "Training time: 3.5022670339994875 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = to_categorical(digits.target)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using MinMaxScaler to scale the data between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "print(num_features)\n",
        "\n",
        "# Create and train the model\n",
        "model = create_optimized_model(num_features, 200, num_classes, 'accuracy')\n",
        "batch_size = 10\n",
        "epochs = 5\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCEhk7EEBPb6",
        "outputId": "59a159d1-1756-4344-b04c-b2b2e407c5fb"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "Epoch 1/5\n",
            "144/144 [==============================] - 3s 4ms/step - loss: 0.0520 - accuracy: 0.8497\n",
            "Epoch 2/5\n",
            "144/144 [==============================] - 1s 4ms/step - loss: 0.0212 - accuracy: 0.9659\n",
            "Epoch 3/5\n",
            "144/144 [==============================] - 1s 4ms/step - loss: 0.0160 - accuracy: 0.9861\n",
            "Epoch 4/5\n",
            "144/144 [==============================] - 1s 4ms/step - loss: 0.0137 - accuracy: 0.9896\n",
            "Epoch 5/5\n",
            "144/144 [==============================] - 1s 4ms/step - loss: 0.0117 - accuracy: 0.9958\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9972\n",
            "Accuracy: 0.9972222447395325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = to_categorical(wine.target)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using MinMaxScaler to scale the data between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "print(num_features)\n",
        "\n",
        "# Create and train the model\n",
        "model = create_optimized_model(num_features, 20, num_classes, 'accuracy')\n",
        "batch_size = 10\n",
        "epochs = 5\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6pEsuuoBPYI",
        "outputId": "8f56b00b-4405-4224-e31e-a129d9d5c2a5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "Epoch 1/5\n",
            "15/15 [==============================] - 2s 5ms/step - loss: 0.2782 - accuracy: 0.3451\n",
            "Epoch 2/5\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1720 - accuracy: 0.7042\n",
            "Epoch 3/5\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.8873\n",
            "Epoch 4/5\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0808 - accuracy: 0.9437\n",
            "Epoch 5/5\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.9366\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0532 - accuracy: 0.9722\n",
            "Accuracy: 0.9722222089767456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using MinMaxScaler to scale the data between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "# num_classes = y_train.shape[1]\n",
        "\n",
        "print(num_features)\n",
        "\n",
        "# Create and train the model\n",
        "model = create_optimized_model(num_features, 20, 1, 'accuracy')\n",
        "batch_size = 10\n",
        "epochs = 5\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7N_ZWiPBPS2",
        "outputId": "cf210dcf-2c2d-426a-8cc7-40224affb509"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "Epoch 1/5\n",
            "46/46 [==============================] - 3s 4ms/step - loss: 0.2308 - accuracy: 0.6593\n",
            "Epoch 2/5\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.1282 - accuracy: 0.8593\n",
            "Epoch 3/5\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.8901\n",
            "Epoch 4/5\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9165\n",
            "Epoch 5/5\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.9187\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0925 - accuracy: 0.9386\n",
            "Accuracy: 0.9385964870452881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load Boston Housing data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "    path=\"boston_housing.npz\", test_split=0.2, seed=113\n",
        ")\n",
        "\n",
        "# Using MinMaxScaler to scale the data between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the model\n",
        "input_dim = X_train.shape[1]\n",
        "optimized_model = create_optimized_model(input_dim, input_dim , 1 , 'mae') # input_dim and hidden_dim\n",
        "batch_size = 5\n",
        "epochs = 30\n",
        "start = timeit.default_timer()\n",
        "history = optimized_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
        "end = timeit.default_timer()\n",
        "\n",
        "# Training time\n",
        "print(f\"Training time: {end - start} seconds\")\n"
      ],
      "metadata": {
        "id": "Bh72fFHYpQ3R",
        "outputId": "daa81cf0-9263-4fec-c3ce-1b3c4fbb5fa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "81/81 [==============================] - 3s 7ms/step - loss: 382.4527 - mae: 16.4929 - val_loss: 195.3077 - val_mae: 11.2270\n",
            "Epoch 2/30\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 134.5626 - mae: 9.0382 - val_loss: 109.9852 - val_mae: 7.8617\n",
            "Epoch 3/30\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 81.2563 - mae: 6.6544 - val_loss: 77.1532 - val_mae: 6.5770\n",
            "Epoch 4/30\n",
            "81/81 [==============================] - 1s 7ms/step - loss: 63.9724 - mae: 5.8040 - val_loss: 66.7118 - val_mae: 6.0760\n",
            "Epoch 5/30\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 56.6062 - mae: 5.4431 - val_loss: 59.9075 - val_mae: 5.8186\n",
            "Epoch 6/30\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 51.0353 - mae: 5.2252 - val_loss: 54.2574 - val_mae: 5.3945\n",
            "Epoch 7/30\n",
            "81/81 [==============================] - 1s 8ms/step - loss: 46.4667 - mae: 4.9456 - val_loss: 49.4718 - val_mae: 5.1582\n",
            "Epoch 8/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 42.3007 - mae: 4.7634 - val_loss: 45.1415 - val_mae: 4.9821\n",
            "Epoch 9/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 38.4027 - mae: 4.4952 - val_loss: 42.0777 - val_mae: 4.7683\n",
            "Epoch 10/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 35.0898 - mae: 4.3508 - val_loss: 38.5465 - val_mae: 4.5410\n",
            "Epoch 11/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 32.4957 - mae: 4.1525 - val_loss: 37.7745 - val_mae: 4.6778\n",
            "Epoch 12/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 29.4092 - mae: 3.9760 - val_loss: 33.7699 - val_mae: 4.2438\n",
            "Epoch 13/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 27.6532 - mae: 3.8015 - val_loss: 31.8112 - val_mae: 4.1074\n",
            "Epoch 14/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 25.5694 - mae: 3.7024 - val_loss: 30.1663 - val_mae: 3.9768\n",
            "Epoch 15/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 23.8896 - mae: 3.5511 - val_loss: 28.4419 - val_mae: 3.7762\n",
            "Epoch 16/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 22.2167 - mae: 3.3781 - val_loss: 27.6034 - val_mae: 3.7300\n",
            "Epoch 17/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 22.0861 - mae: 3.2973 - val_loss: 26.7212 - val_mae: 3.6095\n",
            "Epoch 18/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 20.5958 - mae: 3.1681 - val_loss: 27.1089 - val_mae: 3.6288\n",
            "Epoch 19/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 19.5138 - mae: 3.1356 - val_loss: 23.9045 - val_mae: 3.3775\n",
            "Epoch 20/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 18.6786 - mae: 2.9933 - val_loss: 23.9509 - val_mae: 3.3592\n",
            "Epoch 21/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 18.1354 - mae: 2.9961 - val_loss: 26.4982 - val_mae: 3.6294\n",
            "Epoch 22/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 18.5382 - mae: 2.9618 - val_loss: 24.9061 - val_mae: 3.4076\n",
            "Epoch 23/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 17.4349 - mae: 2.8498 - val_loss: 25.6415 - val_mae: 3.4628\n",
            "Epoch 24/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 17.5597 - mae: 2.9111 - val_loss: 26.3416 - val_mae: 3.5152\n",
            "Epoch 25/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 17.1844 - mae: 2.8882 - val_loss: 23.6023 - val_mae: 3.2758\n",
            "Epoch 26/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 17.4084 - mae: 2.9123 - val_loss: 28.6272 - val_mae: 3.7277\n",
            "Epoch 27/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 16.6995 - mae: 2.8559 - val_loss: 23.1319 - val_mae: 3.2452\n",
            "Epoch 28/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 16.2296 - mae: 2.7212 - val_loss: 27.7157 - val_mae: 3.5129\n",
            "Epoch 29/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 16.9953 - mae: 2.9379 - val_loss: 23.7344 - val_mae: 3.2843\n",
            "Epoch 30/30\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 15.8718 - mae: 2.8265 - val_loss: 23.0124 - val_mae: 3.2052\n",
            "Training time: 22.207246127999497 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features to be between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "# num_classes = y_train.shape[1]\n",
        "\n",
        "print(num_features)\n",
        "\n",
        "# Create and train the model\n",
        "model = create_optimized_model(num_features, 20, 1, 'mae')\n",
        "batch_size = 10\n",
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "KyPriNtHq-OU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7504c57-38c5-4dd1-b15f-f510ad3a7cc8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "Epoch 1/100\n",
            "36/36 [==============================] - 2s 4ms/step - loss: 29246.2324 - mae: 152.4018\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 26537.0234 - mae: 143.9060\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 17714.8398 - mae: 110.8377\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 5976.2329 - mae: 61.3968\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 5028.2324 - mae: 56.4191\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 4813.5527 - mae: 55.6264\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 4495.3188 - mae: 54.1489\n",
            "Epoch 8/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 4355.5532 - mae: 53.1489\n",
            "Epoch 9/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 4260.6431 - mae: 52.5106\n",
            "Epoch 10/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 4065.5613 - mae: 51.6973\n",
            "Epoch 11/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 4016.1802 - mae: 51.5569\n",
            "Epoch 12/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3907.1177 - mae: 50.6698\n",
            "Epoch 13/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3840.5386 - mae: 50.3465\n",
            "Epoch 14/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3751.0771 - mae: 49.8295\n",
            "Epoch 15/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3685.8606 - mae: 49.4225\n",
            "Epoch 16/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3642.8381 - mae: 49.2415\n",
            "Epoch 17/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3607.0237 - mae: 48.5414\n",
            "Epoch 18/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3542.1956 - mae: 48.6750\n",
            "Epoch 19/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3493.0857 - mae: 48.3304\n",
            "Epoch 20/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3468.0669 - mae: 48.0554\n",
            "Epoch 21/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3445.6362 - mae: 47.9115\n",
            "Epoch 22/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3380.6885 - mae: 47.4621\n",
            "Epoch 23/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 3372.0049 - mae: 47.3776\n",
            "Epoch 24/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 3320.8972 - mae: 46.8940\n",
            "Epoch 25/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3297.9348 - mae: 46.8194\n",
            "Epoch 26/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3262.0408 - mae: 46.3613\n",
            "Epoch 27/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3291.2380 - mae: 46.9470\n",
            "Epoch 28/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3229.0354 - mae: 46.3854\n",
            "Epoch 29/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3216.1956 - mae: 46.1275\n",
            "Epoch 30/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3191.3665 - mae: 45.9601\n",
            "Epoch 31/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 3182.3074 - mae: 45.9936\n",
            "Epoch 32/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 3152.0093 - mae: 45.6925\n",
            "Epoch 33/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3129.9719 - mae: 45.4664\n",
            "Epoch 34/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3140.9275 - mae: 45.5859\n",
            "Epoch 35/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3102.9243 - mae: 45.3152\n",
            "Epoch 36/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 3103.9199 - mae: 45.3012\n",
            "Epoch 37/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3113.1912 - mae: 45.1711\n",
            "Epoch 38/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 3097.9983 - mae: 45.2549\n",
            "Epoch 39/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3062.3364 - mae: 45.0081\n",
            "Epoch 40/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3058.4841 - mae: 44.8096\n",
            "Epoch 41/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3058.5085 - mae: 45.0884\n",
            "Epoch 42/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3066.3779 - mae: 44.9379\n",
            "Epoch 43/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3043.1353 - mae: 44.6711\n",
            "Epoch 44/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 3055.4578 - mae: 44.8576\n",
            "Epoch 45/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3029.7563 - mae: 44.6252\n",
            "Epoch 46/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3037.9751 - mae: 44.4934\n",
            "Epoch 47/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3005.3315 - mae: 44.3885\n",
            "Epoch 48/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2996.7695 - mae: 44.4001\n",
            "Epoch 49/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 3012.9709 - mae: 44.4436\n",
            "Epoch 50/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2991.2793 - mae: 44.3859\n",
            "Epoch 51/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2982.0493 - mae: 44.2191\n",
            "Epoch 52/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2975.2249 - mae: 44.1556\n",
            "Epoch 53/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2975.8696 - mae: 44.2951\n",
            "Epoch 54/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2972.5513 - mae: 44.1410\n",
            "Epoch 55/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2971.3672 - mae: 44.2562\n",
            "Epoch 56/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2965.4468 - mae: 44.1788\n",
            "Epoch 57/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2963.2856 - mae: 44.2128\n",
            "Epoch 58/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2960.3184 - mae: 44.0503\n",
            "Epoch 59/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2966.5435 - mae: 44.1709\n",
            "Epoch 60/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2956.7153 - mae: 44.0559\n",
            "Epoch 61/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2959.7532 - mae: 44.3136\n",
            "Epoch 62/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2966.3267 - mae: 44.1770\n",
            "Epoch 63/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2962.2559 - mae: 44.0795\n",
            "Epoch 64/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2961.7305 - mae: 44.1022\n",
            "Epoch 65/100\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 2962.3257 - mae: 44.1210\n",
            "Epoch 66/100\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 2922.7322 - mae: 43.8611\n",
            "Epoch 67/100\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2948.5339 - mae: 44.0065\n",
            "Epoch 68/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2938.7429 - mae: 43.9538\n",
            "Epoch 69/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2930.7297 - mae: 43.9697\n",
            "Epoch 70/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2929.7537 - mae: 43.8208\n",
            "Epoch 71/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2919.1682 - mae: 43.8128\n",
            "Epoch 72/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2936.1575 - mae: 44.0759\n",
            "Epoch 73/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2956.1589 - mae: 44.0366\n",
            "Epoch 74/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2912.2751 - mae: 43.7701\n",
            "Epoch 75/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2923.1970 - mae: 43.7666\n",
            "Epoch 76/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2938.5562 - mae: 43.8696\n",
            "Epoch 77/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2922.8184 - mae: 43.6455\n",
            "Epoch 78/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2913.2708 - mae: 43.8222\n",
            "Epoch 79/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2960.5649 - mae: 43.9816\n",
            "Epoch 80/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2925.5945 - mae: 43.8131\n",
            "Epoch 81/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2899.5010 - mae: 43.8327\n",
            "Epoch 82/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2913.8044 - mae: 43.7365\n",
            "Epoch 83/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2945.8245 - mae: 44.0761\n",
            "Epoch 84/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2902.4771 - mae: 43.8685\n",
            "Epoch 85/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2899.5803 - mae: 43.5354\n",
            "Epoch 86/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2910.4995 - mae: 43.8528\n",
            "Epoch 87/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2896.6648 - mae: 43.5327\n",
            "Epoch 88/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2920.5957 - mae: 43.8088\n",
            "Epoch 89/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2912.5662 - mae: 43.8900\n",
            "Epoch 90/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2897.9583 - mae: 43.7425\n",
            "Epoch 91/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2898.4502 - mae: 43.7148\n",
            "Epoch 92/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2913.7883 - mae: 43.9043\n",
            "Epoch 93/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2907.3271 - mae: 43.6706\n",
            "Epoch 94/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2885.8772 - mae: 43.4363\n",
            "Epoch 95/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2900.7136 - mae: 43.5756\n",
            "Epoch 96/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2909.2161 - mae: 43.7732\n",
            "Epoch 97/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2923.4905 - mae: 43.8965\n",
            "Epoch 98/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2902.9194 - mae: 43.6266\n",
            "Epoch 99/100\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2892.9604 - mae: 43.5353\n",
            "Epoch 100/100\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2888.3196 - mae: 43.7181\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 2965.5586 - mae: 43.6684\n",
            "Accuracy: 43.66841125488281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "faces = fetch_olivetti_faces()\n",
        "X = faces.images\n",
        "y = to_categorical(faces.target)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.reshape((-1, 64*64))\n",
        "X_test = X_test.reshape((-1, 64*64))\n",
        "\n",
        "# Use MinMaxScaler to scale the data between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = y.shape[1]\n",
        "\n",
        "print(num_classes)\n",
        "\n",
        "# Create and train the model\n",
        "model = create_optimized_model(num_features, 90, num_classes, 'accuracy')\n",
        "batch_size = 10\n",
        "epochs = 20\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-zaXKibDx7L",
        "outputId": "96ddc46b-016e-4f2a-c776-6c6a27c1f05c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 4ms/step - loss: 4.1380 - accuracy: 0.0219\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2143 - accuracy: 0.0281\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0693 - accuracy: 0.0719\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0498 - accuracy: 0.1219\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0415 - accuracy: 0.2344\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0387 - accuracy: 0.2844\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0382 - accuracy: 0.3562\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0331 - accuracy: 0.4938\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.5375\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.5969\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.6562\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0272 - accuracy: 0.7063\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.6938\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.7406\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.8188\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.8656\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.8906\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.9125\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.9250\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.9469\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0298 - accuracy: 0.6625\n",
            "Accuracy: 0.6625000238418579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "linnerud = load_linnerud()\n",
        "X = linnerud.data\n",
        "y = linnerud.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using MinMaxScaler to scale the data between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "# num_classes = y_train.shape[1]\n",
        "\n",
        "print(num_features)\n",
        "\n",
        "# Create and train the model\n",
        "model = create_optimized_model(num_features, 20, 1, 'mae')\n",
        "batch_size = 10\n",
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHA1-SM7FDDf",
        "outputId": "f9b4f81a-708b-47bf-eb1a-b33f10cbc05d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Epoch 1/100\n",
            "2/2 [==============================] - 2s 11ms/step - loss: 12104.5264 - mae: 89.2977\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 12082.5508 - mae: 89.1712\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 12063.2148 - mae: 89.0591\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 12041.7520 - mae: 88.9352\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 12019.1719 - mae: 88.8037\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11998.2402 - mae: 88.6818\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11975.5566 - mae: 88.5502\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11952.7979 - mae: 88.4178\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 11927.3809 - mae: 88.2686\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11903.5488 - mae: 88.1303\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 11876.2871 - mae: 87.9690\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11851.1211 - mae: 87.8223\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11823.0234 - mae: 87.6571\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 11792.8145 - mae: 87.4799\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11760.7246 - mae: 87.2878\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11728.3770 - mae: 87.0954\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11693.3613 - mae: 86.8881\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 11657.2197 - mae: 86.6720\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 11620.7324 - mae: 86.4538\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11580.5869 - mae: 86.2135\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 11535.0107 - mae: 85.9366\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 11490.1797 - mae: 85.6637\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11441.0918 - mae: 85.3640\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11391.5312 - mae: 85.0617\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 11340.5811 - mae: 84.7524\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11281.5684 - mae: 84.3878\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 11226.2070 - mae: 84.0462\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11160.2500 - mae: 83.6288\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11098.2764 - mae: 83.2469\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 11023.4980 - mae: 82.7706\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 10955.4355 - mae: 82.3424\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 10877.3906 - mae: 81.8335\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 10794.0430 - mae: 81.2849\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 10715.4512 - mae: 80.7800\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 10620.5137 - mae: 80.1480\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 10530.5361 - mae: 79.5503\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 10435.6777 - mae: 78.9177\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 10334.7715 - mae: 78.2288\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 10225.1445 - mae: 77.4641\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 10120.1963 - mae: 76.7389\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 9997.5137 - mae: 75.8498\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 9875.2236 - mae: 74.9424\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 9756.8936 - mae: 74.1410\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 9623.2783 - mae: 73.2221\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 9488.2285 - mae: 72.2874\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 9348.4961 - mae: 71.2459\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 9207.5762 - mae: 70.2772\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 9056.4043 - mae: 69.1785\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8908.7676 - mae: 68.1877\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 8747.6963 - mae: 67.2883\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 8597.6621 - mae: 66.3824\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 8430.7500 - mae: 65.5191\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8266.7461 - mae: 64.7376\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8112.6816 - mae: 63.9624\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 7944.3403 - mae: 63.2008\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 7787.8955 - mae: 62.3539\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 7631.2642 - mae: 61.7239\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 7477.9971 - mae: 61.1498\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 7328.4868 - mae: 60.5267\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 7208.1226 - mae: 60.2968\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 7068.1064 - mae: 60.1035\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6954.3682 - mae: 59.9745\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6873.4287 - mae: 60.0788\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6794.9854 - mae: 59.9963\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6699.8750 - mae: 59.7596\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6649.5835 - mae: 59.8998\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 6617.2427 - mae: 59.9337\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6564.9951 - mae: 59.9077\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 6547.0220 - mae: 59.9152\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 6508.7773 - mae: 59.9120\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6493.8965 - mae: 59.9323\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6464.8096 - mae: 59.9131\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6442.1953 - mae: 59.8993\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 6415.7646 - mae: 59.8293\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 6392.2593 - mae: 59.7384\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 6368.3223 - mae: 59.6517\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6342.6787 - mae: 59.5396\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6319.2603 - mae: 59.4357\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6293.4604 - mae: 59.3424\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6272.4922 - mae: 59.2373\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 6248.7666 - mae: 59.1185\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6227.4023 - mae: 59.0086\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 6204.2676 - mae: 58.9096\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6184.2676 - mae: 58.8186\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 6161.5962 - mae: 58.7166\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6143.5278 - mae: 58.6564\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6121.9795 - mae: 58.5508\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6102.8628 - mae: 58.4633\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 6083.5171 - mae: 58.3627\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 6063.4800 - mae: 58.2551\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 6044.8003 - mae: 58.1599\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 6024.6323 - mae: 58.0569\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 6004.5029 - mae: 57.9572\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 5986.4521 - mae: 57.8971\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5972.7080 - mae: 57.9056\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5950.2236 - mae: 57.8478\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5930.7910 - mae: 57.8045\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5914.7285 - mae: 57.8114\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5897.2529 - mae: 57.7988\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5881.1494 - mae: 57.8076\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 5714.3330 - mae: 57.5107\n",
            "Accuracy: 57.51069259643555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JXIlXrDnG-PT"
      },
      "execution_count": 57,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}