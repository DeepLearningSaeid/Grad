{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningSaeid/Grad/blob/main/Chapter_3_with_Co_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "18993f35214f6d02bb40d8f7b5660171a3d9384a",
        "collapsed": true,
        "id": "rraZNltGZHsM"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3YC01S2UddSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install autokeras\n",
        "!pip install keras==2.9.0\n",
        "!pip install SWAG-DNN==0.1.31\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "import autokeras as ak\n",
        "from SWAG_DNN.taylor import *\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "tUF4enr0JAzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3a265a00b0b7f1012da45001f83a0d75598ab96b",
        "id": "lcfca3UeZHsk"
      },
      "source": [
        "\n",
        "epochs=5\n",
        "batch_size=1\n",
        "\n",
        "# define 10-fold cross validation test harness\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "# kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "cvscores = []\n",
        "\n",
        "list_fun=['Ch_First','Ch_second','SWAG','Hermite','Legendre','Sin']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function trains various models using different activation functions and performs\n",
        "# 10-fold cross-validation to evaluate their performance. The function takes four arguments:\n",
        "# X (input data), Y (labels), Input_size (number of input features), and output_size\n",
        "# (number of output features). The activation functions used are stored in the activation_functions list,\n",
        "# which includes Chebyshev of the First Kind, Chebyshev of the Second Kind, SWAG, Hermite,\n",
        "# Legendre, and Sin functions.\n",
        "def train_fun(X, Y, Input_size, output_size):\n",
        "    activation_functions = ['Ch_First', 'Ch_second', 'SWAG', 'Hermite', 'Legendre', 'Sin']\n",
        "    results = list()\n",
        "    \n",
        "    # Iterate through each activation function\n",
        "    for act_function in activation_functions:\n",
        "        print('############################################################################################################################')       \n",
        "        cv_scores = []\n",
        "        print(act_function)\n",
        "        model = globals()[act_function](Input_size, Input_size, output_size)\n",
        "        print(model.summary())\n",
        "        \n",
        "        accuracies = np.zeros(shape=(11,))\n",
        "        index = 0\n",
        "\n",
        "        # Perform 10-fold cross-validation\n",
        "        for train, test in kfold.split(X, Y):\n",
        "            model.fit(X[train], Y[train], epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "            scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "            print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
        "            cv_scores.append(scores[1] * 100)\n",
        "            accuracies[index] = scores[1] * 100\n",
        "            index += 1\n",
        "\n",
        "        # Calculate the mean accuracy and append it to the accuracies array\n",
        "        accuracies[index] = np.mean(cv_scores)\n",
        "        results.append(accuracies)\n",
        "        print(\"The Mean accuracy is %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
        "\n",
        "    # AutoKeras model\n",
        "    auto_keras_accuracies = np.zeros(shape=(11,))\n",
        "    index = 0\n",
        "    cv_scores = []\n",
        "    print('AutoKeras Model (max_trials=30) Result:')\n",
        "    \n",
        "    for train, test in kfold.split(X, Y):\n",
        "        model = ak.StructuredDataClassifier(max_trials=30)\n",
        "        model.fit(X[train], Y[train], epochs=epochs, verbose=0)\n",
        "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "        print(\" accuracy : %.2f%%\" % (scores[1] * 100))\n",
        "        cv_scores.append(scores[1] * 100)\n",
        "        auto_keras_accuracies[index] = scores[1] * 100\n",
        "        index += 1\n",
        "    \n",
        "    auto_keras_accuracies[index] = np.mean(cv_scores)\n",
        "    print(np.mean(cv_scores)) \n",
        "    results.append(auto_keras_accuracies)\n",
        "    print('**************************************************************************************************')\n",
        "    \n",
        "    return results\n"
      ],
      "metadata": {
        "id": "MK7UurXBR9Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the results from the models and prints the accuracy scores for each fold\n",
        "# and the mean accuracy for each model. The input is a list of arrays (model_re) containing\n",
        "# accuracy scores for each model.\n",
        "def print_model_results(model_results):\n",
        "    # Iterate through each fold (11 in total: 10 for each fold, and 1 for the mean)\n",
        "    for j in range(11):\n",
        "        result_str = '0'\n",
        "        \n",
        "        # Iterate through each model (7 models in total)\n",
        "        for i in range(7):\n",
        "            # Format the result string with accuracy scores\n",
        "            if i > 0:\n",
        "                result_str = result_str + ' & ' + str(round(model_results[i][j], 2))\n",
        "            else:\n",
        "                result_str = str(round(model_results[i][j], 2))\n",
        "        \n",
        "        # Add the LaTeX formatting for a new row in a table\n",
        "        result_str = result_str + r\" \\\\ \"\n",
        "        \n",
        "        # Print the results\n",
        "        if j < 10:\n",
        "            print(result_str)\n",
        "        else:\n",
        "            print('************************************************')\n",
        "            print(result_str)\n",
        "            print('************************************************************************************************')\n",
        "    \n",
        "    # Print the mean accuracy for each model with its respective name\n",
        "    model_names = ['Chebyshev First kind', 'Chebyshev Second kind', 'SWAG', 'Hermite', 'Legendre', 'Sin(nX)', \"AutoKeras\"]\n",
        "    for i in range(7):\n",
        "        print(model_names[i], model_results[i])\n"
      ],
      "metadata": {
        "id": "VgXsAgyaMmwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function trains and evaluates an AutoKeras model using 10-fold cross-validation.\n",
        "# It takes two input arguments: X_ (features) and Y_ (labels) and returns an array\n",
        "# containing the accuracy scores for each fold and the mean accuracy.\n",
        "def auto_keras_model(X_, Y_):\n",
        "    # Initialize an array to store accuracy scores\n",
        "    accuracy_scores = np.zeros(shape=(10,))\n",
        "    index = 0\n",
        "    cvscores = []\n",
        "\n",
        "    # Perform 10-fold cross-validation\n",
        "    for train, test in kfold.split(X_, Y_):\n",
        "        # Create and train the AutoKeras model\n",
        "        model = ak.StructuredDataClassifier(max_trials=30)\n",
        "        model.fit(X_[train], Y_[train], epochs=epochs, verbose=0)\n",
        "        \n",
        "        # Evaluate the model and print the accuracy\n",
        "        scores = model.evaluate(X_[test], Y_[test], verbose=0)\n",
        "        print(\" accuracy : %.2f%%\" % (scores[1] * 100))\n",
        "        \n",
        "        # Store the accuracy scores\n",
        "        cvscores.append(scores[1] * 100)\n",
        "        accuracy_scores[index] = scores[1] * 100\n",
        "        index += 1\n",
        "\n",
        "    # Print the mean accuracy\n",
        "    print(np.mean(cvscores))\n",
        "    print('**************************************************************************************************')\n",
        "    \n",
        "    # Return the accuracy scores array\n",
        "    return accuracy_scores\n"
      ],
      "metadata": {
        "id": "a8x9E5z19roh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "51659646ab1c7e1c87283d427e7bab1c9ddb58a6",
        "id": "g48shk8hZHsl"
      },
      "source": [
        "#  ========================================================\n",
        "# First experiment\n",
        "##  Load data of   ionosphere data set\n",
        "\n",
        "\n",
        "This is a small dataset that you can <a href=\"https://www.kaggle.com/creepyghost/uci-ionosphere/version/1\">download from the kaggle </a>.\n",
        "\n",
        "\n",
        "### About this file:\n",
        "\n",
        "The Ionosphere dataset is a widely-used benchmark dataset in machine learning and pattern recognition. It was originally collected by researchers at the Johns Hopkins University Applied Physics Laboratory for the purpose of studying the ionosphere's structure and electron content. The dataset consists of radar data collected from a phased array of 16 high-frequency antennas, with a total of 34 attributes and a binary target variable.\n",
        "\n",
        "The dataset contains 351 instances, each representing a radar return from the ionosphere. The 34 attributes correspond to various features extracted from the radar signals, such as the phase and amplitude of the received signal. These features have been pre-processed and scaled to make them suitable for machine learning algorithms. The binary target variable indicates whether the radar return shows evidence of some structure in the ionosphere (labeled \"good\") or not (labeled \"bad\").\n",
        "\n",
        "Source: Donated to UCI Machine Learning Repository by\n",
        "\n",
        "Donor:\n",
        "\n",
        "Vince Sigillito (vgs '@' aplcen.apl.jhu.edu)\n",
        "\n",
        "Source:\n",
        "\n",
        "Space Physics Group Applied Physics Laboratory Johns Hopkins University Johns Hopkins Road Laurel, MD 20723"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9e664b718ffcf89c49865b7c16d56603692fe233",
        "id": "SLSQzg2dZHsl"
      },
      "source": [
        "# This code segment loads the ionosphere dataset from the provided URL and stores it as a Pandas DataFrame. It then converts the DataFrame into a NumPy array and separates the features (X) and labels (Y). The class values are encoded as integers, and the models are trained and evaluated using the train_fun function.\n",
        "\n",
        "# Load the ionosphere dataset from the provided URL and store it as a Pandas DataFrame\n",
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/ionosphere_data_kaggle.csv', header=1)\n",
        "\n",
        "# Convert the DataFrame into a NumPy array\n",
        "dataset = dataframe.values\n",
        "\n",
        "# Get the number of rows and columns\n",
        "num_rows, num_columns = dataset.shape\n",
        "\n",
        "# Separate the features (X) and labels (Y)\n",
        "X = dataset[:, 0:num_columns - 1].astype(float)\n",
        "Y = dataset[:, num_columns - 1]\n",
        "\n",
        "# Encode the class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y_encoded = encoder.transform(Y)\n",
        "\n",
        "# Train and evaluate the models using the train_fun function\n",
        "ionosphere_results = train_fun(X, Y_encoded, num_columns - 1, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "82656e0b85d7dba3468014dc7ed34ac6cf344c04",
        "id": "vW6TNFKoZHsl"
      },
      "source": [
        "### We use 10 k-fold Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja3FhqGCZHsm"
      },
      "source": [
        "model_result(ionosphere)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Nr4Q84oJ9MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "27eed7ff58cf82c8891be7a4897c3e6119787f15",
        "id": "-ZZFPLZXZHsn"
      },
      "source": [
        "#  ========================================================\n",
        "# Second experiment\n",
        "##  Load data of  banknote authentication Data Set \n",
        "\n",
        "This is a small dataset that you can <a href=\"https://www.kaggle.com/jacksonharper/data_banknote_authentication\">download from the kaggle </a>.\n",
        "\n",
        "\n",
        "### About this file:\n",
        "\n",
        "https://www.kaggle.com/jacksonharper/data_banknote_authentication\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "bdb386302802d49db72877cf44748d603cc549f5",
        "id": "khT76wZ8ZHso"
      },
      "source": [
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/data_banknote_authentication.csv' , header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "\n",
        "#Number of Rows and columns\n",
        "Number_rows,Input_size=dataset.shape\n",
        "\n",
        "X = dataset[:,0:Input_size-1].astype(float)\n",
        "Y = dataset[:,Input_size-1]\n",
        "X=normalize(X)\n",
        "X=rescale_range(X)\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "banknote=train_fun(X,Y,Input_size-1,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkhUyKW-ZHso"
      },
      "source": [
        "model_result(banknote)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a4f3d1b35c0a260c3e5deb1d41278f4e1834e807",
        "id": "_JlzgL8OZHso"
      },
      "source": [
        "#  ========================================================\n",
        "# Third experiment\n",
        "##  Load data of  Connectionist Bench (Sonar)\n",
        "\n",
        "This is a small dataset that you can <a href=\"https://www.kaggle.com/adx891/sonar-data-set\">download from the kaggle </a>.\n",
        "\n",
        "\n",
        "### About this file:\n",
        "\n",
        "Connectionist Bench (Sonar, Mines vs. Rocks) Data Set Download: Data Folder, Data Set Description\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4994aeadde7ce8549562411b2762ec43746beecd",
        "id": "JK6MB5B6ZHsp"
      },
      "source": [
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/sonar.all-data.csv', header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "\n",
        "#Number of Rows and columns\n",
        "Number_rows,Input_size=dataset.shape\n",
        "\n",
        "X = dataset[:,0:Input_size-1].astype(float)\n",
        "Y = dataset[:,Input_size-1]\n",
        "X=normalize(X)\n",
        "X=rescale_range(X)\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "sonar=train_fun(X,Y,Input_size-1,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ePxKb_UZHsp"
      },
      "source": [
        "model_result(sonar)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f829d47ace8dfe21048d5d12fea8a401237b518c",
        "collapsed": true,
        "id": "XOkqgdbGZHsp"
      },
      "source": [
        "#  ========================================================\n",
        "# Fourth  experiment    \n",
        "##  Load data of  Pima Indians Diabetes Database\n",
        "\n",
        "\n",
        "\n",
        "This is a small dataset that you can <a href=\"https://www.kaggle.com/uciml/pima-indians-diabetes-database\">download from the kaggle </a>.\n",
        "\n",
        "\n",
        "### About this file:\n",
        "\n",
        "The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ccf19230a88cfcb598b90cb0bf06a8f4695005a8",
        "id": "CtmFc6KzZHsq"
      },
      "source": [
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/diabetes.csv', header=1)\n",
        "dataset = dataframe.values\n",
        "\n",
        "\n",
        "#Number of Rows and columns\n",
        "Number_rows,Input_size=dataset.shape\n",
        "\n",
        "X = dataset[:,0:Input_size-1].astype(float)\n",
        "Y = dataset[:,Input_size-1]\n",
        "X=normalize(X)\n",
        "X=rescale_range(X)\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "diabetes=train_fun(X,Y,Input_size-1,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avrB5UwwZHsq"
      },
      "source": [
        "model_result(diabetes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uhGEXf0MpdbV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbyIXdmflP4T"
      },
      "source": [
        "#  ========================================================\n",
        "# Second experiment\n",
        "##  Load data of   Iris Data Set\n",
        "\n",
        "\n",
        "This is a small dataset that you can <a href=\"https://www.kaggle.com/uciml/iris#Iris.csv\">download from the kaggle </a>.\n",
        "\n",
        "\n",
        "### About this file:\n",
        "\n",
        "https://www.kaggle.com/uciml/iris#Iris.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJseEfIHpfQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiqCqF_mlP4U"
      },
      "outputs": [],
      "source": [
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/Iris.csv', header=1)\n",
        "dataset = dataframe.values\n",
        "\n",
        "\n",
        "# Number of Rows and columns\n",
        "Number_rows,Input_size=dataset.shape\n",
        "\n",
        "X = dataset[:,0:Input_size-1].astype(float)\n",
        "Y = dataset[:,Input_size-1]\n",
        "X=normalize(X)\n",
        "X=rescale_range(X)\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.fit_transform(Y)\n",
        "Y = pd.get_dummies(Y).values\n",
        "\n",
        "iris=train_fun(X,Y,Input_size-1,3)\n",
        "# print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "id": "dktO6N7d8J_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.multiclass import type_of_target\n",
        "type_of_target(Y)"
      ],
      "metadata": {
        "id": "EkJlx3F88MQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris=train_fun(X,Y,Input_size-1,3)"
      ],
      "metadata": {
        "id": "DIntt5aA8Pzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T-ucq9b_pqK9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "MhT54EallP4V"
      },
      "source": [
        "#  ========================================================\n",
        "# Sixth  experiment        \n",
        "##  Load data of  Red Wine Quality\n",
        "\n",
        "\n",
        "This is a small dataset that you can download \n",
        "<a href=\"https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009#winequality-red.csv\">\n",
        "Here </a>.\n",
        "\n",
        "\n",
        "### About this file:\n",
        "\n",
        "This datasets is related to red variants of the Portuguese \"Vinho Verde\" wine. For more details, consult the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/winequality-red.csv', header=1)\n",
        "dataset = dataframe.values\n",
        "\n",
        "#Number of Rows and columns\n",
        "Number_rows,Input_size=dataset.shape\n",
        "\n",
        "X = dataset[:,0:Input_size-1].astype(float)\n",
        "Y = dataset[:,Input_size-1]\n",
        "X=normalize(X)\n",
        "X=rescale_range(X)\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "wine=train_fun(X,Y,Input_size-1)"
      ],
      "metadata": {
        "id": "ZmYFf6-apkVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNDJyiorpsIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpG5eaWgZHss"
      },
      "source": [
        "fig, axs = plt.subplots(2, 2)\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "axs[0, 0].boxplot(ionosphere, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[0, 0].set_title('ionosphere')\n",
        "axs[1, 0].boxplot(banknote, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[1, 0].set_title('banknote')\n",
        "axs[1, 1].boxplot(sonar, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[1, 1].set_title('sonar')\n",
        "axs[0, 1].boxplot(diabetes, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[0, 1].set_title('diabetes')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNfKJgCJZHss"
      },
      "source": [
        "fig, axs = plt.subplots(2, 2)\n",
        "fig.set_size_inches(20, 20)\n",
        "axs[0, 0].boxplot(ionosphere, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[0, 0].set_title('ionosphere')\n",
        "axs[1, 0].boxplot(banknote, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[1, 0].set_title('banknote')\n",
        "axs[1, 1].boxplot(sonar, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[1, 1].set_title('sonar')\n",
        "axs[0, 1].boxplot(diabetes, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[0, 1].set_title('diabetes')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWzsKyazZHst"
      },
      "source": [
        "histories=[]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fd5da88ef403a8b98cc7e1296a2a560a20918346",
        "id": "ckJO8s0XZHsu"
      },
      "source": [
        "# Fifth experiment\n",
        "## Load data of THE MNIST DATABASE of handwritten digits\n",
        "\n",
        "## About this file:\n",
        "The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFPqsAWoZHsu"
      },
      "source": [
        "def plot_(histories):    \n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "    size=1\n",
        "    in_1=histories[0]\n",
        "    in_2=histories[1]\n",
        "    in_3=histories[2]\n",
        "    in_4=histories[3]\n",
        "    in_5=histories[4]\n",
        "    in_6=histories[5]\n",
        "    plt.figure(figsize=(15,15))\n",
        "    #['Ch_First','Ch_second','SWAG','Hermite','Legendre','Sin']\n",
        "###############################################\n",
        "    plt.subplot(321)\n",
        "    training_loss1 = in_1.history['accuracy']\n",
        "    test_loss1 = in_1.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('Ch_First')\n",
        "\n",
        "##################################################\n",
        "    plt.subplot(322)\n",
        "    training_loss1 = in_2.history['accuracy']\n",
        "    test_loss1 = in_2.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('Ch_second')\n",
        "##################################################\n",
        "    plt.subplot(323)\n",
        "    training_loss1 = in_3.history['accuracy']\n",
        "    test_loss1 = in_3.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('SWAG')    \n",
        "##################################################\n",
        "    plt.subplot(324)\n",
        "    training_loss1 = in_4.history['accuracy']\n",
        "    test_loss1 = in_4.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('Hermite') \n",
        "##################################################\n",
        "    plt.subplot(325)\n",
        "    training_loss1 = in_5.history['accuracy']\n",
        "    test_loss1 = in_5.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('Legendre') \n",
        "##################################################\n",
        "    plt.subplot(326)\n",
        "    training_loss1 = in_6.history['accuracy']\n",
        "    test_loss1 = in_6.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('Sin(mx)') \n",
        "    \n",
        "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.0010, right=0.95, hspace=0.3,\n",
        "            wspace=0.18)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0H3o52jAVkpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JE01V2kZHsu"
      },
      "source": [
        "df_XY_train = pd.read_csv('https://raw.githubusercontent.com/sbussmann/kaggle-mnist/master/Data/train.csv')\n",
        "df_X_test  = pd.read_csv('https://raw.githubusercontent.com/sbussmann/kaggle-mnist/master/Data/test.csv')\n",
        "\n",
        "Y_train = df_XY_train['label'].values\n",
        "X_train = df_XY_train.drop('label', axis=1).values\n",
        "X_test  = df_X_test.values\n",
        "\n",
        "# Normalize the data\n",
        "\n",
        "# Normalize the data\n",
        "X_train = X_train +1\n",
        "X_test = X_test +1\n",
        "X_train = X_train / 3.0\n",
        "X_test = X_test / 3.0\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (784, 1) #tensorflow channels_last\n",
        "num_classes = 10\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0],img_rows*img_cols).astype('float32')/255\n",
        "Y_train = to_categorical(Y_train, num_classes)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJqLmPOZZHsu"
      },
      "source": [
        "# create model\n",
        "epochs=3\n",
        "batch_size=128\n",
        "\n",
        "for i in list_fun:\n",
        "    model_7=locals()[i](784,784,10)\n",
        "    model_7.summary()\n",
        "    history=model_7.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs,validation_data = (X_val, Y_val))\n",
        "    histories.append(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ce8d321b7d6a915e260f0ff3a31e820b7ed34fa3",
        "id": "CvA3LVU8ZHsu"
      },
      "source": [
        "plot_(histories)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}