{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningSaeid/Grad/blob/main/Chapter_3_without_coefficient_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "18993f35214f6d02bb40d8f7b5660171a3d9384a",
        "collapsed": true,
        "id": "rraZNltGZHsM"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3YC01S2UddSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install required packages\n",
        "!pip install autokeras\n",
        "!pip install keras==2.9.0\n",
        "!pip install SWAG-DNN==0.1.31\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "import autokeras as ak\n",
        "from SWAG_DNN.taylor_without_Co import *\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "tUF4enr0JAzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3a265a00b0b7f1012da45001f83a0d75598ab96b",
        "id": "lcfca3UeZHsk"
      },
      "source": [
        "# In this code snippet, we set the number of epochs and batch size for training the models. We then define a 10-fold cross-validation test harness using StratifiedKFold from the scikit-learn library. The cvscores list is initialized to store cross-validation scores for each model during the training process.\n",
        "\n",
        "# The activation_functions list contains the names of activation functions that will be used in the models. These functions include Chebyshev of the First Kind, Chebyshev of the Second Kind, SWAG, Hermite, Legendre, and Sin functions.\n",
        "\n",
        "# Set the number of epochs and batch size for training\n",
        "epochs = 5\n",
        "batch_size = 1\n",
        "\n",
        "# Define a 10-fold cross-validation test harness\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "\n",
        "# Initialize an empty list to store cross-validation scores\n",
        "cvscores = []\n",
        "\n",
        "# Create a list of activation functions to be used in the models\n",
        "activation_functions__ = ['Chebyshev Polynomial of the First Kind Model Results', 'Chebyshev Polynomial of the Second Kind Model Results', 'SWAG Model Results', 'Hermite Model Results', 'Legendre polynomials Model Results', 'Sin(nX) Model Results']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function trains various models using different activation functions and performs\n",
        "# 10-fold cross-validation to evaluate their performance. The function takes four arguments:\n",
        "# X (input data), Y (labels), Input_size (number of input features), and output_size\n",
        "# (number of output features). The activation functions used are stored in the activation_functions list,\n",
        "# which includes Chebyshev of the First Kind, Chebyshev of the Second Kind, SWAG, Hermite,\n",
        "# Legendre, and Sin functions.\n",
        "def train_fun(X, Y, Input_size, output_size):\n",
        "    activation_functions = ['Ch_First', 'Ch_second', 'SWAG', 'Hermite', 'Legendre', 'Sin']\n",
        "    activation_functions__ = ['Chebyshev Polynomial of the First Kind Model Results', 'Chebyshev Polynomial of the Second Kind Model Results', 'SWAG Model Results', 'Hermite Model Results', 'Legendre polynomials Model Results', 'Sin(nX) Model Results']\n",
        "    results = list()\n",
        "    model_number=0\n",
        "    # Iterate through each activation function\n",
        "    for act_function in activation_functions:\n",
        "        print('############################################################################################################################')       \n",
        "        cv_scores = []\n",
        "        model = globals()[act_function](Input_size, Input_size, output_size)\n",
        "        # print(model.summary())\n",
        "        print(activation_functions__[model_number])\n",
        "        model_number = model_number + 1\n",
        "        accuracies = np.zeros(shape=(11,))\n",
        "        index = 0\n",
        "\n",
        "        # Perform 10-fold cross-validation\n",
        "        for train, test in kfold.split(X, Y):\n",
        "            model.fit(X[train], Y[train], epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "            scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "            print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
        "            cv_scores.append(scores[1] * 100)\n",
        "            accuracies[index] = scores[1] * 100\n",
        "            index += 1\n",
        "\n",
        "        # Calculate the mean accuracy and append it to the accuracies array\n",
        "        accuracies[index] = np.mean(cv_scores)\n",
        "        accuracies = np.round(accuracies, 2)\n",
        "        results.append(accuracies)\n",
        "        print(\"The Mean accuracy is %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
        "\n",
        "    # AutoKeras model\n",
        "    auto_keras_accuracies = np.zeros(shape=(11,))\n",
        "    index = 0\n",
        "    cv_scores = []\n",
        "\n",
        "    print('############################################################################################################################')      \n",
        "    print('AutoKeras Model (max_trials=30) Result:')\n",
        "    \n",
        "    for train, test in kfold.split(X, Y):\n",
        "        model = ak.StructuredDataClassifier(max_trials=30)\n",
        "        model.fit(X[train], Y[train], epochs=epochs, verbose=0)\n",
        "        scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "        print(\" accuracy : %.2f%%\" % (scores[1] * 100))\n",
        "        cv_scores.append(scores[1] * 100)\n",
        "        auto_keras_accuracies[index] = scores[1] * 100\n",
        "        index += 1\n",
        "    \n",
        "    auto_keras_accuracies[index] = np.mean(cv_scores)\n",
        "    print(np.mean(cv_scores)) \n",
        "    auto_keras_accuracies = np.round(auto_keras_accuracies, 2)\n",
        "    results.append(auto_keras_accuracies)\n",
        "    print('**************************************************************************************************')\n",
        "    \n",
        "    return results\n"
      ],
      "metadata": {
        "id": "MK7UurXBR9Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the results from the models and prints the accuracy scores for each fold\n",
        "# and the mean accuracy for each model. The input is a list of arrays (model_re) containing\n",
        "# accuracy scores for each model.\n",
        "def print_model_results(model_results):\n",
        "    # Iterate through each fold (11 in total: 10 for each fold, and 1 for the mean)\n",
        "    for j in range(11):\n",
        "        result_str = '0'\n",
        "        \n",
        "        # Iterate through each model (7 models in total)\n",
        "        for i in range(7):\n",
        "            # Format the result string with accuracy scores\n",
        "            if i > 0:\n",
        "                result_str = result_str + ' & ' + str(model_results[i][j])\n",
        "            else:\n",
        "                result_str = str(round(model_results[i][j], 2))\n",
        "        \n",
        "        # Add the LaTeX formatting for a new row in a table\n",
        "        result_str = result_str + r\" \\\\ \"\n",
        "        \n",
        "        # Print the results\n",
        "        if j < 10:\n",
        "            print(result_str)\n",
        "        else:\n",
        "            print('************************************************')\n",
        "            print(result_str)\n",
        "            print('************************************************************************************************')\n",
        "    \n",
        "    # Print the mean accuracy for each model with its respective name\n",
        "    model_names = ['Chebyshev First kind', 'Chebyshev Second kind', 'SWAG', 'Hermite', 'Legendre', 'Sin(nX)', \"AutoKeras\"]\n",
        "    for i in range(7):\n",
        "        print(model_names[i], model_results[i])\n"
      ],
      "metadata": {
        "id": "VgXsAgyaMmwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function trains and evaluates an AutoKeras model using 10-fold cross-validation.\n",
        "# It takes two input arguments: X_ (features) and Y_ (labels) and returns an array\n",
        "# containing the accuracy scores for each fold and the mean accuracy.\n",
        "def auto_keras_model(X_, Y_):\n",
        "    # Initialize an array to store accuracy scores\n",
        "    accuracy_scores = np.zeros(shape=(10,))\n",
        "    index = 0\n",
        "    cvscores = []\n",
        "\n",
        "    # Perform 10-fold cross-validation\n",
        "    for train, test in kfold.split(X_, Y_):\n",
        "        # Create and train the AutoKeras model\n",
        "        model = ak.StructuredDataClassifier(max_trials=30)\n",
        "        model.fit(X_[train], Y_[train], epochs=epochs, verbose=0)\n",
        "        \n",
        "        # Evaluate the model and print the accuracy\n",
        "        scores = model.evaluate(X_[test], Y_[test], verbose=0)\n",
        "        print(\" accuracy : %.2f%%\" % (scores[1] * 100))\n",
        "        \n",
        "        # Store the accuracy scores\n",
        "        cvscores.append(scores[1] * 100)\n",
        "        accuracy_scores[index] = scores[1] * 100\n",
        "        index += 1\n",
        "\n",
        "    # Print the mean accuracy\n",
        "    print(np.mean(cvscores))\n",
        "    print('**************************************************************************************************')\n",
        "    \n",
        "    # Return the accuracy scores array\n",
        "    return accuracy_scores\n"
      ],
      "metadata": {
        "id": "a8x9E5z19roh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "51659646ab1c7e1c87283d427e7bab1c9ddb58a6",
        "id": "g48shk8hZHsl"
      },
      "source": [
        "#First Experiment\n",
        "##Load and Analyze the Ionosphere Dataset\n",
        "\n",
        "The Ionosphere dataset is a small dataset available for download from Kaggle.\n",
        "##About the dataset:\n",
        "\n",
        "The Ionosphere dataset is a popular benchmark dataset used in machine learning and pattern recognition. It was originally collected by researchers at Johns Hopkins University Applied Physics Laboratory to study the structure and electron content of the ionosphere. The dataset consists of radar data collected from a phased array of 16 high-frequency antennas, with a total of 34 attributes and a binary target variable.\n",
        "\n",
        "The dataset contains 351 instances, each representing a radar return from the ionosphere. The 34 attributes correspond to various features extracted from the radar signals, such as the phase and amplitude of the received signal. These features have been pre-processed and scaled to make them suitable for machine learning algorithms. The binary target variable indicates whether the radar return shows evidence of some structure in the ionosphere (labeled \"good\") or not (labeled \"bad\").\n",
        "\n",
        "Source: Donated to the UCI Machine Learning Repository by Vince Sigillito (vgs '@' aplcen.apl.jhu.edu)\n",
        "\n",
        "Data Source:\n",
        "\n",
        "Space Physics Group\n",
        "Applied Physics Laboratory\n",
        "Johns Hopkins University\n",
        "Johns Hopkins Road\n",
        "Laurel, MD 20723\n",
        "\n",
        "In this experiment, we will load and analyze the Ionosphere dataset to gain insights and understand the structure of the data before using it for machine learning tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9e664b718ffcf89c49865b7c16d56603692fe233",
        "id": "SLSQzg2dZHsl"
      },
      "source": [
        "# This code segment loads the ionosphere dataset from the provided URL and stores it as a Pandas DataFrame. It then converts the DataFrame into a NumPy array and separates the features (X) and labels (Y). The class values are encoded as integers, and the models are trained and evaluated using the train_fun function.\n",
        "\n",
        "# Load the ionosphere dataset from the provided URL and store it as a Pandas DataFrame\n",
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/ionosphere_data_kaggle.csv', header=1)\n",
        "\n",
        "# Convert the DataFrame into a NumPy array\n",
        "dataset = dataframe.values\n",
        "\n",
        "# Get the number of rows and columns\n",
        "num_rows, num_columns = dataset.shape\n",
        "\n",
        "# Separate the features (X) and labels (Y)\n",
        "X = dataset[:, 0:num_columns - 1].astype(float)\n",
        "Y = dataset[:, num_columns - 1]\n",
        "\n",
        "# Encode the class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y_encoded = encoder.transform(Y)\n",
        "\n",
        "# Train and evaluate the models using the train_fun function\n",
        "ionosphere_results = train_fun(X, Y_encoded, num_columns - 1, 1)\n",
        "print_model_results(ionosphere_results)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "27eed7ff58cf82c8891be7a4897c3e6119787f15",
        "id": "-ZZFPLZXZHsn"
      },
      "source": [
        "#  ========================================================\n",
        "# Second experiment\n",
        "In the second experiment, we focus on the Banknote Authentication dataset, a small yet valuable dataset for machine learning and pattern recognition applications. You can download this dataset from Kaggle at the following link:\n",
        "\n",
        "##Banknote Authentication Dataset\n",
        "\n",
        "##About the Banknote Authentication Dataset:\n",
        "\n",
        "The Banknote Authentication dataset contains data extracted from images of genuine and forged banknotes, making it an excellent resource for training machine learning models to differentiate between authentic and counterfeit banknotes. The dataset consists of 1,372 instances, each representing a banknote with four attributes derived from the Wavelet Transform tool applied to the banknote image. These attributes include variance, skewness, kurtosis, and image entropy. The target variable is binary, indicating whether a banknote is genuine (1) or forged (0).\n",
        "\n",
        "The dataset was created using a combination of genuine and forged banknote samples, with images captured at different scales, rotations, and illumination conditions. The extracted features have been preprocessed and scaled to make them suitable for machine learning algorithms.\n",
        "\n",
        "By utilizing the Banknote Authentication dataset in this experiment, we aim to evaluate the performance of various activation functions and model architectures on a real-world classification task. This will help us gain insights into the effectiveness of the proposed methods and their potential applications in different domains.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "bdb386302802d49db72877cf44748d603cc549f5",
        "id": "khT76wZ8ZHso"
      },
      "source": [
        "# In this code snippet, we read the Banknote Authentication dataset using pandas, preprocess it, and then train the models using the train_fun function. First, the dataset is read and stored in a dataframe. Then, the number of rows and columns is determined. The features (X) and the target variable (Y) are separated, with X containing the attributes and Y containing the class labels.\n",
        "\n",
        "# The features are normalized and rescaled to ensure that they are on a similar scale, which improves the performance of machine learning algorithms. The target variable is encoded as integers using the LabelEncoder class from scikit-learn.\n",
        "\n",
        "# Finally, the models are trained on the Banknote Authentication dataset using the train_fun function, which evaluates the performance of various activation functions and model architectures. The results are stored in the banknote variable.\n",
        "\n",
        "# Load and preprocess the Banknote Authentication dataset\n",
        "# Source: https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/data_banknote_authentication.csv\n",
        "\n",
        "# Read the dataset using pandas and store it in a dataframe\n",
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/data_banknote_authentication.csv', header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# Determine the number of rows and columns in the dataset\n",
        "num_rows, input_size = dataset.shape\n",
        "\n",
        "# Separate the features (X) and target variable (Y)\n",
        "X = dataset[:, 0:input_size-1].astype(float)\n",
        "Y = dataset[:, input_size-1]\n",
        "\n",
        "# Normalize and rescale the features\n",
        "X = normalize(X)\n",
        "X = rescale_range(X)\n",
        "\n",
        "# Encode the target variable as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "\n",
        "# Train the models on the Banknote Authentication dataset using the train_fun function\n",
        "banknote_results = train_fun(X, Y, input_size-1, 1)\n",
        "print_model_results(banknote_results)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a4f3d1b35c0a260c3e5deb1d41278f4e1834e807",
        "id": "_JlzgL8OZHso"
      },
      "source": [
        "#Third Experiment\n",
        "##Load data of Connectionist Bench (Sonar, Mines vs. Rocks) dataset\n",
        "\n",
        "The Connectionist Bench (Sonar, Mines vs. Rocks) dataset is a small dataset that can be downloaded from Kaggle at this link.\n",
        "##About the dataset:\n",
        "\n",
        "The Connectionist Bench (Sonar) dataset is a classic benchmark dataset in machine learning and pattern recognition, focused on the binary classification problem of distinguishing between mines and rocks based on sonar signal data. The dataset was collected through the use of a sonar system that emitted signals and recorded the echoes returned from various materials (mines and rocks) under different conditions.\n",
        "\n",
        "The dataset contains 208 instances, each representing a single sonar return. The attributes consist of 60 real-valued features, corresponding to the energy within a particular frequency band, integrated over a certain period. These features have been pre-processed and scaled, making them suitable for machine learning algorithms. The binary target variable indicates whether the sonar return corresponds to a mine (labeled \"M\") or a rock (labeled \"R\").\n",
        "\n",
        "This dataset is widely used in the field of machine learning to test and evaluate the performance of different classification algorithms and techniques. By training and testing various models on this dataset, researchers can gain insights into the effectiveness and generalization capabilities of the algorithms in real-world scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4994aeadde7ce8549562411b2762ec43746beecd",
        "id": "JK6MB5B6ZHsp"
      },
      "source": [
        "# This code snippet imports the Sonar dataset, preprocesses the input features and target labels, and trains the models using the defined train_fun function. The variable names have been updated to be more meaningful and comments have been added to explain each step in the process.\n",
        "\n",
        "# Load the Sonar dataset\n",
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/sonar.all-data.csv', header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# Get the number of rows and columns\n",
        "num_rows, input_size = dataset.shape\n",
        "\n",
        "# Prepare the input features (X) and target labels (Y)\n",
        "X = dataset[:, 0:input_size - 1].astype(float)\n",
        "Y = dataset[:, input_size - 1]\n",
        "\n",
        "# Normalize and rescale the input features\n",
        "X = normalize(X)\n",
        "X = rescale_range(X)\n",
        "\n",
        "# Encode the class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "\n",
        "# Train the models on the Sonar dataset\n",
        "sonar_results = train_fun(X, Y, input_size - 1, 1)\n",
        "print_model_results(sonar_results)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f829d47ace8dfe21048d5d12fea8a401237b518c",
        "collapsed": true,
        "id": "XOkqgdbGZHsp"
      },
      "source": [
        "#Fourth Experiment\n",
        "##Load data of Pima Indians Diabetes Database\n",
        "\n",
        "This dataset is available for download from Kaggle at the following link: Pima Indians Diabetes Database.\n",
        "About this file:\n",
        "\n",
        "The Pima Indians Diabetes Database consists of several medical predictor (independent) variables and one target (dependent) variable, named \"Outcome\". The independent variables include various health metrics such as the number of pregnancies a patient has had, their BMI, insulin level, age, and more. This dataset is widely used for studying and predicting the onset of diabetes based on these health factors. The data was collected from the Pima Indian population, which has a high prevalence of diabetes, making it a valuable resource for researching and developing machine learning models for diabetes prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ccf19230a88cfcb598b90cb0bf06a8f4695005a8",
        "id": "CtmFc6KzZHsq"
      },
      "source": [
        "\n",
        "# This code snippet loads the Pima Indians Diabetes Database dataset, processes it, and trains the model using the provided train_fun function. The variable names have been updated for better readability and comments have been added to describe each step.\n",
        "# Load the Pima Indians Diabetes Database dataset\n",
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/diabetes.csv', header=1)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# Determine the number of rows and columns in the dataset\n",
        "Number_rows, Input_size = dataset.shape\n",
        "\n",
        "# Extract input features (X) and target variable (Y)\n",
        "X = dataset[:, 0:Input_size-1].astype(float)\n",
        "Y = dataset[:, Input_size-1]\n",
        "\n",
        "# Normalize and rescale input features\n",
        "X = normalize(X)\n",
        "X = rescale_range(X)\n",
        "\n",
        "# Encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "\n",
        "# Train the model using the diabetes dataset\n",
        "diabetes_results = train_fun(X, Y, Input_size-1, 1)\n",
        "print_model_results(diabetes_results)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiqCqF_mlP4U"
      },
      "outputs": [],
      "source": [
        "# for this case we should use Kfold instead of StratifiedKFold\n",
        "# # Load the Iris dataset\n",
        "# url = 'https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/Iris.csv'\n",
        "# dataframe = read_csv(url, header=1)\n",
        "# dataset = dataframe.values\n",
        "\n",
        "# # Determine the number of rows and columns in the dataset\n",
        "# Number_rows, Input_size = dataset.shape\n",
        "\n",
        "# # Extract input features (X) and target variable (Y)\n",
        "# X = dataset[:, 0:Input_size-1].astype(float)\n",
        "# Y = dataset[:, Input_size-1]\n",
        "\n",
        "# # Normalize and rescale input features\n",
        "# X = normalize(X)\n",
        "# X = rescale_range(X)\n",
        "\n",
        "# # Encode class values as integers and one-hot encode the target variable\n",
        "# encoder = LabelEncoder()\n",
        "# Y = encoder.fit_transform(Y)\n",
        "# Y = pd.get_dummies(Y).values\n",
        "\n",
        "# # Train the model using the Iris dataset\n",
        "# iris = train_fun(X, Y, Input_size-1, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This organized code snippet generates a 2x2 grid of boxplots, each representing the performance of different activation functions on four datasets: Ionosphere, Banknote, Sonar, and Diabetes.\n",
        "# Define activation function names\n",
        "activation_functions = ['Ch_First', 'Ch_second', 'SWAG', 'Hermite', 'Legendre', 'Sin', 'AutoKeras']\n",
        "\n",
        "# Create a 2x2 subplot grid for the boxplots\n",
        "fig, axs = plt.subplots(2, 2)\n",
        "fig.set_size_inches(22.5, 15.5)\n",
        "\n",
        "# Ionosphere dataset boxplot\n",
        "axs[0, 0].boxplot(ionosphere_results, labels=[str(func) for func in activation_functions], showmeans=True)\n",
        "axs[0, 0].set_title('Ionosphere')\n",
        "\n",
        "# Banknote dataset boxplot\n",
        "axs[1, 0].boxplot(banknote_results, labels=[str(func) for func in activation_functions], showmeans=True)\n",
        "axs[1, 0].set_title('Banknote')\n",
        "\n",
        "# Sonar dataset boxplot\n",
        "axs[1, 1].boxplot(sonar_results, labels=[str(func) for func in activation_functions], showmeans=True)\n",
        "axs[1, 1].set_title('Sonar')\n",
        "\n",
        "# Diabetes dataset boxplot\n",
        "axs[0, 1].boxplot(diabetes_results, labels=[str(func) for func in activation_functions], showmeans=True)\n",
        "axs[0, 1].set_title('Diabetes')\n",
        "\n",
        "# Display the boxplots\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "f4WbFgeYbLn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fd5da88ef403a8b98cc7e1296a2a560a20918346",
        "id": "ckJO8s0XZHsu"
      },
      "source": [
        "# Fifth experiment\n",
        "## Load data of THE MNIST DATABASE of handwritten digits\n",
        "\n",
        "## About this file:\n",
        "The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This code snippet organizes the previous code and adds comments for better readability. The variable names are more meaningful, and the code is more modular. The function plot_accuracies takes a list of history objects and creates a 3x2 grid of subplots, each showing the training and validation accuracy for a specific activation function.\n",
        "# Define a function to plot training and validation accuracy for different activation functions\n",
        "def plot_accuracies(histories):\n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    # Define activation function names\n",
        "    activation_functions = ['Ch_First', 'Ch_second', 'SWAG', 'Hermite', 'Legendre', 'Sin']\n",
        "\n",
        "    # Loop through each activation function and create a subplot\n",
        "    for i, activation_function in enumerate(activation_functions):\n",
        "        plt.subplot(3, 2, i + 1)\n",
        "\n",
        "        # Extract training and validation accuracy from the history object\n",
        "        training_accuracy = histories[i].history['accuracy']\n",
        "        validation_accuracy = histories[i].history['val_accuracy']\n",
        "        epoch_count = range(1, len(training_accuracy) + 1)\n",
        "\n",
        "        # Plot the training and validation accuracy\n",
        "        plt.plot(epoch_count, training_accuracy, 'r--')\n",
        "        plt.plot(epoch_count, validation_accuracy, 'b-')\n",
        "\n",
        "        # Customize the plot\n",
        "        plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title(activation_function)\n",
        "\n",
        "    # Adjust the layout of the subplots\n",
        "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.0010, right=0.95, hspace=0.3, wspace=0.18)\n",
        "\n",
        "    # Show the plots\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# plot_accuracies(histories)\n"
      ],
      "metadata": {
        "id": "Bh3_aO23b-4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This code snippet organizes the previous code and adds comments for better readability. The variable names are more meaningful, and the code is more modular. The script imports the dataset, processes the data, and trains the model for each activation function. The history objects for each activation function are stored in a list for further analysis or visualization.\n",
        "# Import the dataset\n",
        "df_XY_train = pd.read_csv('https://raw.githubusercontent.com/sbussmann/kaggle-mnist/master/Data/train.csv')\n",
        "df_X_test = pd.read_csv('https://raw.githubusercontent.com/sbussmann/kaggle-mnist/master/Data/test.csv')\n",
        "\n",
        "# Extract features and labels from the training dataset\n",
        "Y_train = df_XY_train['label'].values\n",
        "X_train = df_XY_train.drop('label', axis=1).values\n",
        "X_test = df_X_test.values\n",
        "\n",
        "# Normalize the data\n",
        "X_train = (X_train + 1) / 3.0\n",
        "X_test = (X_test + 1) / 3.0\n",
        "\n",
        "# Define image dimensions and input shape\n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (784, 1)  # Tensorflow uses channels_last\n",
        "num_classes = 10\n",
        "\n",
        "# Reshape and scale the data\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows * img_cols).astype('float32') / 255\n",
        "Y_train = to_categorical(Y_train, num_classes)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=7)\n",
        "\n",
        "# Set training parameters\n",
        "epochs = 3\n",
        "batch_size = 128\n",
        "activation_functions = ['Ch_First', 'Ch_second', 'SWAG', 'Hermite', 'Legendre', 'Sin']\n",
        "activation_functions__ = ['Chebyshev Polynomial of the First Kind Model Results', 'Chebyshev Polynomial of the Second Kind Model Results', 'SWAG Model Results', 'Hermite Model Results', 'Legendre polynomials Model Results', 'Sin(nX) Model Results']\n",
        "\n",
        "# Initialize a list to store the history objects\n",
        "histories = []\n",
        "\n",
        "# Train the model for each activation function\n",
        "model_des=0\n",
        "for activation_function in activation_functions:\n",
        "    print('#################################################################################')\n",
        "    model = locals()[activation_function](784, 784, 10)\n",
        "    print(activation_functions__[model_des])\n",
        "    model_des=model_des + 1\n",
        "    # model.summary()\n",
        "    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, Y_val))\n",
        "    histories.append(history)\n"
      ],
      "metadata": {
        "id": "0H3o52jAVkpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ce8d321b7d6a915e260f0ff3a31e820b7ed34fa3",
        "id": "CvA3LVU8ZHsu"
      },
      "source": [
        "plot_accuracies(histories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define activation function names\n",
        "activation_functions = ['Ch_First', 'Ch_second', 'SWAG', 'Hermite', 'Legendre', 'Sin']\n",
        "\n",
        "# Loop through each activation function and create a subplot\n",
        "for i, activation_function in enumerate(activation_functions):\n",
        "\n",
        "    # Extract training and validation accuracy from the history object\n",
        "    training_accuracy = histories[i].history['accuracy']\n",
        "    validation_accuracy = histories[i].history['val_accuracy']\n",
        "    print('*********************************************')\n",
        "    print(activation_function)\n",
        "    print(\"Training accuracy:\",training_accuracy)\n",
        "    print(\"Validation accuracy:\",validation_accuracy)"
      ],
      "metadata": {
        "id": "DZzfqAcxhv4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Chapter 3 without coefficient .ipynb'\n",
        "from google.colab import files\n",
        "files.download(path)"
      ],
      "metadata": {
        "id": "clgO3oMf5hvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nS64CLSR768E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}