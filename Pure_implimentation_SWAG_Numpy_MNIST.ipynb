{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNn+1PvG97B0t0Hg6qe5sYQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningSaeid/Grad/blob/main/Pure_implimentation_SWAG_Numpy_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def X_activation(x):\n",
        "    return x\n",
        "\n",
        "def X_activation_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def X_2_activation(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "def X_2_activation_derivative(x):\n",
        "    return (x / 4)\n",
        "\n",
        "def X_3_activation(x):\n",
        "    return (x**3) / 24\n",
        "\n",
        "def X_3_activation_derivative(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, output_size):\n",
        "        # Define the architecture\n",
        "        self.input_size = input_size\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "        self.hidden3_size = hidden3_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = {\n",
        "            'W1': np.random.randn(input_size, hidden1_size),\n",
        "            'W2': np.random.randn(input_size, hidden2_size),\n",
        "            'W3': np.random.randn(input_size, hidden3_size),\n",
        "            'W4': np.random.randn(hidden1_size + hidden2_size + hidden3_size, output_size)\n",
        "        }\n",
        "\n",
        "        self.biases = {\n",
        "            'b1': np.random.randn(1, hidden1_size),\n",
        "            'b2': np.random.randn(1, hidden2_size),\n",
        "            'b3': np.random.randn(1, hidden3_size),\n",
        "            'b4': np.random.randn(1, output_size)\n",
        "        }\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def X_activation(self, x):\n",
        "        return X_activation(x)\n",
        "\n",
        "    def X_activation_derivative(self, x):\n",
        "        return X_activation_derivative(x)\n",
        "\n",
        "    def X_2_activation(self, x):\n",
        "        return X_2_activation(x)\n",
        "\n",
        "    def X_2_activation_derivative(self, x):\n",
        "        return X_2_activation_derivative(x)\n",
        "\n",
        "    def X_3_activation(self, x):\n",
        "        return X_3_activation(x)\n",
        "\n",
        "    def X_3_activation_derivative(self, x):\n",
        "        return X_3_activation_derivative(x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Layer 1 (X Activation)\n",
        "        self.z1 = np.dot(X, self.weights['W1']) + self.biases['b1']\n",
        "        self.a1 = self.X_activation(self.z1)  # Use X activation for layer\n",
        "\n",
        "        # Layer 2\n",
        "        self.z2 = np.dot(X, self.weights['W2']) + self.biases['b2']\n",
        "        self.a2 = self.X_2_activation(self.z2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.z3 = np.dot(X, self.weights['W3']) + self.biases['b3']\n",
        "        self.a3 = self.X_3_activation(self.z3)\n",
        "\n",
        "        # Concatenate the outputs of layers 1, 2, and 3\n",
        "        self.concatenated_output = np.concatenate((self.a1, self.a2, self.a3), axis=1)\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        self.z4 = np.dot(self.concatenated_output, self.weights['W4']) + self.biases['b4']\n",
        "        self.output = self.X_activation(self.z4)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Backpropagation\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        delta4 = 2 * (self.output - y) * self.X_activation_derivative(self.output)\n",
        "        dW4 = np.dot(self.concatenated_output.T, delta4)\n",
        "        db4 = np.sum(delta4, axis=0, keepdims=True)\n",
        "\n",
        "        # Split the delta for the concatenation in layer 4\n",
        "        delta4_split = np.dot(delta4, self.weights['W4'].T)\n",
        "\n",
        "        # Separate the deltas for layers 1, 2, and 3\n",
        "        delta3 = delta4_split[:, -self.hidden3_size:] * self.X_3_activation_derivative(self.a3)\n",
        "\n",
        "        delta2 = delta4_split[:, -self.hidden3_size-self.hidden2_size:-self.hidden3_size] * self.X_2_activation_derivative(self.a2)\n",
        "        delta1 = delta4_split[:, :-self.hidden3_size-self.hidden2_size] * self.X_activation_derivative(self.a1)\n",
        "\n",
        "        # Calculate gradients for layers 1, 2, and 3\n",
        "        dW3 = np.dot(X.T, delta3)\n",
        "        db3 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        dW2 = np.dot(X.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights['W4'] -= learning_rate * dW4\n",
        "        self.biases['b4'] -= learning_rate * db4\n",
        "        self.weights['W3'] -= learning_rate * dW3\n",
        "        self.biases['b3'] -= learning_rate * db3\n",
        "        self.weights['W2'] -= learning_rate * dW2\n",
        "        self.biases['b2'] -= learning_rate * db2\n",
        "        self.weights['W1'] -= learning_rate * dW1\n",
        "        self.biases['b1'] -= learning_rate * db1\n",
        "\n",
        "        return dW4, db4, dW3, db3, dW2, db2, dW1, db1\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            self.feedforward(X)\n",
        "\n",
        "            # Backpropagation\n",
        "            self.backpropagation(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 500 == 0:\n",
        "                loss = np.mean((self.output - y) ** 2)\n",
        "                print(f'Epoch {epoch}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "##########################################################\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import time\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Add a small epsilon value to avoid exactly zero or one values\n",
        "epsilon = 1e-4\n",
        "X = np.clip(X, epsilon, 1 - epsilon)  # Clip values to be in the range (epsilon, 1 - epsilon)\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "learning_rate = 0.00001\n",
        "epochs = 3000\n",
        "\n",
        "\n",
        "input_size = 4\n",
        "hidden1_size = 8\n",
        "hidden2_size = 8\n",
        "hidden3_size = 8\n",
        "output_size = 3\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size,hidden3_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "start_time = time.time()\n",
        "\n",
        "nn.train(X_train, y_train, learning_rate, epochs)\n",
        "\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training Execution Time: {execution_time:.2f} seconds\")\n",
        "# Evaluate the trained model\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
        "\n",
        "y_pred = nn.feedforward(X_test)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "WUK1OWKhB-WJ",
        "outputId": "34391806-a9f1-4e79-fc35-101cdf7ec3c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/3000, Loss: 15.7831\n",
            "Epoch 500/3000, Loss: 0.1666\n",
            "Epoch 1000/3000, Loss: 0.1131\n",
            "Epoch 1500/3000, Loss: 0.0945\n",
            "Epoch 2000/3000, Loss: 0.0838\n",
            "Epoch 2500/3000, Loss: 0.0768\n",
            "Training Execution Time: 1.35 seconds\n",
            "Test Accuracy: 93.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def X_activation(x):\n",
        "    return x\n",
        "\n",
        "def X_activation_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def X_2_activation(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "def X_2_activation_derivative(x):\n",
        "    return (x / 4)\n",
        "\n",
        "def X_3_activation(x):\n",
        "    return (x**3) / 24\n",
        "\n",
        "def X_3_activation_derivative(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, output_size):\n",
        "        # Define the architecture\n",
        "        self.input_size = input_size\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "        self.hidden3_size = hidden3_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = {\n",
        "            'W1': np.random.randn(input_size, hidden1_size),\n",
        "            'W2': np.random.randn(input_size, hidden2_size),\n",
        "            'W3': np.random.randn(input_size, hidden3_size),\n",
        "            'W4': np.random.randn(hidden1_size + hidden2_size + hidden3_size, output_size)\n",
        "        }\n",
        "\n",
        "        self.biases = {\n",
        "            'b1': np.random.randn(1, hidden1_size),\n",
        "            'b2': np.random.randn(1, hidden2_size),\n",
        "            'b3': np.random.randn(1, hidden3_size),\n",
        "            'b4': np.random.randn(1, output_size)\n",
        "        }\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def X_activation(self, x):\n",
        "        return X_activation(x)\n",
        "\n",
        "    def X_activation_derivative(self, x):\n",
        "        return X_activation_derivative(x)\n",
        "\n",
        "    def X_2_activation(self, x):\n",
        "        return X_2_activation(x)\n",
        "\n",
        "    def X_2_activation_derivative(self, x):\n",
        "        return X_2_activation_derivative(x)\n",
        "\n",
        "    def X_3_activation(self, x):\n",
        "        return X_3_activation(x)\n",
        "\n",
        "    def X_3_activation_derivative(self, x):\n",
        "        return X_3_activation_derivative(x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Layer 1 (X Activation)\n",
        "        self.z1 = np.dot(X, self.weights['W1']) + self.biases['b1']\n",
        "        self.a1 = self.X_activation(self.z1)  # Use X activation for layer\n",
        "\n",
        "        # Layer 2\n",
        "        self.z2 = np.dot(X, self.weights['W2']) + self.biases['b2']\n",
        "        self.a2 = self.X_2_activation(self.z2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.z3 = np.dot(X, self.weights['W3']) + self.biases['b3']\n",
        "        self.a3 = self.X_3_activation(self.z3)\n",
        "\n",
        "        # Concatenate the outputs of layers 1, 2, and 3\n",
        "        self.concatenated_output = np.concatenate((self.a1, self.a2, self.a3), axis=1)\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        self.z4 = np.dot(self.concatenated_output, self.weights['W4']) + self.biases['b4']\n",
        "        self.output = self.X_activation(self.z4)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Backpropagation\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        delta4 = 2 * (self.output - y) * self.X_activation_derivative(self.output)\n",
        "        dW4 = np.dot(self.concatenated_output.T, delta4)\n",
        "        db4 = np.sum(delta4, axis=0, keepdims=True)\n",
        "\n",
        "        # Split the delta for the concatenation in layer 4\n",
        "        delta4_split = np.dot(delta4, self.weights['W4'].T)\n",
        "\n",
        "        # Separate the deltas for layers 1, 2, and 3\n",
        "        delta3 = delta4_split[:, -self.hidden3_size:] * self.X_3_activation_derivative(self.a3)\n",
        "\n",
        "        delta2 = delta4_split[:, -self.hidden3_size-self.hidden2_size:-self.hidden3_size] * self.X_2_activation_derivative(self.a2)\n",
        "        delta1 = delta4_split[:, :-self.hidden3_size-self.hidden2_size] * self.X_activation_derivative(self.a1)\n",
        "\n",
        "        # Calculate gradients for layers 1, 2, and 3\n",
        "        dW3 = np.dot(X.T, delta3)\n",
        "        db3 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        dW2 = np.dot(X.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights['W4'] -= learning_rate * dW4\n",
        "        self.biases['b4'] -= learning_rate * db4\n",
        "        self.weights['W3'] -= learning_rate * dW3\n",
        "        self.biases['b3'] -= learning_rate * db3\n",
        "        self.weights['W2'] -= learning_rate * dW2\n",
        "        self.biases['b2'] -= learning_rate * db2\n",
        "        self.weights['W1'] -= learning_rate * dW1\n",
        "        self.biases['b1'] -= learning_rate * db1\n",
        "\n",
        "        return dW4, db4, dW3, db3, dW2, db2, dW1, db1\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        batch_size = 10\n",
        "        n_batches = len(X) // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0  # Reset epoch loss\n",
        "\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                X_batch = X[i:i + batch_size]\n",
        "                y_batch = y[i:i + batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                self.feedforward(X_batch)\n",
        "\n",
        "                # Backpropagation\n",
        "                self.backpropagation(X_batch, y_batch, learning_rate)\n",
        "\n",
        "                # Calculate batch loss and add it to epoch loss\n",
        "                batch_loss = np.mean((self.output - y_batch) ** 2)\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            # Calculate average epoch loss\n",
        "            epoch_loss /= n_batches\n",
        "\n",
        "            if epoch % 500 == 0:\n",
        "                print(f'Epoch {epoch}/{epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "##########################################################\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import time\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Add a small epsilon value to avoid exactly zero or one values\n",
        "epsilon = 1e-4\n",
        "X = np.clip(X, epsilon, 1 - epsilon)  # Clip values to be in the range (epsilon, 1 - epsilon)\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "learning_rate = 0.0001\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "input_size = 4\n",
        "hidden1_size = 8\n",
        "hidden2_size = 8\n",
        "hidden3_size = 8\n",
        "output_size = 3\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size,hidden3_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "start_time = time.time()\n",
        "\n",
        "nn.train(X_train, y_train, learning_rate, epochs)\n",
        "\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training Execution Time: {execution_time:.2f} seconds\")\n",
        "# Evaluate the trained model\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
        "\n",
        "y_pred = nn.feedforward(X_test)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "49mz28zO1ee4",
        "outputId": "43c27909-efea-49e4-bd72-d1a01d899505",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/100, Loss: 11.0058\n",
            "Training Execution Time: 0.19 seconds\n",
            "Test Accuracy: 70.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b3': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square(Z3)\n",
        "\n",
        "    return A3, (X, Z1, A1, Z2, A2, Z3, A3, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated = cache\n",
        "\n",
        "    dA3 = 2 * (A3 - y_true)\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated = np.dot(dZ3, params['W3'].T)\n",
        "    dA1, dA2 = d_concatenated[:, :hidden_size1], d_concatenated[:, hidden_size1:]\n",
        "\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = dA2 * square_derivative(Z2)\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]  # Convert to one-hot encoding\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "# Convert predictions to one-hot encoded format\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "\n",
        "# Compute accuracy using one-hot encoded predictions\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "lkkAcYvnRl4V",
        "outputId": "113f8997-9503-4135-9e88-b52f3d4250ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3331\n",
            "Epoch 20, Loss: 0.2046\n",
            "Epoch 40, Loss: 0.0684\n",
            "Epoch 60, Loss: 0.0364\n",
            "Epoch 80, Loss: 0.0379\n",
            "Epoch 100, Loss: 0.0347\n",
            "Epoch 120, Loss: 0.0330\n",
            "Epoch 140, Loss: 0.0318\n",
            "Epoch 160, Loss: 0.0309\n",
            "Epoch 180, Loss: 0.0303\n",
            "Test Loss: 0.0728\n",
            "Test Accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square(Z3)\n",
        "\n",
        "\n",
        "    Z4 = np.dot(A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    dA3 = np.dot(dZ4, params['W4'].T)\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated = np.dot(dZ3, params['W3'].T)\n",
        "    dA1, dA2 = d_concatenated[:, :hidden_size1], d_concatenated[:, hidden_size1:]\n",
        "\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = dA2 * square_derivative(Z2)\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]  # Convert to one-hot encoding\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "# Convert predictions to one-hot encoded format\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "\n",
        "# Compute accuracy using one-hot encoded predictions\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "ct7XnKWhcBTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c1d7ef-e882-4f12-d87c-b52669a81ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3337\n",
            "Epoch 20, Loss: 0.2220\n",
            "Epoch 40, Loss: 0.2210\n",
            "Epoch 60, Loss: 0.2001\n",
            "Epoch 80, Loss: 0.1148\n",
            "Epoch 100, Loss: 0.1125\n",
            "Epoch 120, Loss: 0.1042\n",
            "Epoch 140, Loss: 0.0653\n",
            "Epoch 160, Loss: 0.0712\n",
            "Epoch 180, Loss: 0.0715\n",
            "Test Loss: 0.0688\n",
            "Test Accuracy: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3, output_size) * 0.1,  # Corrected output size\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)  # Concatenate A3 and concatenated\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)  # Update using concatenated A3\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "    d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_Z2 = dA2 * square_derivative(Z2)\n",
        "    dA1, dA2 = d_concatenated[:, :hidden_size1], d_concatenated[:, hidden_size1:]\n",
        "\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = d_concatenated_Z2\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]  # Convert to one-hot encoding\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "# Convert predictions to one-hot encoded format\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "\n",
        "# Compute accuracy using one-hot encoded predictions\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "UGVaHyh_eS4r",
        "outputId": "0a3ba889-6596-4382-a336-48398327d2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (120,15) and (5,3) not aligned: 15 (dim 1) != 5 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-866342005de0>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-866342005de0>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(X, params)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mconcatenated_A3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Concatenate A3 and concatenated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mZ4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated_A3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mA4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Linear activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (120,15) and (5,3) not aligned: 15 (dim 1) != 5 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "    d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_Z2 = dA2 * square_derivative(Z2)\n",
        "    dA1, dA2 = d_concatenated[:, :hidden_size1], d_concatenated[:, hidden_size1:]\n",
        "\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = d_concatenated_Z2[:, hidden_size1:]  # Update dZ2 calculation\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "kBeosOoOhGoy",
        "outputId": "5da7fad2-700d-4b0b-e747-2c13593da487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'dA2' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-7bc2605f96a5>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-7bc2605f96a5>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(y_true, cache, params)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mdb3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0md_concatenated_Z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msquare_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mdA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_concatenated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mhidden_size1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_concatenated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'dA2' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)/4\n",
        "\n",
        "def square_(x):\n",
        "    return np.power(x, 2)/24\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square_(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "    d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = d_concatenated[:, hidden_size1:]  # Corrected dA2 calculation\n",
        "    d_concatenated_Z2 = dA2 * square_derivative(Z2)\n",
        "\n",
        "    dA1 = d_concatenated[:, :hidden_size1]\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = d_concatenated_Z2\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxIbYV68iOcK",
        "outputId": "7d946cf3-a799-4c4e-a867-ac2e862ace0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3420\n",
            "Epoch 20, Loss: 0.0969\n",
            "Epoch 40, Loss: 0.0639\n",
            "Epoch 60, Loss: 0.0479\n",
            "Epoch 80, Loss: 0.0399\n",
            "Epoch 100, Loss: 0.0364\n",
            "Epoch 120, Loss: 0.0346\n",
            "Epoch 140, Loss: 0.0334\n",
            "Epoch 160, Loss: 0.0491\n",
            "Epoch 180, Loss: 0.0484\n",
            "Test Loss: 0.0561\n",
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)/4\n",
        "\n",
        "def square_(x):\n",
        "    return np.power(x, 2)/24\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square_(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "    d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = d_concatenated[:, hidden_size1:]  # Corrected dA2 calculation\n",
        "    d_concatenated_Z2 = dA2 * square_derivative(Z2)\n",
        "\n",
        "    dA1 = d_concatenated[:, :hidden_size1]\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = d_concatenated_Z2\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Breast Cancer Wisconsin dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(2)[y]  # One-hot encode target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 2  # Two classes: benign and malignant\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.00001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "fFpUX6rCBB6z",
        "outputId": "73c9c727-f0eb-41a0-a12c-ce032ffaca98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.5341\n",
            "Epoch 20, Loss: 0.3618\n",
            "Epoch 40, Loss: 0.2569\n",
            "Epoch 60, Loss: 0.1899\n",
            "Epoch 80, Loss: 0.1477\n",
            "Epoch 100, Loss: 0.1213\n",
            "Epoch 120, Loss: 0.1044\n",
            "Epoch 140, Loss: 0.0933\n",
            "Epoch 160, Loss: 0.0859\n",
            "Epoch 180, Loss: 0.0809\n",
            "Test Loss: 0.0752\n",
            "Test Accuracy: 0.9474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "        self.params = {\n",
        "            'W1': np.random.randn(input_size, hidden_size1) * 0.01,\n",
        "            'b1': np.zeros((1, hidden_size1)),\n",
        "            'W2': np.random.randn(input_size, hidden_size2) * 0.01,\n",
        "            'b2': np.zeros((1, hidden_size2)),\n",
        "            'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.01,\n",
        "            'b3': np.zeros((1, hidden_size3)),\n",
        "            'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.01,\n",
        "            'b4': np.zeros((1, output_size))\n",
        "        }\n",
        "\n",
        "    def activation_identity(self, x):\n",
        "        return x\n",
        "\n",
        "    def activation_square(self, x):\n",
        "        return np.power(x, 2) / 4\n",
        "\n",
        "    def activation_square_(self, x):\n",
        "        return np.power(x, 2) / 24\n",
        "\n",
        "    def derivative_identity(self, x):\n",
        "        return np.ones_like(x)\n",
        "\n",
        "    def derivative_square(self, x):\n",
        "        return 2 * x\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        Z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
        "        A1 = self.activation_identity(Z1)\n",
        "\n",
        "        Z2 = np.dot(X, self.params['W2']) + self.params['b2']\n",
        "        A2 = self.activation_square(Z2)\n",
        "\n",
        "        concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "        Z3 = np.dot(concatenated, self.params['W3']) + self.params['b3']\n",
        "        A3 = self.activation_square_(Z3)\n",
        "\n",
        "        concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "        Z4 = np.dot(concatenated_A3, self.params['W4']) + self.params['b4']\n",
        "        A4 = self.activation_identity(Z4)  # Linear activation\n",
        "\n",
        "        return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "    def backward_pass(self, y_true, cache):\n",
        "        X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "        dA4 = 2 * (A4 - y_true)\n",
        "        dZ4 = dA4 * self.derivative_identity(Z4)\n",
        "        dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "        db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "        d_concatenated_A3 = np.dot(dZ4, self.params['W4'].T)\n",
        "        dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "        d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "        dZ3 = dA3 * self.derivative_square(Z3)\n",
        "        dW3 = np.dot(concatenated.T, dZ3)\n",
        "        db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "        dA2 = d_concatenated[:, hidden_size1:]\n",
        "        dZ2 = dA2 * self.derivative_square(Z2)\n",
        "        dW2 = np.dot(X.T, dZ2)\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "        dA1 = d_concatenated[:, :hidden_size1]\n",
        "        dZ1 = dA1 * self.derivative_identity(Z1)\n",
        "        dW1 = np.dot(X.T, dZ1)\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "        grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "        return grads\n",
        "\n",
        "    def update_parameters(self, grads, learning_rate):\n",
        "        for key in self.params.keys():\n",
        "            self.params[key] -= learning_rate * grads['d' + key]\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    X, y = breast_cancer.data, breast_cancer.target\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    Y_onehot = np.eye(2)[y]\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "    input_size = X_train.shape[1]\n",
        "    hidden_size1 = 5\n",
        "    hidden_size2 = 5\n",
        "    hidden_size3 = 5\n",
        "    output_size = 2\n",
        "    # Start the timer\n",
        "    start_time = time.time()\n",
        "    nn = NeuralNetwork(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "    epochs = 5\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        output, cache = nn.forward_pass(X_train)\n",
        "        loss = nn.compute_loss(Y_train, output)\n",
        "        grads = nn.backward_pass(Y_train, cache)\n",
        "        nn.update_parameters(grads, learning_rate)\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the execution time\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    output_test, _ = nn.forward_pass(X_test)\n",
        "    test_loss = nn.compute_loss(Y_test, output_test)\n",
        "    predictions = np.argmax(output_test, axis=1)\n",
        "    predictions_onehot = np.eye(output_size)[predictions]\n",
        "    accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "e6ps3YNYVIhl",
        "outputId": "88b49bc3-5b30-4820-b5e7-20c9a5d7ff7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.5005\n",
            "Execution Time: 0.01 seconds\n",
            "Test Loss: 0.3344\n",
            "Test Accuracy: 0.6228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "# Define the neural network model\n",
        "model = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2 ,verbose=0)\n",
        "\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "daIoOm46ZuBN",
        "outputId": "960bc52a-be81-4e0f-9eef-63c6978b5b69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 11.58 seconds\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9737\n",
            "Test Loss: 0.0903\n",
            "Test Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "\n",
        "def initialize_parameters(input_size, hidden_size, output_size):\n",
        "    np.random.seed(42)\n",
        "    parameters = {\n",
        "        'W1': np.random.randn(input_size, hidden_size) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size)),\n",
        "        'W2': np.random.randn(hidden_size, output_size) * 0.1,\n",
        "        'b2': np.zeros((1, output_size))\n",
        "    }\n",
        "    return parameters\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
        "\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
        "    return A2, cache\n",
        "\n",
        "def compute_cost(A2, Y):\n",
        "    m = Y.shape[0]\n",
        "    cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n",
        "    return cost\n",
        "\n",
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    m = X.shape[0]\n",
        "    W1, W2 = parameters['W1'], parameters['W2']\n",
        "    A1, A2 = cache['A1'], cache['A2']\n",
        "\n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "    dZ1 = np.dot(dZ2, W2.T) * (1 - np.power(A1, 2))\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
        "    return grads\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    parameters['W1'] -= learning_rate * grads['dW1']\n",
        "    parameters['b1'] -= learning_rate * grads['db1']\n",
        "    parameters['W2'] -= learning_rate * grads['dW2']\n",
        "    parameters['b2'] -= learning_rate * grads['db2']\n",
        "    return parameters\n",
        "\n",
        "def model(X_train, Y_train, hidden_size, num_iterations, learning_rate):\n",
        "    np.random.seed(42)\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = Y_train.shape[1]\n",
        "\n",
        "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        A2, cache = forward_propagation(X_train, parameters)\n",
        "        cost = compute_cost(A2, Y_train)\n",
        "        grads = backward_propagation(parameters, cache, X_train, Y_train)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Cost after iteration {i}: {cost}\")\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y = y.reshape(-1, 1)  # Reshape y to be a 2D array\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "# Train the model\n",
        "hidden_size = 10\n",
        "num_iterations = 1000\n",
        "learning_rate = 0.1\n",
        "parameters = model(X_train, y_train, hidden_size, num_iterations, learning_rate)\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "# Predictions\n",
        "A2, _ = forward_propagation(X_test, parameters)\n",
        "predictions = (A2 > 0.5).astype(int)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "bO4QparXB4fs",
        "outputId": "c7cceccf-2b6d-4dd4-b60a-ddfb336915f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.6971116687344172\n",
            "Execution Time: 2.32 seconds\n",
            "Test Accuracy: 98.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "id": "1yBr-q2KI0hP",
        "outputId": "3ee5bfaa-129c-4dcc-b29f-a6b3508aee0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Embedding, Dense, concatenate, Dropout, Flatten, Activation\n",
        "from keras import backend as K\n",
        "from keras.utils import get_custom_objects\n",
        "from keras.utils import  to_categorical, plot_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.datasets import mnist"
      ],
      "metadata": {
        "id": "_HzLEJIIIxGs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 4\n",
        "\n",
        "# Load the MNIST data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the input data\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "# Convert input data to float32\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Add 10 to input data\n",
        "x_train += 10\n",
        "x_test += 10\n",
        "\n",
        "# Normalize input data\n",
        "x_train /= 300\n",
        "x_test /= 300\n",
        "\n",
        "# Print the number of train and test samples\n",
        "print(f'{x_train.shape[0]} train samples')\n",
        "print(f'{x_test.shape[0]} test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Set the number of train and test samples\n",
        "num_train_samples = 60000\n",
        "num_test_samples = 10000\n",
        "\n",
        "# Select the required number of train and test samples\n",
        "x_train = x_train[:num_train_samples, :]\n",
        "x_test = x_test[:num_test_samples, :]\n",
        "\n",
        "y_train = y_train[:num_train_samples]\n",
        "y_test = y_test[:num_test_samples]"
      ],
      "metadata": {
        "id": "0HTTM618Ijiw",
        "outputId": "a12c923c-832f-41f7-c21a-de825b290022",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Constants\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 40\n",
        "\n",
        "# Load the MNIST data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the input data\n",
        "x_train = x_train.reshape(x_train.shape[0], 784)\n",
        "x_test = x_test.reshape(x_test.shape[0], 784)\n",
        "\n",
        "# Convert input data to float32\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Add 10 to input data\n",
        "x_train += 10\n",
        "x_test += 10\n",
        "\n",
        "# Normalize input data\n",
        "x_train /= 300\n",
        "x_test /= 300\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_size = 784\n",
        "    hidden_size1 = 500\n",
        "    hidden_size2 = 120\n",
        "    hidden_size3 = 32\n",
        "    output_size = num_classes\n",
        "\n",
        "    nn = NeuralNetwork(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "    learning_rate = 0.000001\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        output, cache = nn.forward_pass(x_train)\n",
        "        loss = nn.compute_loss(y_train, output)\n",
        "        grads = nn.backward_pass(y_train, cache)\n",
        "        nn.update_parameters(grads, learning_rate)\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    output_test, _ = nn.forward_pass(x_test)\n",
        "    test_loss = nn.compute_loss(y_test, output_test)\n",
        "    predictions = np.argmax(output_test, axis=1)\n",
        "    accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "ukOmfWoqIAh2",
        "outputId": "f259af29-1d51-41db-91c8-7777618cd75f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.1010\n",
            "Epoch 1, Loss: 0.0912\n",
            "Epoch 2, Loss: 0.0874\n",
            "Epoch 3, Loss: 0.0852\n",
            "Epoch 4, Loss: 0.0835\n",
            "Epoch 5, Loss: 0.0820\n",
            "Epoch 6, Loss: 0.0805\n",
            "Epoch 7, Loss: 0.0790\n",
            "Epoch 8, Loss: 0.0776\n",
            "Epoch 9, Loss: 0.0762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwg4ZvvTLUyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}