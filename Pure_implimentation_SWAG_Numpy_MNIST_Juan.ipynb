{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx8H+nNY0UU4exLNto+XWt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningSaeid/Grad/blob/main/Pure_implimentation_SWAG_Numpy_MNIST_Juan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)/4\n",
        "\n",
        "def square_(x):\n",
        "    return np.power(x, 2)/24\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square_(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "    d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = d_concatenated[:, hidden_size1:]  # Corrected dA2 calculation\n",
        "    d_concatenated_Z2 = dA2 * square_derivative(Z2)\n",
        "\n",
        "    dA1 = d_concatenated[:, :hidden_size1]\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = d_concatenated_Z2\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxIbYV68iOcK",
        "outputId": "117e2f89-a4cd-4a4f-c44b-3186ac852385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3247\n",
            "Epoch 20, Loss: 0.0691\n",
            "Epoch 40, Loss: 0.0475\n",
            "Epoch 60, Loss: 0.0390\n",
            "Epoch 80, Loss: 0.0357\n",
            "Epoch 100, Loss: 0.0340\n",
            "Epoch 120, Loss: 0.0331\n",
            "Epoch 140, Loss: 0.0635\n",
            "Epoch 160, Loss: 0.0568\n",
            "Epoch 180, Loss: 0.0546\n",
            "Test Loss: 0.0550\n",
            "Test Accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)/4\n",
        "\n",
        "def square_(x):\n",
        "    return np.power(x, 2)/24\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square_(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "    d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = d_concatenated[:, hidden_size1:]  # Corrected dA2 calculation\n",
        "    d_concatenated_Z2 = dA2 * square_derivative(Z2)\n",
        "\n",
        "    dA1 = d_concatenated[:, :hidden_size1]\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = d_concatenated_Z2\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Breast Cancer Wisconsin dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(2)[y]  # One-hot encode target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 2  # Two classes: benign and malignant\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.00001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "fFpUX6rCBB6z",
        "outputId": "4f97d5c4-9f61-4dc1-966e-4ce24f31a810",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.5283\n",
            "Epoch 20, Loss: 0.4137\n",
            "Epoch 40, Loss: 0.3322\n",
            "Epoch 60, Loss: 0.2670\n",
            "Epoch 80, Loss: 0.2122\n",
            "Epoch 100, Loss: 0.1678\n",
            "Epoch 120, Loss: 0.1353\n",
            "Epoch 140, Loss: 0.1135\n",
            "Epoch 160, Loss: 0.0991\n",
            "Epoch 180, Loss: 0.0894\n",
            "Test Loss: 0.0797\n",
            "Test Accuracy: 0.9386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "        self.params = {\n",
        "            'W1': np.random.randn(input_size, hidden_size1) * 0.01,\n",
        "            'b1': np.zeros((1, hidden_size1)),\n",
        "            'W2': np.random.randn(input_size, hidden_size2) * 0.01,\n",
        "            'b2': np.zeros((1, hidden_size2)),\n",
        "            'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.01,\n",
        "            'b3': np.zeros((1, hidden_size3)),\n",
        "            'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.01,\n",
        "            'b4': np.zeros((1, output_size))\n",
        "        }\n",
        "\n",
        "    def activation_identity(self, x):\n",
        "        return x\n",
        "\n",
        "    def activation_square(self, x):\n",
        "        return np.power(x, 2) / 4\n",
        "\n",
        "    def activation_square_(self, x):\n",
        "        return np.power(x, 2) / 24\n",
        "\n",
        "    def derivative_identity(self, x):\n",
        "        return np.ones_like(x)\n",
        "\n",
        "    def derivative_square(self, x):\n",
        "        return 2 * x\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        Z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
        "        A1 = self.activation_identity(Z1)\n",
        "\n",
        "        Z2 = np.dot(X, self.params['W2']) + self.params['b2']\n",
        "        A2 = self.activation_square(Z2)\n",
        "\n",
        "        concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "        Z3 = np.dot(concatenated, self.params['W3']) + self.params['b3']\n",
        "        A3 = self.activation_square_(Z3)\n",
        "\n",
        "        concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "        Z4 = np.dot(concatenated_A3, self.params['W4']) + self.params['b4']\n",
        "        A4 = self.activation_identity(Z4)  # Linear activation\n",
        "\n",
        "        return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "    def backward_pass(self, y_true, cache):\n",
        "        X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "        dA4 = 2 * (A4 - y_true)\n",
        "        dZ4 = dA4 * self.derivative_identity(Z4)\n",
        "        dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "        db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "        d_concatenated_A3 = np.dot(dZ4, self.params['W4'].T)\n",
        "        dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "        d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "        dZ3 = dA3 * self.derivative_square(Z3)\n",
        "        dW3 = np.dot(concatenated.T, dZ3)\n",
        "        db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "        dA2 = d_concatenated[:, hidden_size1:]\n",
        "        dZ2 = dA2 * self.derivative_square(Z2)\n",
        "        dW2 = np.dot(X.T, dZ2)\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "        dA1 = d_concatenated[:, :hidden_size1]\n",
        "        dZ1 = dA1 * self.derivative_identity(Z1)\n",
        "        dW1 = np.dot(X.T, dZ1)\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "        grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "        return grads\n",
        "\n",
        "    def update_parameters(self, grads, learning_rate):\n",
        "        for key in self.params.keys():\n",
        "            self.params[key] -= learning_rate * grads['d' + key]\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    X, y = breast_cancer.data, breast_cancer.target\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    Y_onehot = np.eye(2)[y]\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "    input_size = X_train.shape[1]\n",
        "    hidden_size1 = 5\n",
        "    hidden_size2 = 5\n",
        "    hidden_size3 = 5\n",
        "    output_size = 2\n",
        "    # Start the timer\n",
        "    start_time = time.time()\n",
        "    nn = NeuralNetwork(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "    epochs = 5\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        output, cache = nn.forward_pass(X_train)\n",
        "        loss = nn.compute_loss(Y_train, output)\n",
        "        grads = nn.backward_pass(Y_train, cache)\n",
        "        nn.update_parameters(grads, learning_rate)\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the execution time\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    output_test, _ = nn.forward_pass(X_test)\n",
        "    test_loss = nn.compute_loss(Y_test, output_test)\n",
        "    predictions = np.argmax(output_test, axis=1)\n",
        "    predictions_onehot = np.eye(output_size)[predictions]\n",
        "    accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "e6ps3YNYVIhl",
        "outputId": "88b49bc3-5b30-4820-b5e7-20c9a5d7ff7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.5005\n",
            "Execution Time: 0.01 seconds\n",
            "Test Loss: 0.3344\n",
            "Test Accuracy: 0.6228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "        glorot_factor = np.sqrt(6.0 / (input_size + hidden_size1))  # Glorot Uniform initialization factor\n",
        "        self.params = {\n",
        "            'W1': np.random.uniform(-glorot_factor, glorot_factor, size=(input_size, hidden_size1)),\n",
        "            'b1': np.zeros((1, hidden_size1)),\n",
        "            'W2': np.random.uniform(-glorot_factor, glorot_factor, size=(input_size, hidden_size2)),\n",
        "            'b2': np.zeros((1, hidden_size2)),\n",
        "            'W3': np.random.uniform(-glorot_factor, glorot_factor, size=(hidden_size1 + hidden_size2, hidden_size3)),\n",
        "            'b3': np.zeros((1, hidden_size3)),\n",
        "            'W4': np.random.uniform(-glorot_factor, glorot_factor, size=(hidden_size3 + hidden_size1 + hidden_size2, output_size)),\n",
        "            'b4': np.zeros((1, output_size))\n",
        "        }\n",
        "\n",
        "    def activation_identity(self, x):\n",
        "        return x\n",
        "\n",
        "    def activation_square(self, x):\n",
        "        return np.power(x, 2) / 4\n",
        "\n",
        "    def activation_square_(self, x):\n",
        "        return np.power(x, 2) / 24\n",
        "\n",
        "    def derivative_identity(self, x):\n",
        "        return np.ones_like(x)\n",
        "\n",
        "    def derivative_square(self, x):\n",
        "        return 2 * x\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        Z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
        "        A1 = self.activation_identity(Z1)\n",
        "\n",
        "        Z2 = np.dot(X, self.params['W2']) + self.params['b2']\n",
        "        A2 = self.activation_square(Z2)\n",
        "\n",
        "        concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "        Z3 = np.dot(concatenated, self.params['W3']) + self.params['b3']\n",
        "        A3 = self.activation_square_(Z3)\n",
        "\n",
        "        concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "        Z4 = np.dot(concatenated_A3, self.params['W4']) + self.params['b4']\n",
        "        A4 = self.activation_identity(Z4)  # Linear activation\n",
        "\n",
        "        return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "    def backward_pass(self, y_true, cache):\n",
        "        X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "        dA4 = 2 * (A4 - y_true)\n",
        "        dZ4 = dA4 * self.derivative_identity(Z4)\n",
        "        dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "        db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "        d_concatenated_A3 = np.dot(dZ4, self.params['W4'].T)\n",
        "        dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "        d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "        dZ3 = dA3 * self.derivative_square(Z3)\n",
        "        dW3 = np.dot(concatenated.T, dZ3)\n",
        "        db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "        dA2 = d_concatenated[:, hidden_size1:]\n",
        "        dZ2 = dA2 * self.derivative_square(Z2)\n",
        "        dW2 = np.dot(X.T, dZ2)\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "        dA1 = d_concatenated[:, :hidden_size1]\n",
        "        dZ1 = dA1 * self.derivative_identity(Z1)\n",
        "        dW1 = np.dot(X.T, dZ1)\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "        grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "        return grads\n",
        "\n",
        "    def update_parameters(self, grads, learning_rate):\n",
        "        for key in self.params.keys():\n",
        "            self.params[key] -= learning_rate * grads['d' + key]\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    breast_cancer = load_breast_cancer()\n",
        "    X, y = breast_cancer.data, breast_cancer.target\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    Y_onehot = np.eye(2)[y]\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "    input_size = X_train.shape[1]\n",
        "    hidden_size1 = 5\n",
        "    hidden_size2 = 5\n",
        "    hidden_size3 = 5\n",
        "    output_size = 2\n",
        "    # Start the timer\n",
        "    start_time = time.time()\n",
        "    nn = NeuralNetwork(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "    epochs = 5\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        output, cache = nn.forward_pass(X_train)\n",
        "        loss = nn.compute_loss(Y_train, output)\n",
        "        grads = nn.backward_pass(Y_train, cache)\n",
        "        nn.update_parameters(grads, learning_rate)\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the execution time\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    output_test, _ = nn.forward_pass(X_test)\n",
        "    test_loss = nn.compute_loss(Y_test, output_test)\n",
        "    predictions = np.argmax(output_test, axis=1)\n",
        "    predictions_onehot = np.eye(output_size)[predictions]\n",
        "    accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG5nY0TgQwsv",
        "outputId": "4f854c75-f309-445d-ceb8-29b54c82f295"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.4084\n",
            "Execution Time: 0.01 seconds\n",
            "Test Loss: 0.1332\n",
            "Test Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Embedding, Dense, concatenate, Dropout, Flatten, Activation\n",
        "from keras import backend as K\n",
        "from keras.utils import get_custom_objects\n",
        "from keras.utils import  to_categorical, plot_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.datasets import mnist"
      ],
      "metadata": {
        "id": "_HzLEJIIIxGs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 4\n",
        "\n",
        "# Load the MNIST data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the input data\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "# Convert input data to float32\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Add 10 to input data\n",
        "x_train += 10\n",
        "x_test += 10\n",
        "\n",
        "# Normalize input data\n",
        "x_train /= 300\n",
        "x_test /= 300\n",
        "\n",
        "# Print the number of train and test samples\n",
        "print(f'{x_train.shape[0]} train samples')\n",
        "print(f'{x_test.shape[0]} test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Set the number of train and test samples\n",
        "num_train_samples = 60000\n",
        "num_test_samples = 10000\n",
        "\n",
        "# Select the required number of train and test samples\n",
        "x_train = x_train[:num_train_samples, :]\n",
        "x_test = x_test[:num_test_samples, :]\n",
        "\n",
        "y_train = y_train[:num_train_samples]\n",
        "y_test = y_test[:num_test_samples]"
      ],
      "metadata": {
        "id": "0HTTM618Ijiw",
        "outputId": "3ca31dea-75e2-4d17-b52e-e6a5bdd706e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Constants\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 40\n",
        "\n",
        "# Load the MNIST data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the input data\n",
        "x_train = x_train.reshape(x_train.shape[0], 784)\n",
        "x_test = x_test.reshape(x_test.shape[0], 784)\n",
        "\n",
        "# Convert input data to float32\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Add 10 to input data\n",
        "x_train += 10\n",
        "x_test += 10\n",
        "\n",
        "# Normalize input data\n",
        "x_train /= 500\n",
        "x_test /= 500\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_size = 784\n",
        "    hidden_size1 = 300\n",
        "    hidden_size2 = 300\n",
        "    hidden_size3 = 300\n",
        "    output_size = num_classes\n",
        "\n",
        "    nn = NeuralNetwork(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "    learning_rate = 0.000001\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        output, cache = nn.forward_pass(x_train)\n",
        "        loss = nn.compute_loss(y_train, output)\n",
        "        grads = nn.backward_pass(y_train, cache)\n",
        "        nn.update_parameters(grads, learning_rate)\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    output_test, _ = nn.forward_pass(x_test)\n",
        "    test_loss = nn.compute_loss(y_test, output_test)\n",
        "    predictions = np.argmax(output_test, axis=1)\n",
        "    accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "ukOmfWoqIAh2",
        "outputId": "77cf27de-7c0a-4afb-e22a-71d3622ee79d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 0, Loss: 0.1212\n",
            "Epoch 1, Loss: 0.1130\n",
            "Epoch 2, Loss: 0.1051\n",
            "Epoch 3, Loss: 0.0950\n",
            "Epoch 4, Loss: 0.0869\n",
            "Epoch 5, Loss: 0.0797\n",
            "Epoch 6, Loss: 0.0748\n",
            "Epoch 7, Loss: 0.0711\n",
            "Epoch 8, Loss: 0.0685\n",
            "Epoch 9, Loss: 0.0664\n",
            "Epoch 10, Loss: 0.0647\n",
            "Epoch 11, Loss: 0.0633\n",
            "Epoch 12, Loss: 0.0620\n",
            "Epoch 13, Loss: 0.0610\n",
            "Epoch 14, Loss: 0.0600\n",
            "Epoch 15, Loss: 0.0592\n",
            "Epoch 16, Loss: 0.0584\n",
            "Epoch 17, Loss: 0.0577\n",
            "Epoch 18, Loss: 0.0570\n",
            "Epoch 19, Loss: 0.0564\n",
            "Epoch 20, Loss: 0.0559\n",
            "Epoch 21, Loss: 0.0553\n",
            "Epoch 22, Loss: 0.0548\n",
            "Epoch 23, Loss: 0.0544\n",
            "Epoch 24, Loss: 0.0540\n",
            "Epoch 25, Loss: 0.0536\n",
            "Epoch 26, Loss: 0.0532\n",
            "Epoch 27, Loss: 0.0528\n",
            "Epoch 28, Loss: 0.0525\n",
            "Epoch 29, Loss: 0.0522\n",
            "Epoch 30, Loss: 0.0519\n",
            "Epoch 31, Loss: 0.0516\n",
            "Epoch 32, Loss: 0.0513\n",
            "Epoch 33, Loss: 0.0510\n",
            "Epoch 34, Loss: 0.0508\n",
            "Epoch 35, Loss: 0.0505\n",
            "Epoch 36, Loss: 0.0503\n",
            "Epoch 37, Loss: 0.0501\n",
            "Epoch 38, Loss: 0.0499\n",
            "Epoch 39, Loss: 0.0497\n",
            "Execution Time: 352.46 seconds\n",
            "Test Loss: 0.0488\n",
            "Test Accuracy: 0.8100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L_fVP_LyZusS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}