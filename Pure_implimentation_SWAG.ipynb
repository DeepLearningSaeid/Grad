{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPvqjjGm7F7uwc+12fMrZd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningSaeid/Grad/blob/main/Pure_implimentation_SWAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrjnEKdTVr4G",
        "outputId": "42d2131b-ae52-401e-e60f-5e12e60f54ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 0.16\n",
            "Average MSE: 0.31\n",
            "Model: \"model_54\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)       [(None, 8)]                  0         []                            \n",
            "                                                                                                  \n",
            " dense_310 (Dense)           (None, 50)                   450       ['input_15[0][0]']            \n",
            "                                                                                                  \n",
            " dense_311 (Dense)           (None, 50)                   450       ['input_15[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_118 (Concatena  (None, 100)                  0         ['dense_310[0][0]',           \n",
            " te)                                                                 'dense_311[0][0]']           \n",
            "                                                                                                  \n",
            " dense_312 (Dense)           (None, 50)                   5050      ['concatenate_118[0][0]']     \n",
            "                                                                                                  \n",
            " dense_313 (Dense)           (None, 50)                   2550      ['dense_312[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_119 (Concatena  (None, 200)                  0         ['dense_310[0][0]',           \n",
            " te)                                                                 'dense_311[0][0]',           \n",
            "                                                                     'dense_312[0][0]',           \n",
            "                                                                     'dense_313[0][0]']           \n",
            "                                                                                                  \n",
            " dense_314 (Dense)           (None, 1)                    201       ['concatenate_119[0][0]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8701 (33.99 KB)\n",
            "Trainable params: 8701 (33.99 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import Input, Dense, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import Activation\n",
        "import keras.backend as K\n",
        "\n",
        "# Load Pima Indians Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop('class', axis=1).values\n",
        "Y = data['class'].values\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "def define_activation_functions():\n",
        "    activations = [\n",
        "        ('X_1', lambda x: K.pow(x, 1)),\n",
        "        ('X_2', lambda x: K.pow(x, 2) / 2),\n",
        "        ('X_2_', lambda x: K.pow(x, 2) / 24),\n",
        "        ('X_2__', lambda x: K.pow(x, 2) / 720),\n",
        "    ]\n",
        "    for name, func in activations:\n",
        "        get_custom_objects().update({name: Activation(func)})\n",
        "\n",
        "define_activation_functions()\n",
        "\n",
        "def create_optimized_model(input_dim, hidden_dim, output_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    x1 = Dense(hidden_dim, activation='X_1')(input_layer)\n",
        "    x2 = Dense(hidden_dim, activation='X_2')(input_layer)\n",
        "    merged1 = concatenate([x1, x2])\n",
        "    x3 = Dense(hidden_dim, activation='X_2_')(merged1)\n",
        "    x4 = Dense(hidden_dim, activation='X_2__')(x3)\n",
        "    merged2 = concatenate([x1, x2, x3, x4])\n",
        "    output = Dense(output_dim, activation='X_1')(merged2)\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "    return model\n",
        "\n",
        "input_dim = 8\n",
        "hidden_dim = 50\n",
        "output_dim = 1\n",
        "n_folds = 10\n",
        "\n",
        "# Initialize Stratified K-Fold\n",
        "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store performance metrics for each fold\n",
        "accuracy_scores = []\n",
        "mse_scores = []\n",
        "\n",
        "# Perform cross-validation\n",
        "for train_index, val_index in kf.split(X, Y):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = create_optimized_model(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss, val_mse = model.evaluate(X_val, Y_val, verbose=0)\n",
        "\n",
        "    # Calculate Mean Squared Error (MSE)\n",
        "    Y_val_pred = model.predict(X_val)\n",
        "    mse = np.mean((Y_val - Y_val_pred)**2)\n",
        "\n",
        "    # Append accuracy and MSE to lists\n",
        "    accuracy_scores.append(val_loss)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Calculate and print the average performance metrics over all folds\n",
        "average_loss = np.mean(accuracy_scores)\n",
        "average_mse = np.mean(mse_scores)\n",
        "\n",
        "print(f'Average Loss: {average_loss:.2f}')\n",
        "print(f'Average MSE: {average_mse:.2f}')\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, concatenate, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class CustomNeuralNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.model = self._create_model()\n",
        "\n",
        "    def _define_activation_functions(self):\n",
        "        activations = [\n",
        "            ('X_1', lambda x: x),  # Identity function for X_1\n",
        "            ('X_2', lambda x: tf.pow(x, 2) / 2),\n",
        "            ('X_2_', lambda x: tf.pow(x, 2) / 24),\n",
        "            ('X_2__', lambda x: tf.pow(x, 2) / 720),\n",
        "        ]\n",
        "        for name, func in activations:\n",
        "            tf.keras.utils.get_custom_objects()[name] = Activation(func)\n",
        "\n",
        "    def _create_model(self):\n",
        "        self._define_activation_functions()\n",
        "\n",
        "        input_layer = Input(shape=(self.input_dim,))\n",
        "        x1 = Dense(self.hidden_dim, activation='X_1')(input_layer)\n",
        "        x2 = Dense(self.hidden_dim, activation='X_2')(input_layer)\n",
        "        merged1 = concatenate([x1, x2])\n",
        "        x3 = Dense(self.hidden_dim, activation='X_2_')(merged1)\n",
        "        x4 = Dense(self.hidden_dim, activation='X_2__')(x3)\n",
        "        merged2 = concatenate([x1, x2, x3, x4])\n",
        "        output = Dense(self.output_dim, activation='X_1')(merged2)\n",
        "\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.model.summary()\n",
        "\n",
        "    def fit(self, x, y, **kwargs):\n",
        "        return self.model.fit(x, y, **kwargs)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.model.predict(x)\n",
        "\n",
        "# Example usage:\n",
        "# custom_nn = CustomNeuralNetwork(input_dim, hidden_dim, output_dim)\n",
        "# custom_nn.summary()  # Display model summary\n",
        "# custom_nn.fit(x_train, y_train, epochs=100, batch_size=32)  # Train the model\n",
        "# predictions = custom_nn.predict(x_test)  # Make predictions\n"
      ],
      "metadata": {
        "id": "wtUHab74e5v1"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsELz_UncFCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LYJgY32OJcwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fph6lAJCJcrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the architecture\n",
        "input_size = 2\n",
        "hidden1_size = 4\n",
        "hidden2_size = 3\n",
        "hidden3_size = 2\n",
        "output_size = 1\n",
        "\n",
        "# Define activation functions (sigmoid in this case)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Define training parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Generate random input data and corresponding target values\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, input_size)\n",
        "y = np.random.randint(2, size=(100, output_size))\n",
        "\n",
        "# Initialize weights and biases\n",
        "weights = {\n",
        "    'W1': np.random.randn(input_size, hidden1_size),\n",
        "    'W2': np.random.randn(hidden1_size, hidden2_size),\n",
        "    'W3': np.random.randn(hidden2_size, hidden3_size),\n",
        "    'W4': np.random.randn(hidden3_size, output_size)\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b1': np.zeros((1, hidden1_size)),\n",
        "    'b2': np.zeros((1, hidden2_size)),\n",
        "    'b3': np.zeros((1, hidden3_size)),\n",
        "    'b4': np.zeros((1, output_size))\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, weights['W1']) + biases['b1']\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    z2 = np.dot(a1, weights['W2']) + biases['b2']\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    z3 = np.dot(a2, weights['W3']) + biases['b3']\n",
        "    a3 = sigmoid(z3)\n",
        "\n",
        "    z4 = np.dot(a3, weights['W4']) + biases['b4']\n",
        "    output = sigmoid(z4)\n",
        "\n",
        "    # Calculate the loss (mean squared error)\n",
        "    loss = np.mean((output - y) ** 2)\n",
        "\n",
        "    # Backward pass\n",
        "    delta4 = 2 * (output - y) * sigmoid_derivative(output)\n",
        "    dW4 = np.dot(a3.T, delta4)\n",
        "    db4 = np.sum(delta4, axis=0, keepdims=True)\n",
        "\n",
        "    delta3 = np.dot(delta4, weights['W4'].T) * sigmoid_derivative(a3)\n",
        "    dW3 = np.dot(a2.T, delta3)\n",
        "    db3 = np.sum(delta3, axis=0, keepdims=True)\n",
        "\n",
        "    delta2 = np.dot(delta3, weights['W3'].T) * sigmoid_derivative(a2)\n",
        "    dW2 = np.dot(a1.T, delta2)\n",
        "    db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "\n",
        "    delta1 = np.dot(delta2, weights['W2'].T) * sigmoid_derivative(a1)\n",
        "    dW1 = np.dot(X.T, delta1)\n",
        "    db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights and biases\n",
        "    weights['W4'] -= learning_rate * dW4\n",
        "    biases['b4'] -= learning_rate * db4\n",
        "    weights['W3'] -= learning_rate * dW3\n",
        "    biases['b3'] -= learning_rate * db3\n",
        "    weights['W2'] -= learning_rate * dW2\n",
        "    biases['b2'] -= learning_rate * db2\n",
        "    weights['W1'] -= learning_rate * dW1\n",
        "    biases['b1'] -= learning_rate * db1\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "# After training, you can use the trained neural network for predictions.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jURuNJqhJcjF",
        "outputId": "4ba98a3f-1d7a-4cfc-e486-9b6e40416ef7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10000, Loss: 0.2882\n",
            "Epoch 1000/10000, Loss: 0.2419\n",
            "Epoch 2000/10000, Loss: 0.2379\n",
            "Epoch 3000/10000, Loss: 0.2251\n",
            "Epoch 4000/10000, Loss: 0.2086\n",
            "Epoch 5000/10000, Loss: 0.2032\n",
            "Epoch 6000/10000, Loss: 0.2003\n",
            "Epoch 7000/10000, Loss: 0.1998\n",
            "Epoch 8000/10000, Loss: 0.2076\n",
            "Epoch 9000/10000, Loss: 0.1965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, output_size):\n",
        "        # Define the architecture\n",
        "        self.input_size = input_size\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "        self.hidden3_size = hidden3_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = {\n",
        "            'W1': np.random.randn(input_size, hidden1_size),\n",
        "            'W2': np.random.randn(input_size, hidden2_size),\n",
        "            'W3': np.random.randn(input_size, hidden3_size),\n",
        "            'W4': np.random.randn(hidden1_size + hidden2_size + hidden3_size, output_size)\n",
        "        }\n",
        "\n",
        "        self.biases = {\n",
        "            'b1': np.zeros((1, hidden1_size)),\n",
        "            'b2': np.zeros((1, hidden2_size)),\n",
        "            'b3': np.zeros((1, hidden3_size)),\n",
        "            'b4': np.zeros((1, output_size))\n",
        "        }\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Layer 1\n",
        "        self.z1 = np.dot(X, self.weights['W1']) + self.biases['b1']\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "\n",
        "        # Layer 2\n",
        "        self.z2 = np.dot(X, self.weights['W2']) + self.biases['b2']\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.z3 = np.dot(X, self.weights['W3']) + self.biases['b3']\n",
        "        self.a3 = self.sigmoid(self.z3)\n",
        "\n",
        "        # Concatenate the outputs of layers 1, 2, and 3\n",
        "        self.concatenated_output = np.concatenate((self.a1, self.a2, self.a3), axis=1)\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        self.z4 = np.dot(self.concatenated_output, self.weights['W4']) + self.biases['b4']\n",
        "        self.output = self.sigmoid(self.z4)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Backpropagation\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        delta4 = 2 * (self.output - y) * self.sigmoid_derivative(self.output)\n",
        "        dW4 = np.dot(self.concatenated_output.T, delta4)\n",
        "        db4 = np.sum(delta4, axis=0, keepdims=True)\n",
        "\n",
        "        # Split the delta for the concatenation in layer 4\n",
        "        delta4_split = np.dot(delta4, self.weights['W4'].T)\n",
        "\n",
        "        # Separate the deltas for layers 1, 2, and 3\n",
        "        delta3 = delta4_split[:, -self.hidden3_size:] * self.sigmoid_derivative(self.a3)\n",
        "        delta2 = delta4_split[:, -self.hidden3_size-self.hidden2_size:-self.hidden3_size] * self.sigmoid_derivative(self.a2)\n",
        "        delta1 = delta4_split[:, :-self.hidden3_size-self.hidden2_size] * self.sigmoid_derivative(self.a1)\n",
        "\n",
        "        # Calculate gradients for layers 1, 2, and 3\n",
        "        dW3 = np.dot(X.T, delta3)\n",
        "        db3 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        dW2 = np.dot(X.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights['W4'] -= learning_rate * dW4\n",
        "        self.biases['b4'] -= learning_rate * db4\n",
        "        self.weights['W3'] -= learning_rate * dW3\n",
        "        self.biases['b3'] -= learning_rate * db3\n",
        "        self.weights['W2'] -= learning_rate * dW2\n",
        "        self.biases['b2'] -= learning_rate * db2\n",
        "        self.weights['W1'] -= learning_rate * dW1\n",
        "        self.biases['b1'] -= learning_rate * db1\n",
        "\n",
        "        return dW4, db4, dW3, db3, dW2, db2, dW1, db1\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            self.feedforward(X)\n",
        "\n",
        "            # Backpropagation\n",
        "            self.backpropagation(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean((self.output - y) ** 2)\n",
        "                print(f'Epoch {epoch}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "# Example usage:\n",
        "input_size = 2\n",
        "hidden1_size = 4\n",
        "hidden2_size = 3\n",
        "hidden3_size = 2\n",
        "output_size = 1\n",
        "learning_rate = 0.1\n",
        "epochs = 10000000\n",
        "\n",
        "# Generate random input data and corresponding target values\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, input_size)\n",
        "y = np.random.randint(2, size=(100, output_size))\n",
        "\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size, hidden3_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "nn.train(X, y, learning_rate, epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CCE1A3nMLEbE",
        "outputId": "1e65805e-f344-42c4-94e9-e77bf1d00688"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10000000, Loss: 0.2618\n",
            "Epoch 1000/10000000, Loss: 0.2438\n",
            "Epoch 2000/10000000, Loss: 0.2437\n",
            "Epoch 3000/10000000, Loss: 0.2435\n",
            "Epoch 4000/10000000, Loss: 0.2431\n",
            "Epoch 5000/10000000, Loss: 0.2424\n",
            "Epoch 6000/10000000, Loss: 0.2417\n",
            "Epoch 7000/10000000, Loss: 0.2417\n",
            "Epoch 8000/10000000, Loss: 0.2404\n",
            "Epoch 9000/10000000, Loss: 0.2383\n",
            "Epoch 10000/10000000, Loss: 0.2362\n",
            "Epoch 11000/10000000, Loss: 0.2342\n",
            "Epoch 12000/10000000, Loss: 0.2322\n",
            "Epoch 13000/10000000, Loss: 0.2305\n",
            "Epoch 14000/10000000, Loss: 0.2290\n",
            "Epoch 15000/10000000, Loss: 0.2277\n",
            "Epoch 16000/10000000, Loss: 0.2267\n",
            "Epoch 17000/10000000, Loss: 0.2258\n",
            "Epoch 18000/10000000, Loss: 0.2251\n",
            "Epoch 19000/10000000, Loss: 0.2244\n",
            "Epoch 20000/10000000, Loss: 0.2238\n",
            "Epoch 21000/10000000, Loss: 0.2233\n",
            "Epoch 22000/10000000, Loss: 0.2229\n",
            "Epoch 23000/10000000, Loss: 0.2225\n",
            "Epoch 24000/10000000, Loss: 0.2221\n",
            "Epoch 25000/10000000, Loss: 0.2218\n",
            "Epoch 26000/10000000, Loss: 0.2215\n",
            "Epoch 27000/10000000, Loss: 0.2212\n",
            "Epoch 28000/10000000, Loss: 0.2210\n",
            "Epoch 29000/10000000, Loss: 0.2208\n",
            "Epoch 30000/10000000, Loss: 0.2206\n",
            "Epoch 31000/10000000, Loss: 0.2204\n",
            "Epoch 32000/10000000, Loss: 0.2202\n",
            "Epoch 33000/10000000, Loss: 0.2200\n",
            "Epoch 34000/10000000, Loss: 0.2198\n",
            "Epoch 35000/10000000, Loss: 0.2196\n",
            "Epoch 36000/10000000, Loss: 0.2195\n",
            "Epoch 37000/10000000, Loss: 0.2193\n",
            "Epoch 38000/10000000, Loss: 0.2192\n",
            "Epoch 39000/10000000, Loss: 0.2191\n",
            "Epoch 40000/10000000, Loss: 0.2189\n",
            "Epoch 41000/10000000, Loss: 0.2188\n",
            "Epoch 42000/10000000, Loss: 0.2187\n",
            "Epoch 43000/10000000, Loss: 0.2186\n",
            "Epoch 44000/10000000, Loss: 0.2185\n",
            "Epoch 45000/10000000, Loss: 0.2184\n",
            "Epoch 46000/10000000, Loss: 0.2182\n",
            "Epoch 47000/10000000, Loss: 0.2181\n",
            "Epoch 48000/10000000, Loss: 0.2181\n",
            "Epoch 49000/10000000, Loss: 0.2180\n",
            "Epoch 50000/10000000, Loss: 0.2179\n",
            "Epoch 51000/10000000, Loss: 0.2178\n",
            "Epoch 52000/10000000, Loss: 0.2177\n",
            "Epoch 53000/10000000, Loss: 0.2176\n",
            "Epoch 54000/10000000, Loss: 0.2175\n",
            "Epoch 55000/10000000, Loss: 0.2175\n",
            "Epoch 56000/10000000, Loss: 0.2174\n",
            "Epoch 57000/10000000, Loss: 0.2173\n",
            "Epoch 58000/10000000, Loss: 0.2172\n",
            "Epoch 59000/10000000, Loss: 0.2172\n",
            "Epoch 60000/10000000, Loss: 0.2171\n",
            "Epoch 61000/10000000, Loss: 0.2170\n",
            "Epoch 62000/10000000, Loss: 0.2170\n",
            "Epoch 63000/10000000, Loss: 0.2169\n",
            "Epoch 64000/10000000, Loss: 0.2168\n",
            "Epoch 65000/10000000, Loss: 0.2168\n",
            "Epoch 66000/10000000, Loss: 0.2167\n",
            "Epoch 67000/10000000, Loss: 0.2166\n",
            "Epoch 68000/10000000, Loss: 0.2166\n",
            "Epoch 69000/10000000, Loss: 0.2165\n",
            "Epoch 70000/10000000, Loss: 0.2165\n",
            "Epoch 71000/10000000, Loss: 0.2164\n",
            "Epoch 72000/10000000, Loss: 0.2164\n",
            "Epoch 73000/10000000, Loss: 0.2163\n",
            "Epoch 74000/10000000, Loss: 0.2162\n",
            "Epoch 75000/10000000, Loss: 0.2162\n",
            "Epoch 76000/10000000, Loss: 0.2162\n",
            "Epoch 77000/10000000, Loss: 0.2163\n",
            "Epoch 78000/10000000, Loss: 0.2166\n",
            "Epoch 79000/10000000, Loss: 0.2167\n",
            "Epoch 80000/10000000, Loss: 0.2168\n",
            "Epoch 81000/10000000, Loss: 0.2169\n",
            "Epoch 82000/10000000, Loss: 0.2168\n",
            "Epoch 83000/10000000, Loss: 0.2168\n",
            "Epoch 84000/10000000, Loss: 0.2167\n",
            "Epoch 85000/10000000, Loss: 0.2165\n",
            "Epoch 86000/10000000, Loss: 0.2164\n",
            "Epoch 87000/10000000, Loss: 0.2163\n",
            "Epoch 88000/10000000, Loss: 0.2161\n",
            "Epoch 89000/10000000, Loss: 0.2160\n",
            "Epoch 90000/10000000, Loss: 0.2158\n",
            "Epoch 91000/10000000, Loss: 0.2156\n",
            "Epoch 92000/10000000, Loss: 0.2155\n",
            "Epoch 93000/10000000, Loss: 0.2153\n",
            "Epoch 94000/10000000, Loss: 0.2151\n",
            "Epoch 95000/10000000, Loss: 0.2149\n",
            "Epoch 96000/10000000, Loss: 0.2148\n",
            "Epoch 97000/10000000, Loss: 0.2146\n",
            "Epoch 98000/10000000, Loss: 0.2144\n",
            "Epoch 99000/10000000, Loss: 0.2142\n",
            "Epoch 100000/10000000, Loss: 0.2140\n",
            "Epoch 101000/10000000, Loss: 0.2138\n",
            "Epoch 102000/10000000, Loss: 0.2137\n",
            "Epoch 103000/10000000, Loss: 0.2135\n",
            "Epoch 104000/10000000, Loss: 0.2133\n",
            "Epoch 105000/10000000, Loss: 0.2131\n",
            "Epoch 106000/10000000, Loss: 0.2129\n",
            "Epoch 107000/10000000, Loss: 0.2127\n",
            "Epoch 108000/10000000, Loss: 0.2125\n",
            "Epoch 109000/10000000, Loss: 0.2123\n",
            "Epoch 110000/10000000, Loss: 0.2121\n",
            "Epoch 111000/10000000, Loss: 0.2119\n",
            "Epoch 112000/10000000, Loss: 0.2117\n",
            "Epoch 113000/10000000, Loss: 0.2116\n",
            "Epoch 114000/10000000, Loss: 0.2114\n",
            "Epoch 115000/10000000, Loss: 0.2112\n",
            "Epoch 116000/10000000, Loss: 0.2111\n",
            "Epoch 117000/10000000, Loss: 0.2109\n",
            "Epoch 118000/10000000, Loss: 0.2108\n",
            "Epoch 119000/10000000, Loss: 0.2107\n",
            "Epoch 120000/10000000, Loss: 0.2105\n",
            "Epoch 121000/10000000, Loss: 0.2104\n",
            "Epoch 122000/10000000, Loss: 0.2103\n",
            "Epoch 123000/10000000, Loss: 0.2102\n",
            "Epoch 124000/10000000, Loss: 0.2101\n",
            "Epoch 125000/10000000, Loss: 0.2100\n",
            "Epoch 126000/10000000, Loss: 0.2099\n",
            "Epoch 127000/10000000, Loss: 0.2098\n",
            "Epoch 128000/10000000, Loss: 0.2098\n",
            "Epoch 129000/10000000, Loss: 0.2097\n",
            "Epoch 130000/10000000, Loss: 0.2096\n",
            "Epoch 131000/10000000, Loss: 0.2095\n",
            "Epoch 132000/10000000, Loss: 0.2094\n",
            "Epoch 133000/10000000, Loss: 0.2094\n",
            "Epoch 134000/10000000, Loss: 0.2093\n",
            "Epoch 135000/10000000, Loss: 0.2092\n",
            "Epoch 136000/10000000, Loss: 0.2092\n",
            "Epoch 137000/10000000, Loss: 0.2091\n",
            "Epoch 138000/10000000, Loss: 0.2091\n",
            "Epoch 139000/10000000, Loss: 0.2090\n",
            "Epoch 140000/10000000, Loss: 0.2089\n",
            "Epoch 141000/10000000, Loss: 0.2089\n",
            "Epoch 142000/10000000, Loss: 0.2085\n",
            "Epoch 143000/10000000, Loss: 0.2090\n",
            "Epoch 144000/10000000, Loss: 0.2083\n",
            "Epoch 145000/10000000, Loss: 0.2082\n",
            "Epoch 146000/10000000, Loss: 0.2104\n",
            "Epoch 147000/10000000, Loss: 0.2082\n",
            "Epoch 148000/10000000, Loss: 0.2081\n",
            "Epoch 149000/10000000, Loss: 0.2081\n",
            "Epoch 150000/10000000, Loss: 0.2080\n",
            "Epoch 151000/10000000, Loss: 0.2080\n",
            "Epoch 152000/10000000, Loss: 0.2080\n",
            "Epoch 153000/10000000, Loss: 0.2079\n",
            "Epoch 154000/10000000, Loss: 0.2079\n",
            "Epoch 155000/10000000, Loss: 0.2079\n",
            "Epoch 156000/10000000, Loss: 0.2078\n",
            "Epoch 157000/10000000, Loss: 0.2080\n",
            "Epoch 158000/10000000, Loss: 0.2078\n",
            "Epoch 159000/10000000, Loss: 0.2077\n",
            "Epoch 160000/10000000, Loss: 0.2077\n",
            "Epoch 161000/10000000, Loss: 0.2077\n",
            "Epoch 162000/10000000, Loss: 0.2077\n",
            "Epoch 163000/10000000, Loss: 0.2076\n",
            "Epoch 164000/10000000, Loss: 0.2076\n",
            "Epoch 165000/10000000, Loss: 0.2076\n",
            "Epoch 166000/10000000, Loss: 0.2075\n",
            "Epoch 167000/10000000, Loss: 0.2076\n",
            "Epoch 168000/10000000, Loss: 0.2075\n",
            "Epoch 169000/10000000, Loss: 0.2075\n",
            "Epoch 170000/10000000, Loss: 0.2074\n",
            "Epoch 171000/10000000, Loss: 0.2074\n",
            "Epoch 172000/10000000, Loss: 0.2100\n",
            "Epoch 173000/10000000, Loss: 0.2074\n",
            "Epoch 174000/10000000, Loss: 0.2076\n",
            "Epoch 175000/10000000, Loss: 0.2073\n",
            "Epoch 176000/10000000, Loss: 0.2073\n",
            "Epoch 177000/10000000, Loss: 0.2073\n",
            "Epoch 178000/10000000, Loss: 0.2073\n",
            "Epoch 179000/10000000, Loss: 0.2072\n",
            "Epoch 180000/10000000, Loss: 0.2072\n",
            "Epoch 181000/10000000, Loss: 0.2072\n",
            "Epoch 182000/10000000, Loss: 0.2072\n",
            "Epoch 183000/10000000, Loss: 0.2071\n",
            "Epoch 184000/10000000, Loss: 0.2071\n",
            "Epoch 185000/10000000, Loss: 0.2080\n",
            "Epoch 186000/10000000, Loss: 0.2071\n",
            "Epoch 187000/10000000, Loss: 0.2070\n",
            "Epoch 188000/10000000, Loss: 0.2070\n",
            "Epoch 189000/10000000, Loss: 0.2069\n",
            "Epoch 190000/10000000, Loss: 0.2070\n",
            "Epoch 191000/10000000, Loss: 0.2098\n",
            "Epoch 192000/10000000, Loss: 0.2105\n",
            "Epoch 193000/10000000, Loss: 0.2102\n",
            "Epoch 194000/10000000, Loss: 0.2099\n",
            "Epoch 195000/10000000, Loss: 0.2097\n",
            "Epoch 196000/10000000, Loss: 0.2096\n",
            "Epoch 197000/10000000, Loss: 0.2094\n",
            "Epoch 198000/10000000, Loss: 0.2093\n",
            "Epoch 199000/10000000, Loss: 0.2091\n",
            "Epoch 200000/10000000, Loss: 0.2090\n",
            "Epoch 201000/10000000, Loss: 0.2089\n",
            "Epoch 202000/10000000, Loss: 0.2087\n",
            "Epoch 203000/10000000, Loss: 0.2086\n",
            "Epoch 204000/10000000, Loss: 0.2085\n",
            "Epoch 205000/10000000, Loss: 0.2084\n",
            "Epoch 206000/10000000, Loss: 0.2083\n",
            "Epoch 207000/10000000, Loss: 0.2082\n",
            "Epoch 208000/10000000, Loss: 0.2081\n",
            "Epoch 209000/10000000, Loss: 0.2080\n",
            "Epoch 210000/10000000, Loss: 0.2079\n",
            "Epoch 211000/10000000, Loss: 0.2078\n",
            "Epoch 212000/10000000, Loss: 0.2077\n",
            "Epoch 213000/10000000, Loss: 0.2076\n",
            "Epoch 214000/10000000, Loss: 0.2076\n",
            "Epoch 215000/10000000, Loss: 0.2075\n",
            "Epoch 216000/10000000, Loss: 0.2074\n",
            "Epoch 217000/10000000, Loss: 0.2074\n",
            "Epoch 218000/10000000, Loss: 0.2073\n",
            "Epoch 219000/10000000, Loss: 0.2072\n",
            "Epoch 220000/10000000, Loss: 0.2072\n",
            "Epoch 221000/10000000, Loss: 0.2071\n",
            "Epoch 222000/10000000, Loss: 0.2071\n",
            "Epoch 223000/10000000, Loss: 0.2070\n",
            "Epoch 224000/10000000, Loss: 0.2070\n",
            "Epoch 225000/10000000, Loss: 0.2069\n",
            "Epoch 226000/10000000, Loss: 0.2069\n",
            "Epoch 227000/10000000, Loss: 0.2069\n",
            "Epoch 228000/10000000, Loss: 0.2068\n",
            "Epoch 229000/10000000, Loss: 0.2068\n",
            "Epoch 230000/10000000, Loss: 0.2068\n",
            "Epoch 231000/10000000, Loss: 0.2067\n",
            "Epoch 232000/10000000, Loss: 0.2067\n",
            "Epoch 233000/10000000, Loss: 0.2067\n",
            "Epoch 234000/10000000, Loss: 0.2066\n",
            "Epoch 235000/10000000, Loss: 0.2066\n",
            "Epoch 236000/10000000, Loss: 0.2066\n",
            "Epoch 237000/10000000, Loss: 0.2065\n",
            "Epoch 238000/10000000, Loss: 0.2065\n",
            "Epoch 239000/10000000, Loss: 0.2065\n",
            "Epoch 240000/10000000, Loss: 0.2065\n",
            "Epoch 241000/10000000, Loss: 0.2064\n",
            "Epoch 242000/10000000, Loss: 0.2064\n",
            "Epoch 243000/10000000, Loss: 0.2064\n",
            "Epoch 244000/10000000, Loss: 0.2064\n",
            "Epoch 245000/10000000, Loss: 0.2063\n",
            "Epoch 246000/10000000, Loss: 0.2063\n",
            "Epoch 247000/10000000, Loss: 0.2063\n",
            "Epoch 248000/10000000, Loss: 0.2063\n",
            "Epoch 249000/10000000, Loss: 0.2063\n",
            "Epoch 250000/10000000, Loss: 0.2062\n",
            "Epoch 251000/10000000, Loss: 0.2062\n",
            "Epoch 252000/10000000, Loss: 0.2062\n",
            "Epoch 253000/10000000, Loss: 0.2062\n",
            "Epoch 254000/10000000, Loss: 0.2062\n",
            "Epoch 255000/10000000, Loss: 0.2061\n",
            "Epoch 256000/10000000, Loss: 0.2061\n",
            "Epoch 257000/10000000, Loss: 0.2061\n",
            "Epoch 258000/10000000, Loss: 0.2061\n",
            "Epoch 259000/10000000, Loss: 0.2061\n",
            "Epoch 260000/10000000, Loss: 0.2060\n",
            "Epoch 261000/10000000, Loss: 0.2060\n",
            "Epoch 262000/10000000, Loss: 0.2060\n",
            "Epoch 263000/10000000, Loss: 0.2060\n",
            "Epoch 264000/10000000, Loss: 0.2060\n",
            "Epoch 265000/10000000, Loss: 0.2060\n",
            "Epoch 266000/10000000, Loss: 0.2059\n",
            "Epoch 267000/10000000, Loss: 0.2059\n",
            "Epoch 268000/10000000, Loss: 0.2059\n",
            "Epoch 269000/10000000, Loss: 0.2059\n",
            "Epoch 270000/10000000, Loss: 0.2059\n",
            "Epoch 271000/10000000, Loss: 0.2059\n",
            "Epoch 272000/10000000, Loss: 0.2058\n",
            "Epoch 273000/10000000, Loss: 0.2058\n",
            "Epoch 274000/10000000, Loss: 0.2058\n",
            "Epoch 275000/10000000, Loss: 0.2058\n",
            "Epoch 276000/10000000, Loss: 0.2058\n",
            "Epoch 277000/10000000, Loss: 0.2058\n",
            "Epoch 278000/10000000, Loss: 0.2057\n",
            "Epoch 279000/10000000, Loss: 0.2057\n",
            "Epoch 280000/10000000, Loss: 0.2057\n",
            "Epoch 281000/10000000, Loss: 0.2057\n",
            "Epoch 282000/10000000, Loss: 0.2057\n",
            "Epoch 283000/10000000, Loss: 0.2057\n",
            "Epoch 284000/10000000, Loss: 0.2057\n",
            "Epoch 285000/10000000, Loss: 0.2056\n",
            "Epoch 286000/10000000, Loss: 0.2056\n",
            "Epoch 287000/10000000, Loss: 0.2056\n",
            "Epoch 288000/10000000, Loss: 0.2056\n",
            "Epoch 289000/10000000, Loss: 0.2056\n",
            "Epoch 290000/10000000, Loss: 0.2056\n",
            "Epoch 291000/10000000, Loss: 0.2055\n",
            "Epoch 292000/10000000, Loss: 0.2055\n",
            "Epoch 293000/10000000, Loss: 0.2055\n",
            "Epoch 294000/10000000, Loss: 0.2055\n",
            "Epoch 295000/10000000, Loss: 0.2055\n",
            "Epoch 296000/10000000, Loss: 0.2055\n",
            "Epoch 297000/10000000, Loss: 0.2055\n",
            "Epoch 298000/10000000, Loss: 0.2055\n",
            "Epoch 299000/10000000, Loss: 0.2054\n",
            "Epoch 300000/10000000, Loss: 0.2054\n",
            "Epoch 301000/10000000, Loss: 0.2054\n",
            "Epoch 302000/10000000, Loss: 0.2054\n",
            "Epoch 303000/10000000, Loss: 0.2054\n",
            "Epoch 304000/10000000, Loss: 0.2054\n",
            "Epoch 305000/10000000, Loss: 0.2054\n",
            "Epoch 306000/10000000, Loss: 0.2053\n",
            "Epoch 307000/10000000, Loss: 0.2053\n",
            "Epoch 308000/10000000, Loss: 0.2053\n",
            "Epoch 309000/10000000, Loss: 0.2053\n",
            "Epoch 310000/10000000, Loss: 0.2053\n",
            "Epoch 311000/10000000, Loss: 0.2053\n",
            "Epoch 312000/10000000, Loss: 0.2053\n",
            "Epoch 313000/10000000, Loss: 0.2053\n",
            "Epoch 314000/10000000, Loss: 0.2052\n",
            "Epoch 315000/10000000, Loss: 0.2052\n",
            "Epoch 316000/10000000, Loss: 0.2052\n",
            "Epoch 317000/10000000, Loss: 0.2052\n",
            "Epoch 318000/10000000, Loss: 0.2052\n",
            "Epoch 319000/10000000, Loss: 0.2052\n",
            "Epoch 320000/10000000, Loss: 0.2052\n",
            "Epoch 321000/10000000, Loss: 0.2051\n",
            "Epoch 322000/10000000, Loss: 0.2051\n",
            "Epoch 323000/10000000, Loss: 0.2051\n",
            "Epoch 324000/10000000, Loss: 0.2051\n",
            "Epoch 325000/10000000, Loss: 0.2051\n",
            "Epoch 326000/10000000, Loss: 0.2051\n",
            "Epoch 327000/10000000, Loss: 0.2051\n",
            "Epoch 328000/10000000, Loss: 0.2051\n",
            "Epoch 329000/10000000, Loss: 0.2050\n",
            "Epoch 330000/10000000, Loss: 0.2050\n",
            "Epoch 331000/10000000, Loss: 0.2050\n",
            "Epoch 332000/10000000, Loss: 0.2050\n",
            "Epoch 333000/10000000, Loss: 0.2050\n",
            "Epoch 334000/10000000, Loss: 0.2050\n",
            "Epoch 335000/10000000, Loss: 0.2050\n",
            "Epoch 336000/10000000, Loss: 0.2050\n",
            "Epoch 337000/10000000, Loss: 0.2049\n",
            "Epoch 338000/10000000, Loss: 0.2049\n",
            "Epoch 339000/10000000, Loss: 0.2049\n",
            "Epoch 340000/10000000, Loss: 0.2049\n",
            "Epoch 341000/10000000, Loss: 0.2049\n",
            "Epoch 342000/10000000, Loss: 0.2049\n",
            "Epoch 343000/10000000, Loss: 0.2049\n",
            "Epoch 344000/10000000, Loss: 0.2048\n",
            "Epoch 345000/10000000, Loss: 0.2048\n",
            "Epoch 346000/10000000, Loss: 0.2048\n",
            "Epoch 347000/10000000, Loss: 0.2048\n",
            "Epoch 348000/10000000, Loss: 0.2048\n",
            "Epoch 349000/10000000, Loss: 0.2048\n",
            "Epoch 350000/10000000, Loss: 0.2048\n",
            "Epoch 351000/10000000, Loss: 0.2047\n",
            "Epoch 352000/10000000, Loss: 0.2047\n",
            "Epoch 353000/10000000, Loss: 0.2047\n",
            "Epoch 354000/10000000, Loss: 0.2047\n",
            "Epoch 355000/10000000, Loss: 0.2047\n",
            "Epoch 356000/10000000, Loss: 0.2047\n",
            "Epoch 357000/10000000, Loss: 0.2047\n",
            "Epoch 358000/10000000, Loss: 0.2047\n",
            "Epoch 359000/10000000, Loss: 0.2046\n",
            "Epoch 360000/10000000, Loss: 0.2046\n",
            "Epoch 361000/10000000, Loss: 0.2046\n",
            "Epoch 362000/10000000, Loss: 0.2046\n",
            "Epoch 363000/10000000, Loss: 0.2046\n",
            "Epoch 364000/10000000, Loss: 0.2046\n",
            "Epoch 365000/10000000, Loss: 0.2046\n",
            "Epoch 366000/10000000, Loss: 0.2046\n",
            "Epoch 367000/10000000, Loss: 0.2045\n",
            "Epoch 368000/10000000, Loss: 0.2045\n",
            "Epoch 369000/10000000, Loss: 0.2045\n",
            "Epoch 370000/10000000, Loss: 0.2045\n",
            "Epoch 371000/10000000, Loss: 0.2045\n",
            "Epoch 372000/10000000, Loss: 0.2045\n",
            "Epoch 373000/10000000, Loss: 0.2045\n",
            "Epoch 374000/10000000, Loss: 0.2045\n",
            "Epoch 375000/10000000, Loss: 0.2045\n",
            "Epoch 376000/10000000, Loss: 0.2044\n",
            "Epoch 377000/10000000, Loss: 0.2044\n",
            "Epoch 378000/10000000, Loss: 0.2044\n",
            "Epoch 379000/10000000, Loss: 0.2044\n",
            "Epoch 380000/10000000, Loss: 0.2044\n",
            "Epoch 381000/10000000, Loss: 0.2044\n",
            "Epoch 382000/10000000, Loss: 0.2044\n",
            "Epoch 383000/10000000, Loss: 0.2044\n",
            "Epoch 384000/10000000, Loss: 0.2044\n",
            "Epoch 385000/10000000, Loss: 0.2043\n",
            "Epoch 386000/10000000, Loss: 0.2043\n",
            "Epoch 387000/10000000, Loss: 0.2043\n",
            "Epoch 388000/10000000, Loss: 0.2043\n",
            "Epoch 389000/10000000, Loss: 0.2043\n",
            "Epoch 390000/10000000, Loss: 0.2043\n",
            "Epoch 391000/10000000, Loss: 0.2043\n",
            "Epoch 392000/10000000, Loss: 0.2043\n",
            "Epoch 393000/10000000, Loss: 0.2043\n",
            "Epoch 394000/10000000, Loss: 0.2043\n",
            "Epoch 395000/10000000, Loss: 0.2042\n",
            "Epoch 396000/10000000, Loss: 0.2042\n",
            "Epoch 397000/10000000, Loss: 0.2042\n",
            "Epoch 398000/10000000, Loss: 0.2042\n",
            "Epoch 399000/10000000, Loss: 0.2042\n",
            "Epoch 400000/10000000, Loss: 0.2042\n",
            "Epoch 401000/10000000, Loss: 0.2042\n",
            "Epoch 402000/10000000, Loss: 0.2042\n",
            "Epoch 403000/10000000, Loss: 0.2042\n",
            "Epoch 404000/10000000, Loss: 0.2042\n",
            "Epoch 405000/10000000, Loss: 0.2042\n",
            "Epoch 406000/10000000, Loss: 0.2041\n",
            "Epoch 407000/10000000, Loss: 0.2041\n",
            "Epoch 408000/10000000, Loss: 0.2041\n",
            "Epoch 409000/10000000, Loss: 0.2041\n",
            "Epoch 410000/10000000, Loss: 0.2041\n",
            "Epoch 411000/10000000, Loss: 0.2041\n",
            "Epoch 412000/10000000, Loss: 0.2041\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-71ba26645f21>\u001b[0m in \u001b[0;36m<cell line: 121>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Train the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-71ba26645f21>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def X_activation(x):\n",
        "    return x\n",
        "\n",
        "def X_activation_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "\n",
        "# Define the custom activation functions and their derivatives\n",
        "def X_2_activation(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "def X_2_activation_derivative(x):\n",
        "    return (x / 4)\n",
        "\n",
        "def X_3_activation(x):\n",
        "    return (x**3) / 24\n",
        "\n",
        "def X_3_activation_derivative(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, output_size):\n",
        "        # Define the architecture\n",
        "        self.input_size = input_size\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "        self.hidden3_size = hidden3_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = {\n",
        "            'W1': np.random.randn(input_size, hidden1_size),\n",
        "            'W2': np.random.randn(input_size, hidden2_size),\n",
        "            'W3': np.random.randn(input_size, hidden3_size),\n",
        "            'W4': np.random.randn(hidden1_size + hidden2_size + hidden3_size, output_size)\n",
        "        }\n",
        "\n",
        "        self.biases = {\n",
        "            'b1': np.random.randn(1, hidden1_size),\n",
        "            'b2': np.random.randn(1, hidden2_size),\n",
        "            'b3': np.random.randn(1, hidden3_size),\n",
        "            'b4': np.random.randn(1, output_size)\n",
        "        }\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "\n",
        "        # Define the X activation function and its derivative\n",
        "    def X_activation(self, x):\n",
        "        return X_activation(x)\n",
        "\n",
        "    def X_activation_derivative(self, x):\n",
        "        return X_activation_derivative(x)\n",
        "\n",
        "    def X_2_activation(self, x):\n",
        "        return X_2_activation(x)\n",
        "\n",
        "    def X_2_activation_derivative(self, x):\n",
        "        return X_2_activation_derivative(x)\n",
        "\n",
        "    def X_3_activation(self, x):\n",
        "        return X_3_activation(x)\n",
        "\n",
        "    def X_3_activation_derivative(self, x):\n",
        "        return X_3_activation_derivative(x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "    # Layer 1 (X Activation)\n",
        "        self.z1 = np.dot(X, self.weights['W1']) + self.biases['b1']\n",
        "        self.a1 = X_activation(self.z1)  # Use X activation for layer\n",
        "\n",
        "        # Layer 2\n",
        "        self.z2 = np.dot(X, self.weights['W2']) + self.biases['b2']\n",
        "        self.a2 = X_2_activation(self.z2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.z3 = np.dot(X, self.weights['W3']) + self.biases['b3']\n",
        "        self.a3 = X_3_activation(self.z3)\n",
        "\n",
        "        # Concatenate the outputs of layers 1, 2, and 3\n",
        "        self.concatenated_output = np.concatenate((self.a1, self.a2, self.a3), axis=1)\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        self.z4 = np.dot(self.concatenated_output, self.weights['W4']) + self.biases['b4']\n",
        "        self.output = X_activation(self.z4)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Backpropagation\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        delta4 = 2 * (self.output - y) * self.X_activation_derivative(self.output)\n",
        "        dW4 = np.dot(self.concatenated_output.T, delta4)\n",
        "        db4 = np.sum(delta4, axis=0, keepdims=True)\n",
        "\n",
        "        # Split the delta for the concatenation in layer 4\n",
        "        delta4_split = np.dot(delta4, self.weights['W4'].T)\n",
        "\n",
        "        # Separate the deltas for layers 1, 2, and 3\n",
        "        delta3 = delta4_split[:, -self.hidden3_size:] * self.X_3_activation_derivative(self.a3)\n",
        "\n",
        "        delta2 = delta4_split[:, -self.hidden3_size-self.hidden2_size:-self.hidden3_size] * self.X_2_activation_derivative(self.a2)\n",
        "        delta1 = delta4_split[:, :-self.hidden3_size-self.hidden2_size] * self.X_activation_derivative(self.a1)\n",
        "\n",
        "        # Calculate gradients for layers 1, 2, and 3\n",
        "        dW3 = np.dot(X.T, delta3)\n",
        "        db3 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        dW2 = np.dot(X.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights['W4'] -= learning_rate * dW4\n",
        "        self.biases['b4'] -= learning_rate * db4\n",
        "        self.weights['W3'] -= learning_rate * dW3\n",
        "        self.biases['b3'] -= learning_rate * db3\n",
        "        self.weights['W2'] -= learning_rate * dW2\n",
        "        self.biases['b2'] -= learning_rate * db2\n",
        "        self.weights['W1'] -= learning_rate * dW1\n",
        "        self.biases['b1'] -= learning_rate * db1\n",
        "\n",
        "        return dW4, db4, dW3, db3, dW2, db2, dW1, db1\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            self.feedforward(X)\n",
        "\n",
        "            # Backpropagation\n",
        "            self.backpropagation(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 1 == 0:\n",
        "                loss = np.mean((self.output - y) ** 2)\n",
        "                print(f'Epoch {epoch}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "# Example usage:\n",
        "input_size = 1\n",
        "hidden1_size = 4\n",
        "hidden2_size = 3\n",
        "hidden3_size = 2\n",
        "output_size = 1\n",
        "learning_rate = 0.000001\n",
        "epochs = 20\n",
        "\n",
        "# Generate random input data and corresponding target values\n",
        "np.random.seed(0)\n",
        "# Generate random input data X between 0 and 1\n",
        "X = np.random.rand(1000)\n",
        "X=X.reshape((1000,1))\n",
        "# Calculate y as the sigmoid of X\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "y = sigmoid(X)\n",
        "\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size, hidden3_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "nn.train(X, y, learning_rate, epochs)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsvsZn7OPB77",
        "outputId": "c50634f4-6e24-4e6f-8161-c9ee88052ef9"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/20, Loss: 0.8188\n",
            "Epoch 1/20, Loss: 0.7830\n",
            "Epoch 2/20, Loss: 0.7488\n",
            "Epoch 3/20, Loss: 0.7161\n",
            "Epoch 4/20, Loss: 0.6850\n",
            "Epoch 5/20, Loss: 0.6553\n",
            "Epoch 6/20, Loss: 0.6269\n",
            "Epoch 7/20, Loss: 0.5998\n",
            "Epoch 8/20, Loss: 0.5739\n",
            "Epoch 9/20, Loss: 0.5492\n",
            "Epoch 10/20, Loss: 0.5257\n",
            "Epoch 11/20, Loss: 0.5031\n",
            "Epoch 12/20, Loss: 0.4816\n",
            "Epoch 13/20, Loss: 0.4610\n",
            "Epoch 14/20, Loss: 0.4414\n",
            "Epoch 15/20, Loss: 0.4227\n",
            "Epoch 16/20, Loss: 0.4047\n",
            "Epoch 17/20, Loss: 0.3876\n",
            "Epoch 18/20, Loss: 0.3712\n",
            "Epoch 19/20, Loss: 0.3556\n",
            "(1000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X, y, label='Actual Data', s=5)\n",
        "plt.scatter(X, nn.output, label='Predicted Data', s=5)\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "q2l_ns4YZ8ID",
        "outputId": "54b3d927-c25b-4de8-f83d-dcde65d62afe"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHdUlEQVR4nO3deXhTZeL28e9J2qQsbQpCKZRCAVF0FBjZLIgsg+LoLKjziopsKg6CDIILorKrRRAtAooLiDsuA+pPEUariEBZRFBmRBShlK0oSjegTZOc94/SSKFAkyZNm96f6+olPTnnyZNDSG6f1TBN00REREQkTFhCXQERERGRQFK4ERERkbCicCMiIiJhReFGREREworCjYiIiIQVhRsREREJKwo3IiIiElYiQl2ByubxeNi/fz/R0dEYhhHq6oiIiEg5mKZJXl4eTZo0wWI5c9tMjQs3+/fvJzExMdTVEBERET/s2bOHpk2bnvGcGhduoqOjgeKbExMTE+LaiIiISHnk5uaSmJjo/R4/kxoXbkq6omJiYhRuREREqpnyDCnRgGIREREJKwo3IiIiElYUbkRERCSs1LgxN+XldrspKioKdTUkzNhstrNOYRQRkYpRuDmJaZpkZWWRnZ0d6qpIGLJYLLRo0QKbzRbqqoiIhC2Fm5OUBJu4uDhq166thf4kYEoWkDxw4ADNmjXTe0tEJEgUbk7gdru9weacc84JdXUkDDVs2JD9+/fjcrmIjIwMdXVERMKSOv9PUDLGpnbt2iGuiYSrku4ot9sd4pqIiIQvhZsyqLtAgkXvLRGR4FO4ERERkbCicCMiIiJhReFGKoVhGLz33nuhroaIiNQACjdhJj09HavVyjXXXOPztUlJSaSmpga+UuUwZMgQDMPAMAwiIyNp1KgRV1xxBQsXLsTj8fhU1qJFi4iNjQ1ORUVEpBSX28PsT3/kpufWcvGk5bQY/xHnP/wxM5dvw+X27fM7UDQVPMwsWLCAUaNGsWDBAvbv30+TJk1CXaVyu+qqq3jppZdwu90cPHiQ5cuXM3r0aN59910++OADIiL0dhURCZX8Y076zv6SfdkFAERaoOg02aXQ5WHeyp3YIiIY3ad1JdaymFpuwkh+fj5vvfUWd955J9dccw2LFi065Zz/+7//o1OnTkRFRdGgQQOuvfZaAHr27Mnu3bsZM2aMtwUFYPLkybRv375UGampqSQlJXl/37hxI1dccQUNGjTA4XDQo0cPvv76a5/rb7fbiY+PJyEhgUsuuYQHH3yQ999/n48//rjUa3nyySe5+OKLqVOnDomJiYwYMYL8/HwAVq5cydChQ8nJyfG+jsmTJwPw6quv0rFjR6Kjo4mPj+fmm2/m559/9rmeIiLhrMDpov/8tbR84COSTvi5aMon3mADpw82J9qY8VsQa3p6CjdBUtJMd8uL65n96Y+V0jT39ttv06ZNG84//3xuueUWFi5ciGma3sc/+ugjrr32Wq6++mo2b95MWloanTt3BmDJkiU0bdqUqVOncuDAAQ4cOFDu583Ly2Pw4MGsXr2adevW0bp1a66++mry8vIq/Jp69+5Nu3btWLJkifeYxWLh6aef5n//+x8vv/wyn332Gffffz8AXbt2JTU1lZiYGO/ruPfee4HidYymTZvGN998w3vvvUdGRgZDhgypcB1FRKqrAqeL//fM6lIhps3EFazPOEwgvrU6JdUPQCm+Uzt/kMz7/CdSP/0BE1iz4xBA0JvmFixYwC233AIUd/Hk5OTwxRdf0LNnTwAeffRRbrzxRqZMmeK9pl27dgDUr18fq9XqbdXwRe/evUv9/vzzzxMbG8sXX3zBX/7ylwq8omJt2rTh22+/9f5+9913e/+clJTEI488wvDhw3nmmWew2Ww4HA4Mwzjlddx6663eP7ds2ZKnn36aTp06kZ+fT926dStcTxGRqqrA6WLgi+vZmJkd1OexGGACNquF2y9LYmSvVkF9vtNRuAmSjRm/UdJmYhL8prnt27ezYcMGli5dCkBERAT9+/dnwYIF3nCzZcsWhg0bFvDnPnjwIA8//DArV67k559/xu12c/ToUTIzMwNSvmmapRa/+/TTT0lJSeH7778nNzcXl8tFQUEBR48ePePq0ps2bWLy5Ml88803HD582DtQOTMzkwsvvDAgdRURCSWX28NTK37gxbW7KHQVf8YZgHnmy/xycrmdmsfy6m1diLKFPlqEvgZhqlNSfdbsOIRJ8Rsg2E1zCxYswOVylRpAbJomdruduXPn4nA4qFWrls/lWiyWUl1b8Ps2FSUGDx7Mr7/+yuzZs2nevDl2u53k5GScTqd/L+Yk27Zto0WLFgBkZGTwl7/8hTvvvJNHH32U+vXrs3r1am677TacTudpw82RI0fo27cvffv25fXXX6dhw4ZkZmbSt2/fgNVTRKQy5R9z0jf1S/blFJzxvEAHm7o2C0O7tWB0n/OIsFbN0S0KN0FS0hS3MeM3OiXVD2rTnMvl4pVXXmHWrFlceeWVpR7r168fb775JsOHD6dt27akpaUxdOjQMsux2Wyn7HnUsGFDsrKySrWebNmypdQ5a9as4ZlnnuHqq68GYM+ePRw6dCggr+2zzz5j69atjBkzBihuffF4PMyaNQuLpfgf1dtvv33W1/H999/z66+/Mn36dBITEwH46quvAlJHEZFgcrk9PPWf7by4JoNCl4dIC8RFR7E/pyAoLTJQ/D/lde1WLkqI5aUhHatEa4wvqldtq5EIq6XSpr99+OGHHD58mNtuuw2Hw1Hqseuvv54FCxYwfPhwJk2axJ/+9CdatWrFjTfeiMvlYtmyZYwbNw4oHr+yatUqbrzxRux2Ow0aNKBnz5788ssvzJgxg3/84x8sX76cjz/+mJiYGO9ztG7d2jsTKTc3l/vuu8+vVqLCwkKysrJKTQVPSUnhL3/5C4MGDQLg3HPPpaioiDlz5vDXv/6VNWvWMH/+/FLlJCUlkZ+fT1paGu3ataN27do0a9YMm83GnDlzGD58OP/973+ZNm2az3UUEQm2Q3lHufSxz3GdJrkUeThra42vqlKXUiBUzfYk8cmCBQvo06fPKcEGisPNV199xbfffkvPnj155513+OCDD2jfvj29e/dmw4YN3nOnTp1KRkYGrVq1omHDhgBccMEFPPPMM8ybN4927dqxYcMG7+yjE5//8OHDXHLJJQwcOJB//etfxMXF+fw6li9fTuPGjUlKSuKqq67i888/5+mnn+b999/HarUCxQOgn3zySR5//HEuuugiXn/9dVJSUkqV07VrV4YPH07//v1p2LAhM2bMoGHDhixatIh33nmHCy+8kOnTp/PEE0/4XEcRkUDIPlLARROXe2conTv+I1oc/3PHR08fbPxx4na9TRx2/jvpCjKmX1Pq5507u4VNsAEwzJMHVIS53NxcHA4HOTk5pVofAAoKCti1axctWrQgKioqRDWUcKb3mEjNU+B0MXjhBtZnHK6057QY0Kl5PV6+tXPYhJYzfX+fLDxesYiISIgVT7dex8bMnKA+jwE0cUSR73RxYWNHtRwTE2y6GyIiIj5yuT089cl2Xly9i8LjfUg2qwVnEBdstVkNhl3WgjFXnl9lZylVFQo3IiIip+Fye5iTtoOlW/ZhmiamCTlHC3F68K4jU6IiwSbCALfJ78uHNI/llTAa4FvZdNdERET4fWzMpsxsPKZJfEwUB3MLcAdxZGqCI4oVd3enbi1b8J6kBlK4ERGRGqckyGzIOHzatWL2B3i6tc1qsPaBnjSIPv1K6hIYCjciIhLWso8UcPnML8gtcGGzGtgjreQVuAL+PE0cUVgtBoZhcG37BEb96VyNjQkRhRsREQkLJeNjlmzeS/ZRJ063SZHLU2p3a6fbxOn2P9jEREVgejwYFguGgWYrVVH62xARkWrl95lKGacM6g2kaHsEhS43TrepmUrVjMKN+GzIkCFkZ2fz3nvvAdCzZ0/at29PampqpdZj5cqV9OrVi8OHDxMbG1upzy0ilavA6WLooq/4777DHHF68ARpkG+kBeIdtbjukgRG9W6tIFNNKdyEiSFDhvDyyy8DEBkZSbNmzRg0aBAPPvggERHB/WtesmQJkZGR5Tq3sgNJUlISu3fvBiAqKopGjRrRuXNnhg8fTu/evX0q6+RQJyKBlX/MyVVPryYrp4BG0XYaO6LYvDcHA6gVaSGv0H3WMsojOiqCo043dWxWBic3r9K7W4t/FG7CyFVXXcVLL71EYWEhy5YtY+TIkURGRjJ+/PhTznU6ndhsgZl6WL9+/YCUEyxTp05l2LBhOJ1OMjIyeO211+jTpw/Tpk3joYceCnX1RGqk/GNOrpq9mn3Zx4iwQMPju1yX2JdTUGpzSF+Djc1aPLC3yOXBsBhEWOCPifVYNLSTxsfUAIqqYcRutxMfH0/z5s2588476dOnDx988AFQ3OrQr18/Hn30UZo0acL5558PwJ49e7jhhhuIjY2lfv36/P3vfycjI8NbptvtZuzYscTGxnLOOedw//33c/J2ZD179uTuu+/2/l5YWMi4ceNITEzEbrdz7rnnsmDBAjIyMujVqxcA9erVwzAMhgwZAoDH4yElJYUWLVpQq1Yt2rVrx7vvvlvqeZYtW8Z5551HrVq16NWrV6l6nkl0dDTx8fE0a9aMyy+/nOeff54JEyYwceJEtm/f7n2dt912m/f5zz//fGbPnu0tY/Lkybz88su8//77GEbxh+bKlSsBGDduHOeddx61a9emZcuWTJgwgaKionLVTaQmOHmTyKQHPuKiKZ+wN/sYJsW7XAdq2nVdm4VRvVrx3dSr2P7In9k5/Rp+euxqtj9yNYv/maxgU0PobzmM1apVi19//dX7e1paGjExMXzyyScAFBUV0bdvX5KTk/nyyy+JiIjgkUce4aqrruLbb7/FZrMxa9YsFi1axMKFC7nggguYNWsWS5cuPWOXzqBBg0hPT+fpp5+mXbt27Nq1i0OHDpGYmMi///1vrr/+erZv305MTAy1atUCICUlhddee4358+fTunVrVq1axS233ELDhg3p0aMHe/bs4brrrmPkyJHccccdfPXVV9xzzz1+35vRo0czbdo03n//fe6//348Hg9NmzblnXfe4ZxzzmHt2rXccccdNG7cmBtuuIF7772Xbdu2kZuby0svvQT83mIVHR3NokWLaNKkCVu3bmXYsGFER0dz//33+10/keqqZP2YrzIPE+idCGKiIjBNDwUukyK3eXwl33q8clv4bA4pgaF3Q7C4XfDlLMhMh2bJ0P0esFbO7TZNk7S0NFasWMGoUaO8x+vUqcOLL77o7Y567bXX8Hg8vPjiixiGAcBLL71EbGwsK1eu5MorryQ1NZXx48dz3XXXATB//nxWrFhx2uf+4YcfePvtt/nkk0/o06cPAC1btvQ+XhII4uLivGNuCgsLeeyxx/j0009JTk72XrN69Wqee+45evTowbPPPkurVq2YNWsWAOeffz5bt27l8ccf9+se1a9fn7i4OG/rT2RkJFOmTPE+3qJFC9LT03n77be54YYbqFu3LrVq1aKwsJD4+PhSZT388MPePyclJXHvvfeyePFihRsJay63h9mf/Mgr63dzzOnCbYI7gKN8bVaDtgkO75ibDs1iw2qHawkuvUuC5ctZsDIFMGHnyuJjPccF9Sk//PBD6tatS1FRER6Ph5tvvpnJkyd7H7/44otLjbP55ptv2LFjB9HR0aXKKSgo4KeffiInJ4cDBw7QpUsX72MRERF07NjxlK6pElu2bMFqtdKjR49y13vHjh0cPXqUK664otRxp9PJH//4RwC2bdtWqh6ANwj5yzRNb6gDmDdvHgsXLiQzM5Njx47hdDpp3779Wct56623ePrpp/npp5/Iz8/H5XIRExNTobqJVDXZRwq4bMZK8gM0qPdkTRzFWx0YhqEgIxWmd06wZKaDd1Fv8/jvwdWrVy+effZZbDYbTZo0OWWWVJ06dUr9np+fT4cOHXj99ddPKathw4Z+1aGkm8kX+fn5AHz00UckJCSUesxut/tVj7P59ddf+eWXX2jRogUAixcv5t5772XWrFkkJycTHR3NzJkzWb9+/RnLSU9PZ8CAAUyZMoW+ffvicDhYvHixt4VJpLo5caCvxWJQKwKOuQLbKnOiaHsEX97fg9g6UUEpX2omhZtgaZZ8vMXm+B6vzSrWylAederU4dxzzy33+ZdccglvvfUWcXFxp21paNy4MevXr+fyyy8HwOVysWnTJi655JIyz7/44ovxeDx88cUX3m6pE5W0HLndv//f34UXXojdbiczM/O0LT4XXHCBd3B0iXXr1p39RZ7G7NmzsVgs9OvXD4A1a9bQtWtXRowY4T3np59+OqXuJ9YbYO3atTRv3rzUrKuSqeciVZ3L7WHW8u08t3pnmevGuD0m+c7APV+03cqX9/dUkJGgU7gJlu7HB7ueOOamihkwYAAzZ87k73//O1OnTqVp06bs3r2bJUuWcP/999O0aVNGjx7N9OnTad26NW3atOHJJ58kOzv7tGUmJSUxePBgbr31Vu+A4t27d/Pzzz9zww030Lx5cwzD4MMPP+Tqq6+mVq1aREdHc++99zJmzBg8Hg+XXXYZOTk5rFmzhpiYGAYPHszw4cOZNWsW9913H7fffjubNm1i0aJF5XqdeXl5ZGVlUVRUxK5du3jttdd48cUXSUlJ8YbB1q1b88orr7BixQpatGjBq6++ysaNG70tOyWvbcWKFWzfvp1zzjkHh8NB69atyczMZPHixXTq1ImPPvqIpUuXVuSvRSQoCpwuhry0kc17snG5PUHd6RqKB/+uuk8tMhIaCjfBYo0I+hibiqpduzarVq1i3LhxXHfddeTl5ZGQkMCf/vQnb0vOPffcw4EDBxg8eDAWi4Vbb72Va6+9lpycnNOW++yzz/Lggw8yYsQIfv31V5o1a8aDDz4IQEJCAlOmTOGBBx5g6NChDBo0iEWLFjFt2jQaNmxISkoKO3fuJDY2lksuucR7XbNmzfj3v//NmDFjmDNnDp07d+axxx7j1ltvPevrnDhxIhMnTsRmsxEfH8+ll15KWlqad1o6wD//+U82b95M//79MQyDm266iREjRvDxxx97zxk2bBgrV66kY8eO5Ofn8/nnn/O3v/2NMWPGcNddd1FYWMg111zDhAkTSo11EgmVEzeMDAaLAXXtEfyhifZXkqrFME83MjRM5ebm4nA4yMnJOaUrpqCggF27dtGiRQuiovR/GxJ4eo9JoBU4Xdzy4nq+yswGihcvi4oAp8fAFYBxMgZgsRi4PSaGAZ2axfLKbV0UZKTSnen7+2R6d4qIVCMnDviNtBqYJhSdEGI8wFEX/D6hwXcGEGmFS5rV14q+Ui3pHSsiUgXlH3NyZeqXZ1y51xmAgTM2q0FctJ3rOzTVRpESNhRuRERCzOX2MGvF9zy/ehduD1gN8JgVaXs5lWEUd1k1jq3F8n9dRt1agdlbTqQqUrgREalELreHeZ//xIZdv+Jye9iWlceRQlep2UsVbZCpfXzMTW3tei01lMJNGWrYGGupRHpv1Swut4c5aTv499d72JddENCWGCjuUmpQx8avR4uIirQyqEtzRl+hriURhZsTREZGAnD06FG/VtoVORuns3hFNKvVGuKaSDC43B6e+s92nlu1E1cQc2zHZrG8drtmLImcjv5lnMBqtRIbG8vPP/8MFK8Dc+LeQyIV4fF4+OWXX6hdu/YpW2NI9VLStbQx4zfaJcSwZPM+DuQWBqx8A4iPsfNbSYvMpc3UtSTiA33CnqRkx+eSgCMSSBaLhWbNmik0VyMlXUtLNu/lYG4BRR6TE3sXV+84VKHy7RHFgaVhXTvN6teiS8sGjOzVSkFGpAIUbk5iGAaNGzcmLi6OoqKiUFdHwozNZsNi0ZdWVedye3jqk+28uDqDQpcnaM8zrFsS466+QEFGJMAUbk7DarVqXIRIGDs5wBgUbycQ6D2XrAZ0bB5L13Pj1CIjUkkUbkSkRjiUd5Su07/A6S67JcbE/2BT124lv/D3HeM7Jsbw2rBkDfgVCRH9yxORsHPiFgWBbIixAKYBplk8ViYu2s51lyRoZV+RKkbhRkSqtZLp1y+uycB5fHxMMNaTGXZZC8Zceb5CjEg1oHAjItWGy+1h9qc/8HL6bvIKXIHdnoDSY26axNj5z5jLtU2BSDWkcCMiVVZJmFm0NoP8QnfAW2QA6kRaWPNAL2LrRAWhdBEJBYUbEakSXG4Pcz77kSVf7yP7qJNCl+eUNWUqwmoABjR2aONIkXCncCMilS7/mJMrU1exPydwq/qWMCgecxMTFcGq+3qoRUakBlK4EZGgKnC6GPLSRjbvOUxhEDdcSoiNYsXo7mqRERGFGxEJnAKni0EL1rMxMztg3UknKhn0G2E1+GNiPRYN7aS1ZETkFCH/VJg3bx4zZ84kKyuLdu3aMWfOHDp37nza87Ozs3nooYdYsmQJv/32G82bNyc1NZWrr766EmstIiWbR67+IYuNmbkBLTvSUrwVituEOjYrg5Oba+NIESm3kIabt956i7FjxzJ//ny6dOlCamoqffv2Zfv27cTFxZ1yvtPp5IorriAuLo53332XhIQEdu/eTWxsbOVXXqQGKRnsu3TzfjymyW/5BRwtCnzTTIQB/+zeijF9FWRExH+GaQaj8bh8unTpQqdOnZg7dy4AHo+HxMRERo0axQMPPHDK+fPnz2fmzJl8//33REZG+vWcubm5OBwOcnJyiImJqVD9RcJVgdPFwBfXsTEzJ2jPEW23MqRrklpkRKRcfPn+DlnLjdPpZNOmTYwfP957zGKx0KdPH9LT08u85oMPPiA5OZmRI0fy/vvv07BhQ26++WbGjRt32k0uCwsLKSz8fUZGbm5gm89FqjuX28OctB0s2byXg7nHcLrPfo2vbFaDYd1bMOYKrfArIsEXsnBz6NAh3G43jRo1KnW8UaNGfP/992Ves3PnTj777DMGDBjAsmXL2LFjByNGjKCoqIhJkyaVeU1KSgpTpkwJeP1FqqMTtyoo2QnbAMreStJ/apURkVAK+YBiX3g8HuLi4nj++eexWq106NCBffv2MXPmzNOGm/HjxzN27Fjv77m5uSQmJlZWlUVCKvtIAd1nfEFeoavMx00qvg+TpmCLSFUTsnDToEEDrFYrBw8eLHX84MGDxMfHl3lN48aNiYyMLNUFdcEFF5CVlYXT6cRmO/XD1W63Y7fbA1t5kSrI5fYwa8X3PL96F+5AN8WcoFPzWF69rYumYItIlRWyTyebzUaHDh1IS0ujX79+QHHLTFpaGnfddVeZ13Tr1o033ngDj8eDxVLc1P3DDz/QuHHjMoONSLhyuT3MWr6d51fv9G70GGFARdbIs1mL1/Y9ecxN5+b1eOW2zgozIlJthPTTauzYsQwePJiOHTvSuXNnUlNTOXLkCEOHDgVg0KBBJCQkkJKSAsCdd97J3LlzGT16NKNGjeLHH3/kscce41//+lcoX4ZI0JWs8vt15mGc7rITjL/BxmaBS5rX14J4IhI2QvpJ1r9/f3755RcmTpxIVlYW7du3Z/ny5d5BxpmZmd4WGoDExERWrFjBmDFjaNu2LQkJCYwePZpx48aF6iWIBFz+MSdXzV7N3uxjWA04TZapsKaxtVg+WhtIikj4Cek6N6GgdW6kKilwuhi0cAMbMw5XeGDv6VgtBndclsQ9fdto5pKIVFvVYp0bkZqoZMuC9J9+YfOeHApdgRv5WyfSoMBlggEdm9Xj5Vs1TkZEaiZ98okESfEmkhvYsPtw0J7DZjVY+0BPGkTXDtpziIhUNwo3IgHicnuYuXwbz32ZEbAyTx5zE2238uX9PYmtExWw5xARCTcKNyJ++H2l310UVmT+dRmsBnRsrm4lERF/6ZNT5CzyjznpO/tL9mcXYDGKV/T1BDDPWC0GdSIt/CEhlpeGdFSgERGpIH2KipygZMDv+p2H2P3bUbJyCkp1CwVqWnZ0VARf3tdD3UsiIkGgcCM1Wv4xJ1c+tYr9uYVnP9lPdW0WVo/rpSAjIlJJFG6kxsk+UkD3mV+QV1D2ZpIV1alZLK/err2XRERCRZ++EraKp2KvZ8Pu7ICWazF+H3PTJMbOf8ZcrlV+RUSqEIUbCQsFThe3vLierzKzg1K+BeiUpBlMIiLVgT6lpVpyuT3M+exH3v1qD/tygjNeRmvKiIhUTwo3Ui2U7Iq9ec9hXG4TDAN3AOdja/aSiEj4ULiRKiX/mJMrU1ex/4TWmEgLWAyDwhPnYVdwv9eE2ChWjO6usTIiImFI4UZCKv+Ykyue+oIDuc7TnlPkAfzcM9sAOjWP5ZXbNHtJRKSm0Ke9VBpv11LmYTAM7BEWcis4HTvSUhJ+ijVxRPGfu9UiIyJSkyncSFC43B7mpO1gyea9ZB91Uujy4Cy1vK9Joctz2utPx2oU/zfCauGPibEsGtpJLTIiIlKKvhUkIErCzNsbMziQVxTQsiMtEO+oxXWXJDCqd2sirJaAli8iIuFF4UZ8VuB0MXjhBr7KzA7ojKUTNY2txfLRl6l7SUREfKZwI2dVspnkxozf+GOig4Wrd3GkyPcupZNZjeLuJUyTPzarpy4mEREJCH2TSCklQWbtjp/ZuDubkxtmVu845Fe5VqP0jtodmjl4/fZLFWZERCTg9M1Sw+Ufc9J39mr2ZR8LSvkJsVH8o0NTjZUREZFKo3BTw7jcHh7/+DteWL07KOVrcTwREQk1hZswduJ07JxjRRQUuU+aju27hNgoGkfb+WpPDqBtC0REpOpRuAkz+cecXHW8mynQ85guSXTwxjCNkxERkapN31LV1KG8oyRPX0lRBVtiTsdmNRjWvQVjrjhfY2VERKRaUbipJgqcLgYtWM+G3dlBKd9mNVj7QE8aRNcOSvkiIiKVReGmCnK5Pcz+9AdeTt/NkUIXJsWbYFekjcYAIq0Gbo9JHXsEg5ObM7rPeWqVERGRsKNwE2Iut4fZn/zIovRd5BW6g/IcWu1XRERqEoWbEMg/5uSqp1ez93Bw1pbp1MzBq1ogT0REaih9+wVR9pECLp/5BbkFrqA9R+MYO5+MuVytMiIiIscp3ARIyTiZRWszyHe6oYJjZEoYgGFApNXCHxNjtf+SiIjIWehbMkDmpO1gzuc/Baw8e4SF27slMeZKTcUWERHxhcJNgCzdss+v6wyjeCZUTFQEq7TSr4iISIUp3IRIkxg7/9FYGRERkYBTuAmQa//YhNlpO0odi4nSejIiIiKVTeEmQEb1bo3FsLAx4zc6JdVnZK9WCjQiIiIhoHATIBFWC6P7tA51NURERGo8NS2IiIhIWFG4ERERkbCicCMiIiJhReFGREREworCjYiIiIQVhRsREREJKwo3IiIiElYUbkRERCSsKNyIiIhIWFG4ERERkbCicCMiIiJhReFGREREworCjYiIiIQVhRsREREJKwo3IiIiElYUbkRERCSsKNyIiIhIWFG4ERERkbCicCMiIiJhReFGREREworCjYiIiIQVhRsREREJKwo3IiIiElYUbkRERCSsKNyIiIhIWFG4ERERkbCicCMiIiJhReFGREREworCjYiIiIQVhRsREREJKwo3IiIiElYUbkRERCSsVIlwM2/ePJKSkoiKiqJLly5s2LChXNctXrwYwzDo169fcCsoIiIi1UbIw81bb73F2LFjmTRpEl9//TXt2rWjb9++/Pzzz2e8LiMjg3vvvZfu3btXUk1FRESkOgh5uHnyyScZNmwYQ4cO5cILL2T+/PnUrl2bhQsXnvYat9vNgAEDmDJlCi1btjxj+YWFheTm5pb6ERERkfAV0nDjdDrZtGkTffr08R6zWCz06dOH9PT00143depU4uLiuO222876HCkpKTgcDu9PYmJiQOouIiIiVVNIw82hQ4dwu900atSo1PFGjRqRlZVV5jWrV69mwYIFvPDCC+V6jvHjx5OTk+P92bNnT4XrLSIiIlVXRKgr4Iu8vDwGDhzICy+8QIMGDcp1jd1ux263B7lmIiIiUlWENNw0aNAAq9XKwYMHSx0/ePAg8fHxp5z/008/kZGRwV//+lfvMY/HA0BERATbt2+nVatWwa20iIiIVGkh7Zay2Wx06NCBtLQ07zGPx0NaWhrJycmnnN+mTRu2bt3Kli1bvD9/+9vf6NWrF1u2bNF4GhEREQl9t9TYsWMZPHgwHTt2pHPnzqSmpnLkyBGGDh0KwKBBg0hISCAlJYWoqCguuuiiUtfHxsYCnHJcREREaqaQh5v+/fvzyy+/MHHiRLKysmjfvj3Lly/3DjLOzMzEYgn5jHURERGpJgzTNM1QV6Iy5ebm4nA4yMnJISYmJtTVERERkXLw5ftbTSIiIiISVhRuREREJKwo3IiIiEhYUbgRERGRsKJwIyIiImFF4UZERETCisKNiIiIhBWFGxEREQkrfoWbli1b8uuvv55yPDs7m5YtW1a4UiIiIiL+8ivcZGRk4Ha7TzleWFjIvn37KlwpEREREX/5tLfUBx984P3zihUrcDgc3t/dbjdpaWkkJSUFrHIiIiIivvIp3PTr1w8AwzAYPHhwqcciIyNJSkpi1qxZAauciIiIiK98CjcejweAFi1asHHjRho0aBCUSomIiIj4y6dwU2LXrl2BroeIiIhIQPgVbqZOnXrGxydOnOhXZUREREQqyq9ws3Tp0lK/FxUVsWvXLiIiImjVqpXCjYiIiISMX+Fm8+bNpxzLzc1lyJAhXHvttRWulIiIiIi/ArZCcUxMDFOmTGHChAmBKlJERETEZwHdfiEnJ4ecnJxAFikiIiLiE7+6pZ5++ulSv5umyYEDB3j11Vf585//HJCKiYiIiPjDr3Dz1FNPlfrdYrHQsGFDBg8ezPjx4wNSMRERERF/aJ0bERERCSsVHnOzZ88e9uzZE4i6iIiIiFSYX+HG5XIxYcIEHA4HSUlJJCUl4XA4ePjhhykqKgp0HUVERETKza9uqVGjRrFkyRJmzJhBcnIyAOnp6UyePJlff/2VZ599NqCVFBERESkvwzRN09eLHA4HixcvPmVm1LJly7jpppuq9HTw3NxcHA4HOTk5xMTEhLo6IiIiUg6+fH/71S1lt9tJSko65XiLFi2w2Wz+FCkiIiISEH6Fm7vuuotp06ZRWFjoPVZYWMijjz7KXXfdFbDKiYiIiPjK772l0tLSaNq0Ke3atQPgm2++wel08qc//YnrrrvOe+6SJUsCU1MRERGRcvAr3MTGxnL99deXOpaYmBiQComIiIhUhF/h5qWXXgp0PUREREQCwq8xN7179yY7O/uU47m5ufTu3buidRIRERHxm1/hZuXKlTidzlOOFxQU8OWXX1a4UiIiIiL+8qlb6ttvv/X++bvvviMrK8v7u9vtZvny5SQkJASudiIiIiI+8inctG/fHsMwMAyjzO6nWrVqMWfOnIBVTkRERMRXPoWbXbt2YZomLVu2ZMOGDTRs2ND7mM1mIy4uDqvVGvBKioiIiJSXT+GmefPmAHg8nqBURkRERKSi/JoK/sorr5zx8UGDBvlVGREREZGK8mvjzHr16pX6vaioiKNHj2Kz2ahduza//fZbwCoYaNo4U0REpPoJ+saZhw8fLvWTn5/P9u3bueyyy3jzzTf9qrSIiIhIIPgVbsrSunVrpk+fzujRowNVpIiIiIjPAhZuACIiIti/f38gixQRERHxiV8Dij/44INSv5umyYEDB5g7dy7dunULSMVERERE/OFXuOnXr1+p3w3DoGHDhvTu3ZtZs2YFol4iIiIifvEr3JSsc/PLL78AlFrMT0RERCSUfB5zk52dzciRI2nQoAHx8fHEx8fToEED7rrrrjJ3ChcRERGpTD613Pz2228kJyezb98+BgwYwAUXXAAUb6K5aNEi0tLSWLt27Snr4IiIiIhUFp/CzdSpU7HZbPz00080atTolMeuvPJKpk6dylNPPRXQSoqIiIiUl0/dUu+99x5PPPHEKcEGID4+nhkzZrB06dKAVU5ERETEVz6FmwMHDvCHP/zhtI9fdNFFZGVlVbhSIiIiIv7yKdw0aNCAjIyM0z6+a9cu6tevX9E6iYiIiPjNp3DTt29fHnroIZxO5ymPFRYWMmHCBK666qqAVU5ERETEVz7tCr537146duyI3W5n5MiRtGnTBtM02bZtG8888wyFhYV89dVXJCYmBrPOFaJdwUVERKofX76/fZot1bRpU9LT0xkxYgTjx4+nJBcZhsEVV1zB3Llzq3SwERERkfDn8wrFLVq04OOPP+bw4cP8+OOPAJx77rkaayMiIiJVgl/bLwDUq1ePzp07B7IuIiIiIhXm8/YLIiIiIlWZwo2IiIiEFYUbERERCSsKNyIiIhJWFG5EREQkrCjciIiISFhRuBEREZGwonAjIiIiYUXhRkRERMKKwo2IiIiEFYUbERERCSsKNyIiIhJWqkS4mTdvHklJSURFRdGlSxc2bNhw2nNfeOEFunfvTr169ahXrx59+vQ54/kiIiJSs4Q83Lz11luMHTuWSZMm8fXXX9OuXTv69u3Lzz//XOb5K1eu5KabbuLzzz8nPT2dxMRErrzySvbt21fJNRcREZGqyDBN0wxlBbp06UKnTp2YO3cuAB6Ph8TEREaNGsUDDzxw1uvdbjf16tVj7ty5DBo06Kzn5+bm4nA4yMnJISYmpsL1FxERkeDz5fs7pC03TqeTTZs20adPH+8xi8VCnz59SE9PL1cZR48epaioiPr165f5eGFhIbm5uaV+REREJHyFNNwcOnQIt9tNo0aNSh1v1KgRWVlZ5Spj3LhxNGnSpFRAOlFKSgoOh8P7k5iYWOF6i4iISNUV8jE3FTF9+nQWL17M0qVLiYqKKvOc8ePHk5OT4/3Zs2dPJddSRESkBigqgJeugUcawfTm8Nmj4HaFpCoRIXnW4xo0aIDVauXgwYOljh88eJD4+PgzXvvEE08wffp0Pv30U9q2bXva8+x2O3a7PSD1FRERkRMUFcBr18O+r8BdBKa7+LirAFbNAEsE9BxX6dUKacuNzWajQ4cOpKWleY95PB7S0tJITk4+7XUzZsxg2rRpLF++nI4dO1ZGVUVERASKW2M+S4HUi+HRRrB7dXGYKQk2J8os3/jZQAtpyw3A2LFjGTx4MB07dqRz586kpqZy5MgRhg4dCsCgQYNISEggJSUFgMcff5yJEyfyxhtvkJSU5B2bU7duXerWrRuy1yEiIhK23K7ibqa1s8sOMafT7PQNFcEU8nDTv39/fvnlFyZOnEhWVhbt27dn+fLl3kHGmZmZWCy/NzA9++yzOJ1O/vGPf5QqZ9KkSUyePLkyqy4iIhLeCvNhXhfI3ev7tc27Qfd7Al+ncgj5OjeVTevciIiInMGx7OIup0I/lk5p1g2skdC8a3GwsQauDcWX7++Qt9yIiIhICFUkzJSwx8DdW6FWbKBqVSEKNyIiIjVNUQG8eh3sWQ+mn9O1mybD4PcgsuylWEJJ4UZERKQmcLtg1UzY8ibkZAJ+jkrpdjf0nhDQLqdAq7o1ExERkYpxu2BlCqydA+5CPwsxisfQ3LKkSrbSlEXhRkREJJyULKy3dyO4nfjVQmO1wdhtUKdBwKtXGRRuREREqju3C1Y+Dhvm+z8wuFk3GFh9WmfOROFGRESkOirMh3mXQm4F9ky0REDTLmETakoo3IiIiFQnRQXw+j8gYw3g8a8MRyKMWAf28FzZX+FGRESkKisZFJw+t3gPJ8MAn9bfNSDKAZ3vgB7jqvQsp0AJ/1coIiJS3XjXoVl36l5O5Q02Vjt0HQU9x9eIQHOimvVqRUREqqqS7qa9m8GV7385TbrA0A/CagyNrxRuREREQqmoAF65Fvas9e26iCho2gkGvFujg0xZFG5EREQqU0V22gYwrHDZmBrZ3VReuisiIiLBVlQAL/eDven+l1GDx9D4SndHREQk0Nwu+GImbHm9YuvQYEC30VV+L6eqRndKREQkENwu+PxRWPs0ePzcaRvCfg2ayqBwIyIiUlFuF7zyN9i9xr/rY5rCyPUKNAGicCMiIuILtwtWzYRv3oScvaeuQ1Ne0U3hLgWaYFC4ERERKQ/vOjQbi1cK9kfCpTDkfU3dDjKFGxERkbIUFcBr18HudWCYgOFfK43dAXd/C7ViA11DOQ2FGxERkRIlrTNZW6HoGLgLi4+XdysnwwrWSEjoCLf8Wy00IaJwIyIiNVdJ60zmOjA9lD/FnMyAZskwcKkCTRWgcCMiIjWPt8vJj9lNEVFQNw7qtYDm3aD7PVqDporR34aIiNQMbhd88ThseB4Ksn2/3movXlCvxziFmSpOfzsiIhKe3C5Y+ThseA4Kc3y/3rCCoykYBlx8I/S4T6GmmtDfkoiIhI+SbQ++eRPy9vm5UrBxfJXgdK1BU00p3IiISPV1YleTMx9M0/9F9bTtQdhQuBERkeqlqABevRYy0/F7dpMRAfY60PmfGkMThvS3KSIiVZ/bBV/Ogl1fQuaa49O2/WCLhjH/1YJ6YU7hRkREqp4Tu5tcBVC7IeTu8a0MS0TxjxbUq3EUbkREpGoozIdnkyF3P0TWKT3DyZdg42gO7W7S7KYaTH/rIiISWmUtqOfz1G3NcJLfKdyIiEjlOpYNqRdDYW7FyomIguS7oOd4tdBIKXo3iIhI8B05BE9eAG6nf9fHJEL7m2DvxuI9nLTlgZyB3hkiIhJ4RQXw6nWwZ51/685Y7ZA8CvZvUpgRn+mdIiIigVHh7iYDLFZo2gUGLtHsJvGbwo2IiPinZO2Znatg/0ZwFfpZUAR0v1tjZyRg9C4SEZHyK5munb0P8HObAyjelDLxUrXQSFAo3IiIyJkVFcDL/WBvesXLssfA3Vu1QrAElcKNiIicyu2CVTNhyxuQswe/9nAyrNDsUrhFrTNSuRRuRESkuLvpmWT/g0yJKAeM/lYtMxJSCjciIjVVoLqbtJieVDF6F4qI1BTezSifg8I8/3fWLtF1FPxpsgKNVDl6R4qIhKuiAnj9H3Dg2+Kdtd1+TtU2LGCa2rtJqg2FGxGRcFJUAK9dD/u+AneRf6sDlzAs0PVf0HuCWmekWtG7VUSkujtyCJ66sAKL6B1nWKHbaOj1kMKMVGt694qIVDclKwNnrIbfdkLuXv/Lsjugyz+hxzgFGgkbeieLiFQHJeNnsraCMw88fnY3GRZoeikMWqq1ZyRsKdyIiFRFbhd8/iisTgX8nNVktUPduOKZUfEXw4B3FWikRlC4ERGpKk7sbjqwxf/dtRO7Qase0P0edTVJjaR3vYhIqJzY1eRxF3c3+cqwFk/TxnN8qvY6TdWWGk/hRkSkMp0YaFzHKjDDyQqxCXCn1p0ROZnCjYhIMLldsDIF1jwNHmfFy7PaYex3UKdBxcsSCVMKNyIigVRUAC//Hfauq2BBxvH/mhCTCCPV3SRSXgo3IiIVdeQQPHmh/9sbAFht4D7esmOPgbu3amdtET8p3IiI+KpkVlNmOiR0gC+f8L+sFj2geTfNbBIJIP1LEhEpj6ICePU62LsePK7fj+/83L/yDGvx2Jno+MDUT0S8FG5ERMpSmA/PJEPOnuJVfcH/TSjtDmjcVovoiVQShRsRETi+InAKrJt7wvRs8/h/fAw1RgQ4EqDdTXD5fepuEqlk+hcnIjWXt3Ums2Ll2B1QdARimmjdGZEqQOFGRGoOtws+mwZrZuNtlfGX1V48XkatMyJVjv41ikj4KpnVtOvL4r2a/NneAACjeHp2rXrQ7kaFGZEqTv86RST8BKK7ybAWT9FOukzTtEWqGf1rFZHqrWR7g/QyBgL7KsIOGJDQEW75t2Y2iVRTCjciUr24XfDF47DhOSjICUCBFrjsbuj1kFpnRMKE/iWLSNXmdsHK42HGeQRM19mvKY9ud0PvCQo0ImFI/6pFpOopKoDXroe9G8HjrligMSzF684kdlZXk0gNoXAjIqHndsEXM+HbN4u7mpz5pbc48IkBmBDlgNHfavNJkRrIEuoKAMybN4+kpCSioqLo0qULGzZsOOP577zzDm3atCEqKoqLL76YZcuWVVJNRSQgCvPhyT/AZEfxz7RzYNV0yN4NBdn+BRtHIozfB5OzYXIOPJCpYCNSQ4W85eatt95i7NixzJ8/ny5dupCamkrfvn3Zvn07cXFxp5y/du1abrrpJlJSUvjLX/7CG2+8Qb9+/fj666+56KKLQvAKROSs3C74/FFYPRvwc3+mk1ntkDwKeo3XuBkRKcUwTbOCy3RWTJcuXejUqRNz584FwOPxkJiYyKhRo3jggQdOOb9///4cOXKEDz/80Hvs0ksvpX379syfP/+sz5ebm4vD4SAnJ4eYmJjAvRAR+V3J9Oy1c8BdePbzz8SIAHsdiIqFtjdBDy2gJ1IT+fL9HdJPCKfTyaZNmxg/frz3mMVioU+fPqSnp5d5TXp6OmPHji11rG/fvrz33ntlnl9YWEhh4e8frrm5uRWvuIicqqgAXrkW9qytWDmGpXg14CiHtjYQEb+E9BPj0KFDuN1uGjVqVOp4o0aN+P7778u8Jisrq8zzs7Kyyjw/JSWFKVOmBKbCIvK7kq0NdnwOe8v+nxGf1U2AURu08aSIVEjY/+/Q+PHjS7X05ObmkpiYGMIaiVRTJWEm40vYtxmK8itWni0GLh0OPcapZUZEAiqknygNGjTAarVy8ODBUscPHjxIfHx8mdfEx8f7dL7dbsdutwemwiI1SVEBvNIP9gSoVSYiStsaiEilCOlUcJvNRocOHUhLS/Me83g8pKWlkZycXOY1ycnJpc4H+OSTT057voj4oDAfnryoeHr2o40qHmysdrjsXpjwKzx8EIZ+pGAjIkEX8rbgsWPHMnjwYDp27Ejnzp1JTU3lyJEjDB06FIBBgwaRkJBASkoKAKNHj6ZHjx7MmjWLa665hsWLF/PVV1/x/PPPh/JliFRfJy6gl7274uU17QyD/08hRkRCJuThpn///vzyyy9MnDiRrKws2rdvz/Lly72DhjMzM7FYfm9g6tq1K2+88QYPP/wwDz74IK1bt+a9997TGjciZ1Oy1sya2WCeuNaMBfD4X64jEUas0yBgEakyQr7OTWXTOjdSY5w2zFSQoymMWK8wIyKVqtqscyMiAXYsG1IvhsIAr+fULBkGvqeuJhGpFhRuRKortwu+eBw2PA+uguKVfCsyPdtqB8Mo/rNmNYlINaZwI1KdFObD3M6Qty9wZVpskNhZYUZEwobCjUhVdiy7ePfsii6YV4oBY/5bPHZGRCQMKdyIVBVFBfD6PyBrKxQdq/iGkyXsDrj7W6gVG5jyRESqOIUbkVAK+ABgAzA1PVtEajSFG5HKciwbUttCYQ4YVsACZlFgynY0gxHpCjMiIijciARPUQG8cu3xLQxOWk7KdAN+rj1ji4ZL79SGkyIip6FPRpFAcbvgs2mwJjUAhR3vXrJEQEwCtLsJLr9PYUZEpBz0SSnir8J8eCYZcjIDW26zrjBwqaZli4j4SeFGpDzcLlg1E755E3L2BmY7AyOyeMxNlANGazaTiEigKNyIlMW7+u9zUJDLKWNm/GVYIfFSGLhELTMiIkGicCMCxV1M8y6F3D2BLzsyGsb+Vy0zIiKVROFGaqaiAni5H+xND3zZickw6D21zIiIhIjCjYS/ki6m9c8dXywvQF1MAIYNLvsX9ByvmUwiIlWEPo0l/BTmwzOXQk4Qupi08q+ISJWncCPVX7DGyxhWaHYptOgB3e9Ry4yISDWhT2upnkqmZm9+PbChxh4Dd2/V4F8RkWpM4UaqNrcLvpwFu9fAbzsh/2fwuMAwiv/rt+MrAGuNGRGRsKNwI1WH2wUrU2DN0+Bxnvlcf8YEW+3QdZQG/4qIhDl9wkvoeDeWXBuc8rvdDb0nKMiIiNQw+tSXyuF2wWePQvps8Lgp7hYyAE/FyrXYwHSB6QG7A+5WF5OISE2ncCPB4XbBFzPh2zfhWDY480/aj8nE574lewy4CovH2lgjIaEj3PJvLZYnIiKlKNxIxXk3lVwMR38FZ15gy7fFwKXDocc4dTGJiMhZ6ZtCfJeXBbPaENCVfr2M4q6mxE5qlREREb8o3MjZlUzHzkyHpp1g1YzAlGuPgVr1oe2N0OM+tcqIiEhA6NtESisqgNf/AVnfQkEZ+zDt/Ny/cg3r72NuLDa4ZxvUaVChqoqIiJRF4aamC+Y+TACOZtD+ZrhcLTMiIlI59G1Tk5QM/P36NcjbG/jyS/ZiumWJxsqIiEjIKNyEq8J8eCa5uEXGYoWI2uA6cnyNmQpwJEJsc/j5f9DoIhjwroKMiIhUKQo34cDtgi8ehw3Pg/No8b5L7sLfH/e4wJlb8edp2hkG/5/CjIiIVGkKN9VNyf5La58G91n2X6qI6KZw13qw1w3ec4iIiASBwk1V5529tLW4SynQC+SV0AwmEREJEwo3VYl3nExm8TgZI6J095K/DAtE1gVPYfHM7qZaIE9ERMKXwk0oHcuG1LZQmHPqYx43UJHBvwZE2OHSu6DXeE3DFhGRGkPfeJWhMB/mdYHcE6dfW6jwjthlcSTCiHUaKyMiIjWWwk0wlGxXsGsV7N14mq4lP4KNxQaeEwYRxzSFkRr0KyIiciKFm0ApKoDXroPd66hYd9IJrPbiady2utDln9oVW0REpBz0TRkor/8Ddq+peDlGBCR2gYFa5VdERMQfCjeBcvC/Pl5gKZ7F5GgCd6ara0lERCRAFG4CpdFFkPFlGQ8YgFkcZKIT4I8DtImkiIhIEOkbNlAGvHvCmBtPcZhJvFTdSyIiIpVM4SZQIqNg6LJQ10JERKTGs4S6AiIiIiKBpHAjIiIiYUXhRkRERMKKwo2IiIiEFYUbERERCSsKNyIiIhJWFG5EREQkrCjciIiISFhRuBEREZGwonAjIiIiYUXhRkRERMKKwo2IiIiElRq3caZpmgDk5uaGuCYiIiJSXiXf2yXf42dS48JNXl4eAImJiSGuiYiIiPgqLy8Ph8NxxnMMszwRKIx4PB72799PdHQ0hmEEtOzc3FwSExPZs2cPMTExAS1bfqf7XDl0nyuH7nPl0b2uHMG6z6ZpkpeXR5MmTbBYzjyqpsa13FgsFpo2bRrU54iJidE/nEqg+1w5dJ8rh+5z5dG9rhzBuM9na7EpoQHFIiIiElYUbkRERCSsKNwEkN1uZ9KkSdjt9lBXJazpPlcO3efKoftceXSvK0dVuM81bkCxiIiIhDe13IiIiEhYUbgRERGRsKJwIyIiImFF4UZERETCisKNj+bNm0dSUhJRUVF06dKFDRs2nPH8d955hzZt2hAVFcXFF1/MsmXLKqmm1Zsv9/mFF16ge/fu1KtXj3r16tGnT5+z/r1IMV/fzyUWL16MYRj069cvuBUME77e5+zsbEaOHEnjxo2x2+2cd955+uwoB1/vc2pqKueffz61atUiMTGRMWPGUFBQUEm1rZ5WrVrFX//6V5o0aYJhGLz33ntnvWblypVccskl2O12zj33XBYtWhT0emJKuS1evNi02WzmwoULzf/973/msGHDzNjYWPPgwYNlnr9mzRrTarWaM2bMML/77jvz4YcfNiMjI82tW7dWcs2rF1/v880332zOmzfP3Lx5s7lt2zZzyJAhpsPhMPfu3VvJNa9efL3PJXbt2mUmJCSY3bt3N//+979XTmWrMV/vc2FhodmxY0fz6quvNlevXm3u2rXLXLlypblly5ZKrnn14ut9fv3110273W6+/vrr5q5du8wVK1aYjRs3NseMGVPJNa9eli1bZj700EPmkiVLTMBcunTpGc/fuXOnWbt2bXPs2LHmd999Z86ZM8e0Wq3m8uXLg1pPhRsfdO7c2Rw5cqT3d7fbbTZp0sRMSUkp8/wbbrjBvOaaa0od69Kli/nPf/4zqPWs7ny9zydzuVxmdHS0+fLLLwerimHBn/vscrnMrl27mi+++KI5ePBghZty8PU+P/vss2bLli1Np9NZWVUMC77e55EjR5q9e/cudWzs2LFmt27dglrPcFKecHP//febf/jDH0od69+/v9m3b98g1sw01S1VTk6nk02bNtGnTx/vMYvFQp8+fUhPTy/zmvT09FLnA/Tt2/e054t/9/lkR48epaioiPr16wermtWev/d56tSpxMXFcdttt1VGNas9f+7zBx98QHJyMiNHjqRRo0ZcdNFFPPbYY7jd7sqqdrXjz33u2rUrmzZt8nZd7dy5k2XLlnH11VdXSp1rilB9D9a4jTP9dejQIdxuN40aNSp1vFGjRnz//fdlXpOVlVXm+VlZWUGrZ3Xnz30+2bhx42jSpMkp/6Dkd/7c59WrV7NgwQK2bNlSCTUMD/7c5507d/LZZ58xYMAAli1bxo4dOxgxYgRFRUVMmjSpMqpd7fhzn2+++WYOHTrEZZddhmmauFwuhg8fzoMPPlgZVa4xTvc9mJuby7Fjx6hVq1ZQnlctNxJWpk+fzuLFi1m6dClRUVGhrk7YyMvLY+DAgbzwwgs0aNAg1NUJax6Ph7i4OJ5//nk6dOhA//79eeihh5g/f36oqxZWVq5cyWOPPcYzzzzD119/zZIlS/joo4+YNm1aqKsmAaCWm3Jq0KABVquVgwcPljp+8OBB4uPjy7wmPj7ep/PFv/tc4oknnmD69Ol8+umntG3bNpjVrPZ8vc8//fQTGRkZ/PWvf/Ue83g8AERERLB9+3ZatWoV3EpXQ/68nxs3bkxkZCRWq9V77IILLiArKwun04nNZgtqnasjf+7zhAkTGDhwILfffjsAF198MUeOHOGOO+7goYcewmLR//sHwum+B2NiYoLWagNquSk3m81Ghw4dSEtL8x7zeDykpaWRnJxc5jXJycmlzgf45JNPTnu++HefAWbMmMG0adNYvnw5HTt2rIyqVmu+3uc2bdqwdetWtmzZ4v3529/+Rq9evdiyZQuJiYmVWf1qw5/3c7du3dixY4c3PAL88MMPNG7cWMHmNPy5z0ePHj0lwJQESlNbLgZMyL4HgzpcOcwsXrzYtNvt5qJFi8zvvvvOvOOOO8zY2FgzKyvLNE3THDhwoPnAAw94z1+zZo0ZERFhPvHEE+a2bdvMSZMmaSp4Ofh6n6dPn27abDbz3XffNQ8cOOD9ycvLC9VLqBZ8vc8n02yp8vH1PmdmZprR0dHmXXfdZW7fvt388MMPzbi4OPORRx4J1UuoFny9z5MmTTKjo6PNN99809y5c6f5n//8x2zVqpV5ww03hOolVAt5eXnm5s2bzc2bN5uA+eSTT5qbN282d+/ebZqmaT7wwAPmwIEDveeXTAW/7777zG3btpnz5s3TVPCqaM6cOWazZs1Mm81mdu7c2Vy3bp33sR49epiDBw8udf7bb79tnnfeeabNZjP/8Ic/mB999FEl17h68uU+N2/e3ARO+Zk0aVLlV7ya8fX9fCKFm/Lz9T6vXbvW7NKli2m3282WLVuajz76qOlyuSq51tWPL/e5qKjInDx5stmqVSszKirKTExMNEeMGGEePny48itejXz++edlft6W3NvBgwebPXr0OOWa9u3bmzabzWzZsqX50ksvBb2ehmmq/U1ERETCh8bciIiISFhRuBEREZGwonAjIiIiYUXhRkRERMKKwo2IiIiEFYUbERERCSsKNyIiIhJWFG5EREQkrCjciIiISFhRuBGRKmXIkCH069evUp9z0aJFxMbGVupzikjwKNyIiIhIWFG4EZEqq2fPnvzrX//i/vvvp379+sTHxzN58uRS5xiGwbPPPsuf//xnatWqRcuWLXn33Xe9j69cuRLDMMjOzvYe27JlC4ZhkJGRwcqVKxk6dCg5OTkYhoFhGKc8h4hULwo3IlKlvfzyy9SpU4f169czY8YMpk6dyieffFLqnAkTJnD99dfzzTffMGDAAG688Ua2bdtWrvK7du1KamoqMTExHDhwgAMHDnDvvfcG46WISCVRuBGRKq1t27ZMmjSJ1q1bM2jQIDp27EhaWlqpc/7f//t/3H777Zx33nlMmzaNjh07MmfOnHKVb7PZcDgcGIZBfHw88fHx1K1bNxgvRUQqicKNiFRpbdu2LfV748aN+fnnn0sdS05OPuX38rbciEj4UbgRkSotMjKy1O+GYeDxeMp9vcVS/DFnmqb3WFFRUWAqJyJVksKNiFR769atO+X3Cy64AICGDRsCcODAAe/jW7ZsKXW+zWbD7XYHt5IiUmkUbkSk2nvnnXdYuHAhP/zwA5MmTWLDhg3cddddAJx77rkkJiYyefJkfvzxRz766CNmzZpV6vqkpCTy8/NJS0vj0KFDHD16NBQvQ0QCROFGRKq9KVOmsHjxYtq2bcsrr7zCm2++yYUXXggUd2u9+eabfP/997Rt25bHH3+cRx55pNT1Xbt2Zfjw4fTv35+GDRsyY8aMULwMEQkQwzyxI1pEpJoxDIOlS5dW+qrGIlJ1qeVGREREworCjYiIiISViFBXQESkItSzLiInU8uNiIiIhBWFGxEREQkrCjciIiISVhRuREREJKwo3IiIiEhYUbgRERGRsKJwIyIiImFF4UZERETCyv8HUKJUIIxhqMwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load Iris dataset and preprocess\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "learning_rate = 0.00001\n",
        "epochs = 1500\n",
        "\n",
        "\n",
        "input_size = 4\n",
        "hidden1_size = 8\n",
        "hidden2_size = 6\n",
        "hidden3_size = 4\n",
        "output_size = 3\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size,hidden3_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "nn.train(X_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
        "\n",
        "y_pred = nn.feedforward(X_test)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85I_l2M_feBi",
        "outputId": "f6f40922-39fd-4e65-96f9-9a504fcb6ade"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1500, Loss: 24.5142\n",
            "Epoch 1/1500, Loss: 20.1839\n",
            "Epoch 2/1500, Loss: 16.9633\n",
            "Epoch 3/1500, Loss: 14.5262\n",
            "Epoch 4/1500, Loss: 12.6527\n",
            "Epoch 5/1500, Loss: 11.1913\n",
            "Epoch 6/1500, Loss: 10.0348\n",
            "Epoch 7/1500, Loss: 9.1068\n",
            "Epoch 8/1500, Loss: 8.3516\n",
            "Epoch 9/1500, Loss: 7.7283\n",
            "Epoch 10/1500, Loss: 7.2068\n",
            "Epoch 11/1500, Loss: 6.7645\n",
            "Epoch 12/1500, Loss: 6.3846\n",
            "Epoch 13/1500, Loss: 6.0540\n",
            "Epoch 14/1500, Loss: 5.7632\n",
            "Epoch 15/1500, Loss: 5.5046\n",
            "Epoch 16/1500, Loss: 5.2724\n",
            "Epoch 17/1500, Loss: 5.0623\n",
            "Epoch 18/1500, Loss: 4.8706\n",
            "Epoch 19/1500, Loss: 4.6946\n",
            "Epoch 20/1500, Loss: 4.5322\n",
            "Epoch 21/1500, Loss: 4.3815\n",
            "Epoch 22/1500, Loss: 4.2411\n",
            "Epoch 23/1500, Loss: 4.1099\n",
            "Epoch 24/1500, Loss: 3.9869\n",
            "Epoch 25/1500, Loss: 3.8712\n",
            "Epoch 26/1500, Loss: 3.7622\n",
            "Epoch 27/1500, Loss: 3.6594\n",
            "Epoch 28/1500, Loss: 3.5621\n",
            "Epoch 29/1500, Loss: 3.4699\n",
            "Epoch 30/1500, Loss: 3.3826\n",
            "Epoch 31/1500, Loss: 3.2996\n",
            "Epoch 32/1500, Loss: 3.2207\n",
            "Epoch 33/1500, Loss: 3.1457\n",
            "Epoch 34/1500, Loss: 3.0743\n",
            "Epoch 35/1500, Loss: 3.0062\n",
            "Epoch 36/1500, Loss: 2.9413\n",
            "Epoch 37/1500, Loss: 2.8793\n",
            "Epoch 38/1500, Loss: 2.8201\n",
            "Epoch 39/1500, Loss: 2.7635\n",
            "Epoch 40/1500, Loss: 2.7094\n",
            "Epoch 41/1500, Loss: 2.6577\n",
            "Epoch 42/1500, Loss: 2.6081\n",
            "Epoch 43/1500, Loss: 2.5606\n",
            "Epoch 44/1500, Loss: 2.5151\n",
            "Epoch 45/1500, Loss: 2.4715\n",
            "Epoch 46/1500, Loss: 2.4296\n",
            "Epoch 47/1500, Loss: 2.3894\n",
            "Epoch 48/1500, Loss: 2.3508\n",
            "Epoch 49/1500, Loss: 2.3137\n",
            "Epoch 50/1500, Loss: 2.2780\n",
            "Epoch 51/1500, Loss: 2.2437\n",
            "Epoch 52/1500, Loss: 2.2107\n",
            "Epoch 53/1500, Loss: 2.1790\n",
            "Epoch 54/1500, Loss: 2.1484\n",
            "Epoch 55/1500, Loss: 2.1190\n",
            "Epoch 56/1500, Loss: 2.0906\n",
            "Epoch 57/1500, Loss: 2.0632\n",
            "Epoch 58/1500, Loss: 2.0368\n",
            "Epoch 59/1500, Loss: 2.0113\n",
            "Epoch 60/1500, Loss: 1.9868\n",
            "Epoch 61/1500, Loss: 1.9630\n",
            "Epoch 62/1500, Loss: 1.9401\n",
            "Epoch 63/1500, Loss: 1.9180\n",
            "Epoch 64/1500, Loss: 1.8966\n",
            "Epoch 65/1500, Loss: 1.8759\n",
            "Epoch 66/1500, Loss: 1.8559\n",
            "Epoch 67/1500, Loss: 1.8366\n",
            "Epoch 68/1500, Loss: 1.8179\n",
            "Epoch 69/1500, Loss: 1.7998\n",
            "Epoch 70/1500, Loss: 1.7823\n",
            "Epoch 71/1500, Loss: 1.7653\n",
            "Epoch 72/1500, Loss: 1.7489\n",
            "Epoch 73/1500, Loss: 1.7329\n",
            "Epoch 74/1500, Loss: 1.7175\n",
            "Epoch 75/1500, Loss: 1.7026\n",
            "Epoch 76/1500, Loss: 1.6881\n",
            "Epoch 77/1500, Loss: 1.6740\n",
            "Epoch 78/1500, Loss: 1.6604\n",
            "Epoch 79/1500, Loss: 1.6472\n",
            "Epoch 80/1500, Loss: 1.6344\n",
            "Epoch 81/1500, Loss: 1.6219\n",
            "Epoch 82/1500, Loss: 1.6099\n",
            "Epoch 83/1500, Loss: 1.5981\n",
            "Epoch 84/1500, Loss: 1.5868\n",
            "Epoch 85/1500, Loss: 1.5757\n",
            "Epoch 86/1500, Loss: 1.5650\n",
            "Epoch 87/1500, Loss: 1.5546\n",
            "Epoch 88/1500, Loss: 1.5445\n",
            "Epoch 89/1500, Loss: 1.5347\n",
            "Epoch 90/1500, Loss: 1.5252\n",
            "Epoch 91/1500, Loss: 1.5159\n",
            "Epoch 92/1500, Loss: 1.5069\n",
            "Epoch 93/1500, Loss: 1.4982\n",
            "Epoch 94/1500, Loss: 1.4897\n",
            "Epoch 95/1500, Loss: 1.4815\n",
            "Epoch 96/1500, Loss: 1.4735\n",
            "Epoch 97/1500, Loss: 1.4657\n",
            "Epoch 98/1500, Loss: 1.4581\n",
            "Epoch 99/1500, Loss: 1.4508\n",
            "Epoch 100/1500, Loss: 1.4437\n",
            "Epoch 101/1500, Loss: 1.4368\n",
            "Epoch 102/1500, Loss: 1.4300\n",
            "Epoch 103/1500, Loss: 1.4235\n",
            "Epoch 104/1500, Loss: 1.4171\n",
            "Epoch 105/1500, Loss: 1.4110\n",
            "Epoch 106/1500, Loss: 1.4050\n",
            "Epoch 107/1500, Loss: 1.3992\n",
            "Epoch 108/1500, Loss: 1.3936\n",
            "Epoch 109/1500, Loss: 1.3881\n",
            "Epoch 110/1500, Loss: 1.3828\n",
            "Epoch 111/1500, Loss: 1.3777\n",
            "Epoch 112/1500, Loss: 1.3727\n",
            "Epoch 113/1500, Loss: 1.3678\n",
            "Epoch 114/1500, Loss: 1.3631\n",
            "Epoch 115/1500, Loss: 1.3586\n",
            "Epoch 116/1500, Loss: 1.3542\n",
            "Epoch 117/1500, Loss: 1.3499\n",
            "Epoch 118/1500, Loss: 1.3458\n",
            "Epoch 119/1500, Loss: 1.3418\n",
            "Epoch 120/1500, Loss: 1.3380\n",
            "Epoch 121/1500, Loss: 1.3342\n",
            "Epoch 122/1500, Loss: 1.3306\n",
            "Epoch 123/1500, Loss: 1.3272\n",
            "Epoch 124/1500, Loss: 1.3238\n",
            "Epoch 125/1500, Loss: 1.3206\n",
            "Epoch 126/1500, Loss: 1.3175\n",
            "Epoch 127/1500, Loss: 1.3145\n",
            "Epoch 128/1500, Loss: 1.3116\n",
            "Epoch 129/1500, Loss: 1.3088\n",
            "Epoch 130/1500, Loss: 1.3062\n",
            "Epoch 131/1500, Loss: 1.3036\n",
            "Epoch 132/1500, Loss: 1.3012\n",
            "Epoch 133/1500, Loss: 1.2989\n",
            "Epoch 134/1500, Loss: 1.2967\n",
            "Epoch 135/1500, Loss: 1.2946\n",
            "Epoch 136/1500, Loss: 1.2926\n",
            "Epoch 137/1500, Loss: 1.2907\n",
            "Epoch 138/1500, Loss: 1.2889\n",
            "Epoch 139/1500, Loss: 1.2872\n",
            "Epoch 140/1500, Loss: 1.2856\n",
            "Epoch 141/1500, Loss: 1.2841\n",
            "Epoch 142/1500, Loss: 1.2827\n",
            "Epoch 143/1500, Loss: 1.2814\n",
            "Epoch 144/1500, Loss: 1.2802\n",
            "Epoch 145/1500, Loss: 1.2791\n",
            "Epoch 146/1500, Loss: 1.2781\n",
            "Epoch 147/1500, Loss: 1.2771\n",
            "Epoch 148/1500, Loss: 1.2763\n",
            "Epoch 149/1500, Loss: 1.2756\n",
            "Epoch 150/1500, Loss: 1.2750\n",
            "Epoch 151/1500, Loss: 1.2745\n",
            "Epoch 152/1500, Loss: 1.2740\n",
            "Epoch 153/1500, Loss: 1.2737\n",
            "Epoch 154/1500, Loss: 1.2734\n",
            "Epoch 155/1500, Loss: 1.2733\n",
            "Epoch 156/1500, Loss: 1.2732\n",
            "Epoch 157/1500, Loss: 1.2733\n",
            "Epoch 158/1500, Loss: 1.2734\n",
            "Epoch 159/1500, Loss: 1.2736\n",
            "Epoch 160/1500, Loss: 1.2740\n",
            "Epoch 161/1500, Loss: 1.2744\n",
            "Epoch 162/1500, Loss: 1.2749\n",
            "Epoch 163/1500, Loss: 1.2755\n",
            "Epoch 164/1500, Loss: 1.2762\n",
            "Epoch 165/1500, Loss: 1.2771\n",
            "Epoch 166/1500, Loss: 1.2780\n",
            "Epoch 167/1500, Loss: 1.2790\n",
            "Epoch 168/1500, Loss: 1.2801\n",
            "Epoch 169/1500, Loss: 1.2813\n",
            "Epoch 170/1500, Loss: 1.2826\n",
            "Epoch 171/1500, Loss: 1.2840\n",
            "Epoch 172/1500, Loss: 1.2855\n",
            "Epoch 173/1500, Loss: 1.2872\n",
            "Epoch 174/1500, Loss: 1.2889\n",
            "Epoch 175/1500, Loss: 1.2907\n",
            "Epoch 176/1500, Loss: 1.2927\n",
            "Epoch 177/1500, Loss: 1.2947\n",
            "Epoch 178/1500, Loss: 1.2969\n",
            "Epoch 179/1500, Loss: 1.2992\n",
            "Epoch 180/1500, Loss: 1.3015\n",
            "Epoch 181/1500, Loss: 1.3040\n",
            "Epoch 182/1500, Loss: 1.3067\n",
            "Epoch 183/1500, Loss: 1.3094\n",
            "Epoch 184/1500, Loss: 1.3123\n",
            "Epoch 185/1500, Loss: 1.3153\n",
            "Epoch 186/1500, Loss: 1.3184\n",
            "Epoch 187/1500, Loss: 1.3216\n",
            "Epoch 188/1500, Loss: 1.3250\n",
            "Epoch 189/1500, Loss: 1.3285\n",
            "Epoch 190/1500, Loss: 1.3321\n",
            "Epoch 191/1500, Loss: 1.3359\n",
            "Epoch 192/1500, Loss: 1.3398\n",
            "Epoch 193/1500, Loss: 1.3438\n",
            "Epoch 194/1500, Loss: 1.3480\n",
            "Epoch 195/1500, Loss: 1.3523\n",
            "Epoch 196/1500, Loss: 1.3568\n",
            "Epoch 197/1500, Loss: 1.3615\n",
            "Epoch 198/1500, Loss: 1.3663\n",
            "Epoch 199/1500, Loss: 1.3712\n",
            "Epoch 200/1500, Loss: 1.3764\n",
            "Epoch 201/1500, Loss: 1.3817\n",
            "Epoch 202/1500, Loss: 1.3872\n",
            "Epoch 203/1500, Loss: 1.3928\n",
            "Epoch 204/1500, Loss: 1.3986\n",
            "Epoch 205/1500, Loss: 1.4047\n",
            "Epoch 206/1500, Loss: 1.4109\n",
            "Epoch 207/1500, Loss: 1.4173\n",
            "Epoch 208/1500, Loss: 1.4239\n",
            "Epoch 209/1500, Loss: 1.4307\n",
            "Epoch 210/1500, Loss: 1.4377\n",
            "Epoch 211/1500, Loss: 1.4450\n",
            "Epoch 212/1500, Loss: 1.4524\n",
            "Epoch 213/1500, Loss: 1.4601\n",
            "Epoch 214/1500, Loss: 1.4681\n",
            "Epoch 215/1500, Loss: 1.4762\n",
            "Epoch 216/1500, Loss: 1.4847\n",
            "Epoch 217/1500, Loss: 1.4934\n",
            "Epoch 218/1500, Loss: 1.5023\n",
            "Epoch 219/1500, Loss: 1.5116\n",
            "Epoch 220/1500, Loss: 1.5211\n",
            "Epoch 221/1500, Loss: 1.5309\n",
            "Epoch 222/1500, Loss: 1.5410\n",
            "Epoch 223/1500, Loss: 1.5515\n",
            "Epoch 224/1500, Loss: 1.5622\n",
            "Epoch 225/1500, Loss: 1.5733\n",
            "Epoch 226/1500, Loss: 1.5848\n",
            "Epoch 227/1500, Loss: 1.5966\n",
            "Epoch 228/1500, Loss: 1.6088\n",
            "Epoch 229/1500, Loss: 1.6213\n",
            "Epoch 230/1500, Loss: 1.6343\n",
            "Epoch 231/1500, Loss: 1.6476\n",
            "Epoch 232/1500, Loss: 1.6614\n",
            "Epoch 233/1500, Loss: 1.6757\n",
            "Epoch 234/1500, Loss: 1.6904\n",
            "Epoch 235/1500, Loss: 1.7056\n",
            "Epoch 236/1500, Loss: 1.7213\n",
            "Epoch 237/1500, Loss: 1.7375\n",
            "Epoch 238/1500, Loss: 1.7542\n",
            "Epoch 239/1500, Loss: 1.7715\n",
            "Epoch 240/1500, Loss: 1.7894\n",
            "Epoch 241/1500, Loss: 1.8079\n",
            "Epoch 242/1500, Loss: 1.8270\n",
            "Epoch 243/1500, Loss: 1.8468\n",
            "Epoch 244/1500, Loss: 1.8673\n",
            "Epoch 245/1500, Loss: 1.8885\n",
            "Epoch 246/1500, Loss: 1.9105\n",
            "Epoch 247/1500, Loss: 1.9332\n",
            "Epoch 248/1500, Loss: 1.9567\n",
            "Epoch 249/1500, Loss: 1.9811\n",
            "Epoch 250/1500, Loss: 2.0065\n",
            "Epoch 251/1500, Loss: 2.0327\n",
            "Epoch 252/1500, Loss: 2.0599\n",
            "Epoch 253/1500, Loss: 2.0882\n",
            "Epoch 254/1500, Loss: 2.1175\n",
            "Epoch 255/1500, Loss: 2.1480\n",
            "Epoch 256/1500, Loss: 2.1796\n",
            "Epoch 257/1500, Loss: 2.2125\n",
            "Epoch 258/1500, Loss: 2.2468\n",
            "Epoch 259/1500, Loss: 2.2824\n",
            "Epoch 260/1500, Loss: 2.3194\n",
            "Epoch 261/1500, Loss: 2.3580\n",
            "Epoch 262/1500, Loss: 2.3982\n",
            "Epoch 263/1500, Loss: 2.4400\n",
            "Epoch 264/1500, Loss: 2.4837\n",
            "Epoch 265/1500, Loss: 2.5292\n",
            "Epoch 266/1500, Loss: 2.5767\n",
            "Epoch 267/1500, Loss: 2.6263\n",
            "Epoch 268/1500, Loss: 2.6781\n",
            "Epoch 269/1500, Loss: 2.7323\n",
            "Epoch 270/1500, Loss: 2.7889\n",
            "Epoch 271/1500, Loss: 2.8481\n",
            "Epoch 272/1500, Loss: 2.9100\n",
            "Epoch 273/1500, Loss: 2.9749\n",
            "Epoch 274/1500, Loss: 3.0428\n",
            "Epoch 275/1500, Loss: 3.1140\n",
            "Epoch 276/1500, Loss: 3.1885\n",
            "Epoch 277/1500, Loss: 3.2668\n",
            "Epoch 278/1500, Loss: 3.3488\n",
            "Epoch 279/1500, Loss: 3.4348\n",
            "Epoch 280/1500, Loss: 3.5251\n",
            "Epoch 281/1500, Loss: 3.6199\n",
            "Epoch 282/1500, Loss: 3.7194\n",
            "Epoch 283/1500, Loss: 3.8238\n",
            "Epoch 284/1500, Loss: 3.9335\n",
            "Epoch 285/1500, Loss: 4.0485\n",
            "Epoch 286/1500, Loss: 4.1691\n",
            "Epoch 287/1500, Loss: 4.2956\n",
            "Epoch 288/1500, Loss: 4.4281\n",
            "Epoch 289/1500, Loss: 4.5667\n",
            "Epoch 290/1500, Loss: 4.7117\n",
            "Epoch 291/1500, Loss: 4.8629\n",
            "Epoch 292/1500, Loss: 5.0204\n",
            "Epoch 293/1500, Loss: 5.1840\n",
            "Epoch 294/1500, Loss: 5.3534\n",
            "Epoch 295/1500, Loss: 5.5281\n",
            "Epoch 296/1500, Loss: 5.7076\n",
            "Epoch 297/1500, Loss: 5.8907\n",
            "Epoch 298/1500, Loss: 6.0764\n",
            "Epoch 299/1500, Loss: 6.2629\n",
            "Epoch 300/1500, Loss: 6.4483\n",
            "Epoch 301/1500, Loss: 6.6301\n",
            "Epoch 302/1500, Loss: 6.8054\n",
            "Epoch 303/1500, Loss: 6.9706\n",
            "Epoch 304/1500, Loss: 7.1217\n",
            "Epoch 305/1500, Loss: 7.2543\n",
            "Epoch 306/1500, Loss: 7.3637\n",
            "Epoch 307/1500, Loss: 7.4449\n",
            "Epoch 308/1500, Loss: 7.4931\n",
            "Epoch 309/1500, Loss: 7.5040\n",
            "Epoch 310/1500, Loss: 7.4741\n",
            "Epoch 311/1500, Loss: 7.4013\n",
            "Epoch 312/1500, Loss: 7.2850\n",
            "Epoch 313/1500, Loss: 7.1266\n",
            "Epoch 314/1500, Loss: 6.9296\n",
            "Epoch 315/1500, Loss: 6.6996\n",
            "Epoch 316/1500, Loss: 6.4434\n",
            "Epoch 317/1500, Loss: 6.1691\n",
            "Epoch 318/1500, Loss: 5.8853\n",
            "Epoch 319/1500, Loss: 5.5998\n",
            "Epoch 320/1500, Loss: 5.3198\n",
            "Epoch 321/1500, Loss: 5.0511\n",
            "Epoch 322/1500, Loss: 4.7978\n",
            "Epoch 323/1500, Loss: 4.5625\n",
            "Epoch 324/1500, Loss: 4.3465\n",
            "Epoch 325/1500, Loss: 4.1498\n",
            "Epoch 326/1500, Loss: 3.9719\n",
            "Epoch 327/1500, Loss: 3.8114\n",
            "Epoch 328/1500, Loss: 3.6668\n",
            "Epoch 329/1500, Loss: 3.5367\n",
            "Epoch 330/1500, Loss: 3.4193\n",
            "Epoch 331/1500, Loss: 3.3132\n",
            "Epoch 332/1500, Loss: 3.2170\n",
            "Epoch 333/1500, Loss: 3.1296\n",
            "Epoch 334/1500, Loss: 3.0497\n",
            "Epoch 335/1500, Loss: 2.9765\n",
            "Epoch 336/1500, Loss: 2.9091\n",
            "Epoch 337/1500, Loss: 2.8469\n",
            "Epoch 338/1500, Loss: 2.7892\n",
            "Epoch 339/1500, Loss: 2.7356\n",
            "Epoch 340/1500, Loss: 2.6854\n",
            "Epoch 341/1500, Loss: 2.6384\n",
            "Epoch 342/1500, Loss: 2.5942\n",
            "Epoch 343/1500, Loss: 2.5524\n",
            "Epoch 344/1500, Loss: 2.5128\n",
            "Epoch 345/1500, Loss: 2.4752\n",
            "Epoch 346/1500, Loss: 2.4394\n",
            "Epoch 347/1500, Loss: 2.4051\n",
            "Epoch 348/1500, Loss: 2.3723\n",
            "Epoch 349/1500, Loss: 2.3407\n",
            "Epoch 350/1500, Loss: 2.3103\n",
            "Epoch 351/1500, Loss: 2.2809\n",
            "Epoch 352/1500, Loss: 2.2525\n",
            "Epoch 353/1500, Loss: 2.2250\n",
            "Epoch 354/1500, Loss: 2.1983\n",
            "Epoch 355/1500, Loss: 2.1723\n",
            "Epoch 356/1500, Loss: 2.1470\n",
            "Epoch 357/1500, Loss: 2.1223\n",
            "Epoch 358/1500, Loss: 2.0982\n",
            "Epoch 359/1500, Loss: 2.0746\n",
            "Epoch 360/1500, Loss: 2.0515\n",
            "Epoch 361/1500, Loss: 2.0289\n",
            "Epoch 362/1500, Loss: 2.0068\n",
            "Epoch 363/1500, Loss: 1.9850\n",
            "Epoch 364/1500, Loss: 1.9637\n",
            "Epoch 365/1500, Loss: 1.9427\n",
            "Epoch 366/1500, Loss: 1.9221\n",
            "Epoch 367/1500, Loss: 1.9019\n",
            "Epoch 368/1500, Loss: 1.8820\n",
            "Epoch 369/1500, Loss: 1.8624\n",
            "Epoch 370/1500, Loss: 1.8431\n",
            "Epoch 371/1500, Loss: 1.8241\n",
            "Epoch 372/1500, Loss: 1.8054\n",
            "Epoch 373/1500, Loss: 1.7869\n",
            "Epoch 374/1500, Loss: 1.7688\n",
            "Epoch 375/1500, Loss: 1.7509\n",
            "Epoch 376/1500, Loss: 1.7332\n",
            "Epoch 377/1500, Loss: 1.7158\n",
            "Epoch 378/1500, Loss: 1.6987\n",
            "Epoch 379/1500, Loss: 1.6817\n",
            "Epoch 380/1500, Loss: 1.6650\n",
            "Epoch 381/1500, Loss: 1.6486\n",
            "Epoch 382/1500, Loss: 1.6323\n",
            "Epoch 383/1500, Loss: 1.6163\n",
            "Epoch 384/1500, Loss: 1.6005\n",
            "Epoch 385/1500, Loss: 1.5849\n",
            "Epoch 386/1500, Loss: 1.5695\n",
            "Epoch 387/1500, Loss: 1.5543\n",
            "Epoch 388/1500, Loss: 1.5393\n",
            "Epoch 389/1500, Loss: 1.5245\n",
            "Epoch 390/1500, Loss: 1.5099\n",
            "Epoch 391/1500, Loss: 1.4955\n",
            "Epoch 392/1500, Loss: 1.4813\n",
            "Epoch 393/1500, Loss: 1.4672\n",
            "Epoch 394/1500, Loss: 1.4534\n",
            "Epoch 395/1500, Loss: 1.4397\n",
            "Epoch 396/1500, Loss: 1.4262\n",
            "Epoch 397/1500, Loss: 1.4128\n",
            "Epoch 398/1500, Loss: 1.3997\n",
            "Epoch 399/1500, Loss: 1.3867\n",
            "Epoch 400/1500, Loss: 1.3739\n",
            "Epoch 401/1500, Loss: 1.3612\n",
            "Epoch 402/1500, Loss: 1.3487\n",
            "Epoch 403/1500, Loss: 1.3364\n",
            "Epoch 404/1500, Loss: 1.3242\n",
            "Epoch 405/1500, Loss: 1.3121\n",
            "Epoch 406/1500, Loss: 1.3003\n",
            "Epoch 407/1500, Loss: 1.2885\n",
            "Epoch 408/1500, Loss: 1.2770\n",
            "Epoch 409/1500, Loss: 1.2655\n",
            "Epoch 410/1500, Loss: 1.2542\n",
            "Epoch 411/1500, Loss: 1.2431\n",
            "Epoch 412/1500, Loss: 1.2321\n",
            "Epoch 413/1500, Loss: 1.2212\n",
            "Epoch 414/1500, Loss: 1.2105\n",
            "Epoch 415/1500, Loss: 1.1999\n",
            "Epoch 416/1500, Loss: 1.1895\n",
            "Epoch 417/1500, Loss: 1.1791\n",
            "Epoch 418/1500, Loss: 1.1689\n",
            "Epoch 419/1500, Loss: 1.1589\n",
            "Epoch 420/1500, Loss: 1.1489\n",
            "Epoch 421/1500, Loss: 1.1391\n",
            "Epoch 422/1500, Loss: 1.1294\n",
            "Epoch 423/1500, Loss: 1.1199\n",
            "Epoch 424/1500, Loss: 1.1104\n",
            "Epoch 425/1500, Loss: 1.1011\n",
            "Epoch 426/1500, Loss: 1.0919\n",
            "Epoch 427/1500, Loss: 1.0828\n",
            "Epoch 428/1500, Loss: 1.0738\n",
            "Epoch 429/1500, Loss: 1.0649\n",
            "Epoch 430/1500, Loss: 1.0562\n",
            "Epoch 431/1500, Loss: 1.0475\n",
            "Epoch 432/1500, Loss: 1.0390\n",
            "Epoch 433/1500, Loss: 1.0305\n",
            "Epoch 434/1500, Loss: 1.0222\n",
            "Epoch 435/1500, Loss: 1.0140\n",
            "Epoch 436/1500, Loss: 1.0058\n",
            "Epoch 437/1500, Loss: 0.9978\n",
            "Epoch 438/1500, Loss: 0.9899\n",
            "Epoch 439/1500, Loss: 0.9821\n",
            "Epoch 440/1500, Loss: 0.9743\n",
            "Epoch 441/1500, Loss: 0.9667\n",
            "Epoch 442/1500, Loss: 0.9592\n",
            "Epoch 443/1500, Loss: 0.9517\n",
            "Epoch 444/1500, Loss: 0.9444\n",
            "Epoch 445/1500, Loss: 0.9371\n",
            "Epoch 446/1500, Loss: 0.9299\n",
            "Epoch 447/1500, Loss: 0.9229\n",
            "Epoch 448/1500, Loss: 0.9159\n",
            "Epoch 449/1500, Loss: 0.9090\n",
            "Epoch 450/1500, Loss: 0.9021\n",
            "Epoch 451/1500, Loss: 0.8954\n",
            "Epoch 452/1500, Loss: 0.8887\n",
            "Epoch 453/1500, Loss: 0.8822\n",
            "Epoch 454/1500, Loss: 0.8757\n",
            "Epoch 455/1500, Loss: 0.8693\n",
            "Epoch 456/1500, Loss: 0.8629\n",
            "Epoch 457/1500, Loss: 0.8567\n",
            "Epoch 458/1500, Loss: 0.8505\n",
            "Epoch 459/1500, Loss: 0.8444\n",
            "Epoch 460/1500, Loss: 0.8383\n",
            "Epoch 461/1500, Loss: 0.8324\n",
            "Epoch 462/1500, Loss: 0.8265\n",
            "Epoch 463/1500, Loss: 0.8207\n",
            "Epoch 464/1500, Loss: 0.8150\n",
            "Epoch 465/1500, Loss: 0.8093\n",
            "Epoch 466/1500, Loss: 0.8037\n",
            "Epoch 467/1500, Loss: 0.7982\n",
            "Epoch 468/1500, Loss: 0.7927\n",
            "Epoch 469/1500, Loss: 0.7873\n",
            "Epoch 470/1500, Loss: 0.7820\n",
            "Epoch 471/1500, Loss: 0.7767\n",
            "Epoch 472/1500, Loss: 0.7715\n",
            "Epoch 473/1500, Loss: 0.7663\n",
            "Epoch 474/1500, Loss: 0.7613\n",
            "Epoch 475/1500, Loss: 0.7562\n",
            "Epoch 476/1500, Loss: 0.7513\n",
            "Epoch 477/1500, Loss: 0.7464\n",
            "Epoch 478/1500, Loss: 0.7415\n",
            "Epoch 479/1500, Loss: 0.7368\n",
            "Epoch 480/1500, Loss: 0.7320\n",
            "Epoch 481/1500, Loss: 0.7274\n",
            "Epoch 482/1500, Loss: 0.7228\n",
            "Epoch 483/1500, Loss: 0.7182\n",
            "Epoch 484/1500, Loss: 0.7137\n",
            "Epoch 485/1500, Loss: 0.7093\n",
            "Epoch 486/1500, Loss: 0.7049\n",
            "Epoch 487/1500, Loss: 0.7005\n",
            "Epoch 488/1500, Loss: 0.6962\n",
            "Epoch 489/1500, Loss: 0.6920\n",
            "Epoch 490/1500, Loss: 0.6878\n",
            "Epoch 491/1500, Loss: 0.6837\n",
            "Epoch 492/1500, Loss: 0.6796\n",
            "Epoch 493/1500, Loss: 0.6755\n",
            "Epoch 494/1500, Loss: 0.6715\n",
            "Epoch 495/1500, Loss: 0.6676\n",
            "Epoch 496/1500, Loss: 0.6637\n",
            "Epoch 497/1500, Loss: 0.6598\n",
            "Epoch 498/1500, Loss: 0.6560\n",
            "Epoch 499/1500, Loss: 0.6523\n",
            "Epoch 500/1500, Loss: 0.6486\n",
            "Epoch 501/1500, Loss: 0.6449\n",
            "Epoch 502/1500, Loss: 0.6412\n",
            "Epoch 503/1500, Loss: 0.6377\n",
            "Epoch 504/1500, Loss: 0.6341\n",
            "Epoch 505/1500, Loss: 0.6306\n",
            "Epoch 506/1500, Loss: 0.6271\n",
            "Epoch 507/1500, Loss: 0.6237\n",
            "Epoch 508/1500, Loss: 0.6203\n",
            "Epoch 509/1500, Loss: 0.6170\n",
            "Epoch 510/1500, Loss: 0.6137\n",
            "Epoch 511/1500, Loss: 0.6104\n",
            "Epoch 512/1500, Loss: 0.6072\n",
            "Epoch 513/1500, Loss: 0.6040\n",
            "Epoch 514/1500, Loss: 0.6008\n",
            "Epoch 515/1500, Loss: 0.5977\n",
            "Epoch 516/1500, Loss: 0.5946\n",
            "Epoch 517/1500, Loss: 0.5916\n",
            "Epoch 518/1500, Loss: 0.5886\n",
            "Epoch 519/1500, Loss: 0.5856\n",
            "Epoch 520/1500, Loss: 0.5827\n",
            "Epoch 521/1500, Loss: 0.5797\n",
            "Epoch 522/1500, Loss: 0.5769\n",
            "Epoch 523/1500, Loss: 0.5740\n",
            "Epoch 524/1500, Loss: 0.5712\n",
            "Epoch 525/1500, Loss: 0.5684\n",
            "Epoch 526/1500, Loss: 0.5657\n",
            "Epoch 527/1500, Loss: 0.5630\n",
            "Epoch 528/1500, Loss: 0.5603\n",
            "Epoch 529/1500, Loss: 0.5576\n",
            "Epoch 530/1500, Loss: 0.5550\n",
            "Epoch 531/1500, Loss: 0.5524\n",
            "Epoch 532/1500, Loss: 0.5498\n",
            "Epoch 533/1500, Loss: 0.5473\n",
            "Epoch 534/1500, Loss: 0.5448\n",
            "Epoch 535/1500, Loss: 0.5423\n",
            "Epoch 536/1500, Loss: 0.5399\n",
            "Epoch 537/1500, Loss: 0.5374\n",
            "Epoch 538/1500, Loss: 0.5350\n",
            "Epoch 539/1500, Loss: 0.5327\n",
            "Epoch 540/1500, Loss: 0.5303\n",
            "Epoch 541/1500, Loss: 0.5280\n",
            "Epoch 542/1500, Loss: 0.5257\n",
            "Epoch 543/1500, Loss: 0.5234\n",
            "Epoch 544/1500, Loss: 0.5212\n",
            "Epoch 545/1500, Loss: 0.5190\n",
            "Epoch 546/1500, Loss: 0.5168\n",
            "Epoch 547/1500, Loss: 0.5146\n",
            "Epoch 548/1500, Loss: 0.5125\n",
            "Epoch 549/1500, Loss: 0.5103\n",
            "Epoch 550/1500, Loss: 0.5082\n",
            "Epoch 551/1500, Loss: 0.5062\n",
            "Epoch 552/1500, Loss: 0.5041\n",
            "Epoch 553/1500, Loss: 0.5021\n",
            "Epoch 554/1500, Loss: 0.5001\n",
            "Epoch 555/1500, Loss: 0.4981\n",
            "Epoch 556/1500, Loss: 0.4961\n",
            "Epoch 557/1500, Loss: 0.4942\n",
            "Epoch 558/1500, Loss: 0.4922\n",
            "Epoch 559/1500, Loss: 0.4903\n",
            "Epoch 560/1500, Loss: 0.4884\n",
            "Epoch 561/1500, Loss: 0.4866\n",
            "Epoch 562/1500, Loss: 0.4847\n",
            "Epoch 563/1500, Loss: 0.4829\n",
            "Epoch 564/1500, Loss: 0.4811\n",
            "Epoch 565/1500, Loss: 0.4793\n",
            "Epoch 566/1500, Loss: 0.4775\n",
            "Epoch 567/1500, Loss: 0.4758\n",
            "Epoch 568/1500, Loss: 0.4741\n",
            "Epoch 569/1500, Loss: 0.4724\n",
            "Epoch 570/1500, Loss: 0.4707\n",
            "Epoch 571/1500, Loss: 0.4690\n",
            "Epoch 572/1500, Loss: 0.4673\n",
            "Epoch 573/1500, Loss: 0.4657\n",
            "Epoch 574/1500, Loss: 0.4641\n",
            "Epoch 575/1500, Loss: 0.4625\n",
            "Epoch 576/1500, Loss: 0.4609\n",
            "Epoch 577/1500, Loss: 0.4593\n",
            "Epoch 578/1500, Loss: 0.4577\n",
            "Epoch 579/1500, Loss: 0.4562\n",
            "Epoch 580/1500, Loss: 0.4547\n",
            "Epoch 581/1500, Loss: 0.4531\n",
            "Epoch 582/1500, Loss: 0.4517\n",
            "Epoch 583/1500, Loss: 0.4502\n",
            "Epoch 584/1500, Loss: 0.4487\n",
            "Epoch 585/1500, Loss: 0.4473\n",
            "Epoch 586/1500, Loss: 0.4458\n",
            "Epoch 587/1500, Loss: 0.4444\n",
            "Epoch 588/1500, Loss: 0.4430\n",
            "Epoch 589/1500, Loss: 0.4416\n",
            "Epoch 590/1500, Loss: 0.4402\n",
            "Epoch 591/1500, Loss: 0.4389\n",
            "Epoch 592/1500, Loss: 0.4375\n",
            "Epoch 593/1500, Loss: 0.4362\n",
            "Epoch 594/1500, Loss: 0.4348\n",
            "Epoch 595/1500, Loss: 0.4335\n",
            "Epoch 596/1500, Loss: 0.4322\n",
            "Epoch 597/1500, Loss: 0.4309\n",
            "Epoch 598/1500, Loss: 0.4297\n",
            "Epoch 599/1500, Loss: 0.4284\n",
            "Epoch 600/1500, Loss: 0.4271\n",
            "Epoch 601/1500, Loss: 0.4259\n",
            "Epoch 602/1500, Loss: 0.4247\n",
            "Epoch 603/1500, Loss: 0.4235\n",
            "Epoch 604/1500, Loss: 0.4223\n",
            "Epoch 605/1500, Loss: 0.4211\n",
            "Epoch 606/1500, Loss: 0.4199\n",
            "Epoch 607/1500, Loss: 0.4187\n",
            "Epoch 608/1500, Loss: 0.4176\n",
            "Epoch 609/1500, Loss: 0.4164\n",
            "Epoch 610/1500, Loss: 0.4153\n",
            "Epoch 611/1500, Loss: 0.4142\n",
            "Epoch 612/1500, Loss: 0.4131\n",
            "Epoch 613/1500, Loss: 0.4119\n",
            "Epoch 614/1500, Loss: 0.4109\n",
            "Epoch 615/1500, Loss: 0.4098\n",
            "Epoch 616/1500, Loss: 0.4087\n",
            "Epoch 617/1500, Loss: 0.4076\n",
            "Epoch 618/1500, Loss: 0.4066\n",
            "Epoch 619/1500, Loss: 0.4055\n",
            "Epoch 620/1500, Loss: 0.4045\n",
            "Epoch 621/1500, Loss: 0.4035\n",
            "Epoch 622/1500, Loss: 0.4025\n",
            "Epoch 623/1500, Loss: 0.4015\n",
            "Epoch 624/1500, Loss: 0.4005\n",
            "Epoch 625/1500, Loss: 0.3995\n",
            "Epoch 626/1500, Loss: 0.3985\n",
            "Epoch 627/1500, Loss: 0.3975\n",
            "Epoch 628/1500, Loss: 0.3966\n",
            "Epoch 629/1500, Loss: 0.3956\n",
            "Epoch 630/1500, Loss: 0.3947\n",
            "Epoch 631/1500, Loss: 0.3937\n",
            "Epoch 632/1500, Loss: 0.3928\n",
            "Epoch 633/1500, Loss: 0.3919\n",
            "Epoch 634/1500, Loss: 0.3910\n",
            "Epoch 635/1500, Loss: 0.3901\n",
            "Epoch 636/1500, Loss: 0.3892\n",
            "Epoch 637/1500, Loss: 0.3883\n",
            "Epoch 638/1500, Loss: 0.3874\n",
            "Epoch 639/1500, Loss: 0.3865\n",
            "Epoch 640/1500, Loss: 0.3857\n",
            "Epoch 641/1500, Loss: 0.3848\n",
            "Epoch 642/1500, Loss: 0.3839\n",
            "Epoch 643/1500, Loss: 0.3831\n",
            "Epoch 644/1500, Loss: 0.3823\n",
            "Epoch 645/1500, Loss: 0.3814\n",
            "Epoch 646/1500, Loss: 0.3806\n",
            "Epoch 647/1500, Loss: 0.3798\n",
            "Epoch 648/1500, Loss: 0.3790\n",
            "Epoch 649/1500, Loss: 0.3782\n",
            "Epoch 650/1500, Loss: 0.3774\n",
            "Epoch 651/1500, Loss: 0.3766\n",
            "Epoch 652/1500, Loss: 0.3758\n",
            "Epoch 653/1500, Loss: 0.3750\n",
            "Epoch 654/1500, Loss: 0.3743\n",
            "Epoch 655/1500, Loss: 0.3735\n",
            "Epoch 656/1500, Loss: 0.3727\n",
            "Epoch 657/1500, Loss: 0.3720\n",
            "Epoch 658/1500, Loss: 0.3712\n",
            "Epoch 659/1500, Loss: 0.3705\n",
            "Epoch 660/1500, Loss: 0.3698\n",
            "Epoch 661/1500, Loss: 0.3690\n",
            "Epoch 662/1500, Loss: 0.3683\n",
            "Epoch 663/1500, Loss: 0.3676\n",
            "Epoch 664/1500, Loss: 0.3669\n",
            "Epoch 665/1500, Loss: 0.3662\n",
            "Epoch 666/1500, Loss: 0.3655\n",
            "Epoch 667/1500, Loss: 0.3648\n",
            "Epoch 668/1500, Loss: 0.3641\n",
            "Epoch 669/1500, Loss: 0.3634\n",
            "Epoch 670/1500, Loss: 0.3627\n",
            "Epoch 671/1500, Loss: 0.3621\n",
            "Epoch 672/1500, Loss: 0.3614\n",
            "Epoch 673/1500, Loss: 0.3607\n",
            "Epoch 674/1500, Loss: 0.3601\n",
            "Epoch 675/1500, Loss: 0.3594\n",
            "Epoch 676/1500, Loss: 0.3588\n",
            "Epoch 677/1500, Loss: 0.3581\n",
            "Epoch 678/1500, Loss: 0.3575\n",
            "Epoch 679/1500, Loss: 0.3568\n",
            "Epoch 680/1500, Loss: 0.3562\n",
            "Epoch 681/1500, Loss: 0.3556\n",
            "Epoch 682/1500, Loss: 0.3550\n",
            "Epoch 683/1500, Loss: 0.3544\n",
            "Epoch 684/1500, Loss: 0.3537\n",
            "Epoch 685/1500, Loss: 0.3531\n",
            "Epoch 686/1500, Loss: 0.3525\n",
            "Epoch 687/1500, Loss: 0.3519\n",
            "Epoch 688/1500, Loss: 0.3513\n",
            "Epoch 689/1500, Loss: 0.3507\n",
            "Epoch 690/1500, Loss: 0.3502\n",
            "Epoch 691/1500, Loss: 0.3496\n",
            "Epoch 692/1500, Loss: 0.3490\n",
            "Epoch 693/1500, Loss: 0.3484\n",
            "Epoch 694/1500, Loss: 0.3479\n",
            "Epoch 695/1500, Loss: 0.3473\n",
            "Epoch 696/1500, Loss: 0.3467\n",
            "Epoch 697/1500, Loss: 0.3462\n",
            "Epoch 698/1500, Loss: 0.3456\n",
            "Epoch 699/1500, Loss: 0.3451\n",
            "Epoch 700/1500, Loss: 0.3445\n",
            "Epoch 701/1500, Loss: 0.3440\n",
            "Epoch 702/1500, Loss: 0.3434\n",
            "Epoch 703/1500, Loss: 0.3429\n",
            "Epoch 704/1500, Loss: 0.3424\n",
            "Epoch 705/1500, Loss: 0.3418\n",
            "Epoch 706/1500, Loss: 0.3413\n",
            "Epoch 707/1500, Loss: 0.3408\n",
            "Epoch 708/1500, Loss: 0.3403\n",
            "Epoch 709/1500, Loss: 0.3397\n",
            "Epoch 710/1500, Loss: 0.3392\n",
            "Epoch 711/1500, Loss: 0.3387\n",
            "Epoch 712/1500, Loss: 0.3382\n",
            "Epoch 713/1500, Loss: 0.3377\n",
            "Epoch 714/1500, Loss: 0.3372\n",
            "Epoch 715/1500, Loss: 0.3367\n",
            "Epoch 716/1500, Loss: 0.3362\n",
            "Epoch 717/1500, Loss: 0.3357\n",
            "Epoch 718/1500, Loss: 0.3352\n",
            "Epoch 719/1500, Loss: 0.3348\n",
            "Epoch 720/1500, Loss: 0.3343\n",
            "Epoch 721/1500, Loss: 0.3338\n",
            "Epoch 722/1500, Loss: 0.3333\n",
            "Epoch 723/1500, Loss: 0.3328\n",
            "Epoch 724/1500, Loss: 0.3324\n",
            "Epoch 725/1500, Loss: 0.3319\n",
            "Epoch 726/1500, Loss: 0.3314\n",
            "Epoch 727/1500, Loss: 0.3310\n",
            "Epoch 728/1500, Loss: 0.3305\n",
            "Epoch 729/1500, Loss: 0.3301\n",
            "Epoch 730/1500, Loss: 0.3296\n",
            "Epoch 731/1500, Loss: 0.3292\n",
            "Epoch 732/1500, Loss: 0.3287\n",
            "Epoch 733/1500, Loss: 0.3283\n",
            "Epoch 734/1500, Loss: 0.3278\n",
            "Epoch 735/1500, Loss: 0.3274\n",
            "Epoch 736/1500, Loss: 0.3269\n",
            "Epoch 737/1500, Loss: 0.3265\n",
            "Epoch 738/1500, Loss: 0.3261\n",
            "Epoch 739/1500, Loss: 0.3256\n",
            "Epoch 740/1500, Loss: 0.3252\n",
            "Epoch 741/1500, Loss: 0.3248\n",
            "Epoch 742/1500, Loss: 0.3244\n",
            "Epoch 743/1500, Loss: 0.3239\n",
            "Epoch 744/1500, Loss: 0.3235\n",
            "Epoch 745/1500, Loss: 0.3231\n",
            "Epoch 746/1500, Loss: 0.3227\n",
            "Epoch 747/1500, Loss: 0.3223\n",
            "Epoch 748/1500, Loss: 0.3219\n",
            "Epoch 749/1500, Loss: 0.3215\n",
            "Epoch 750/1500, Loss: 0.3211\n",
            "Epoch 751/1500, Loss: 0.3207\n",
            "Epoch 752/1500, Loss: 0.3203\n",
            "Epoch 753/1500, Loss: 0.3199\n",
            "Epoch 754/1500, Loss: 0.3195\n",
            "Epoch 755/1500, Loss: 0.3191\n",
            "Epoch 756/1500, Loss: 0.3187\n",
            "Epoch 757/1500, Loss: 0.3183\n",
            "Epoch 758/1500, Loss: 0.3179\n",
            "Epoch 759/1500, Loss: 0.3175\n",
            "Epoch 760/1500, Loss: 0.3171\n",
            "Epoch 761/1500, Loss: 0.3167\n",
            "Epoch 762/1500, Loss: 0.3164\n",
            "Epoch 763/1500, Loss: 0.3160\n",
            "Epoch 764/1500, Loss: 0.3156\n",
            "Epoch 765/1500, Loss: 0.3152\n",
            "Epoch 766/1500, Loss: 0.3148\n",
            "Epoch 767/1500, Loss: 0.3145\n",
            "Epoch 768/1500, Loss: 0.3141\n",
            "Epoch 769/1500, Loss: 0.3137\n",
            "Epoch 770/1500, Loss: 0.3134\n",
            "Epoch 771/1500, Loss: 0.3130\n",
            "Epoch 772/1500, Loss: 0.3126\n",
            "Epoch 773/1500, Loss: 0.3123\n",
            "Epoch 774/1500, Loss: 0.3119\n",
            "Epoch 775/1500, Loss: 0.3116\n",
            "Epoch 776/1500, Loss: 0.3112\n",
            "Epoch 777/1500, Loss: 0.3109\n",
            "Epoch 778/1500, Loss: 0.3105\n",
            "Epoch 779/1500, Loss: 0.3101\n",
            "Epoch 780/1500, Loss: 0.3098\n",
            "Epoch 781/1500, Loss: 0.3095\n",
            "Epoch 782/1500, Loss: 0.3091\n",
            "Epoch 783/1500, Loss: 0.3088\n",
            "Epoch 784/1500, Loss: 0.3084\n",
            "Epoch 785/1500, Loss: 0.3081\n",
            "Epoch 786/1500, Loss: 0.3077\n",
            "Epoch 787/1500, Loss: 0.3074\n",
            "Epoch 788/1500, Loss: 0.3071\n",
            "Epoch 789/1500, Loss: 0.3067\n",
            "Epoch 790/1500, Loss: 0.3064\n",
            "Epoch 791/1500, Loss: 0.3061\n",
            "Epoch 792/1500, Loss: 0.3057\n",
            "Epoch 793/1500, Loss: 0.3054\n",
            "Epoch 794/1500, Loss: 0.3051\n",
            "Epoch 795/1500, Loss: 0.3047\n",
            "Epoch 796/1500, Loss: 0.3044\n",
            "Epoch 797/1500, Loss: 0.3041\n",
            "Epoch 798/1500, Loss: 0.3038\n",
            "Epoch 799/1500, Loss: 0.3034\n",
            "Epoch 800/1500, Loss: 0.3031\n",
            "Epoch 801/1500, Loss: 0.3028\n",
            "Epoch 802/1500, Loss: 0.3025\n",
            "Epoch 803/1500, Loss: 0.3022\n",
            "Epoch 804/1500, Loss: 0.3019\n",
            "Epoch 805/1500, Loss: 0.3015\n",
            "Epoch 806/1500, Loss: 0.3012\n",
            "Epoch 807/1500, Loss: 0.3009\n",
            "Epoch 808/1500, Loss: 0.3006\n",
            "Epoch 809/1500, Loss: 0.3003\n",
            "Epoch 810/1500, Loss: 0.3000\n",
            "Epoch 811/1500, Loss: 0.2997\n",
            "Epoch 812/1500, Loss: 0.2994\n",
            "Epoch 813/1500, Loss: 0.2991\n",
            "Epoch 814/1500, Loss: 0.2988\n",
            "Epoch 815/1500, Loss: 0.2985\n",
            "Epoch 816/1500, Loss: 0.2982\n",
            "Epoch 817/1500, Loss: 0.2979\n",
            "Epoch 818/1500, Loss: 0.2976\n",
            "Epoch 819/1500, Loss: 0.2973\n",
            "Epoch 820/1500, Loss: 0.2970\n",
            "Epoch 821/1500, Loss: 0.2967\n",
            "Epoch 822/1500, Loss: 0.2964\n",
            "Epoch 823/1500, Loss: 0.2961\n",
            "Epoch 824/1500, Loss: 0.2958\n",
            "Epoch 825/1500, Loss: 0.2955\n",
            "Epoch 826/1500, Loss: 0.2952\n",
            "Epoch 827/1500, Loss: 0.2950\n",
            "Epoch 828/1500, Loss: 0.2947\n",
            "Epoch 829/1500, Loss: 0.2944\n",
            "Epoch 830/1500, Loss: 0.2941\n",
            "Epoch 831/1500, Loss: 0.2938\n",
            "Epoch 832/1500, Loss: 0.2935\n",
            "Epoch 833/1500, Loss: 0.2933\n",
            "Epoch 834/1500, Loss: 0.2930\n",
            "Epoch 835/1500, Loss: 0.2927\n",
            "Epoch 836/1500, Loss: 0.2924\n",
            "Epoch 837/1500, Loss: 0.2921\n",
            "Epoch 838/1500, Loss: 0.2919\n",
            "Epoch 839/1500, Loss: 0.2916\n",
            "Epoch 840/1500, Loss: 0.2913\n",
            "Epoch 841/1500, Loss: 0.2910\n",
            "Epoch 842/1500, Loss: 0.2908\n",
            "Epoch 843/1500, Loss: 0.2905\n",
            "Epoch 844/1500, Loss: 0.2902\n",
            "Epoch 845/1500, Loss: 0.2900\n",
            "Epoch 846/1500, Loss: 0.2897\n",
            "Epoch 847/1500, Loss: 0.2894\n",
            "Epoch 848/1500, Loss: 0.2892\n",
            "Epoch 849/1500, Loss: 0.2889\n",
            "Epoch 850/1500, Loss: 0.2886\n",
            "Epoch 851/1500, Loss: 0.2884\n",
            "Epoch 852/1500, Loss: 0.2881\n",
            "Epoch 853/1500, Loss: 0.2878\n",
            "Epoch 854/1500, Loss: 0.2876\n",
            "Epoch 855/1500, Loss: 0.2873\n",
            "Epoch 856/1500, Loss: 0.2871\n",
            "Epoch 857/1500, Loss: 0.2868\n",
            "Epoch 858/1500, Loss: 0.2865\n",
            "Epoch 859/1500, Loss: 0.2863\n",
            "Epoch 860/1500, Loss: 0.2860\n",
            "Epoch 861/1500, Loss: 0.2858\n",
            "Epoch 862/1500, Loss: 0.2855\n",
            "Epoch 863/1500, Loss: 0.2853\n",
            "Epoch 864/1500, Loss: 0.2850\n",
            "Epoch 865/1500, Loss: 0.2848\n",
            "Epoch 866/1500, Loss: 0.2845\n",
            "Epoch 867/1500, Loss: 0.2843\n",
            "Epoch 868/1500, Loss: 0.2840\n",
            "Epoch 869/1500, Loss: 0.2838\n",
            "Epoch 870/1500, Loss: 0.2835\n",
            "Epoch 871/1500, Loss: 0.2833\n",
            "Epoch 872/1500, Loss: 0.2830\n",
            "Epoch 873/1500, Loss: 0.2828\n",
            "Epoch 874/1500, Loss: 0.2825\n",
            "Epoch 875/1500, Loss: 0.2823\n",
            "Epoch 876/1500, Loss: 0.2821\n",
            "Epoch 877/1500, Loss: 0.2818\n",
            "Epoch 878/1500, Loss: 0.2816\n",
            "Epoch 879/1500, Loss: 0.2813\n",
            "Epoch 880/1500, Loss: 0.2811\n",
            "Epoch 881/1500, Loss: 0.2809\n",
            "Epoch 882/1500, Loss: 0.2806\n",
            "Epoch 883/1500, Loss: 0.2804\n",
            "Epoch 884/1500, Loss: 0.2801\n",
            "Epoch 885/1500, Loss: 0.2799\n",
            "Epoch 886/1500, Loss: 0.2797\n",
            "Epoch 887/1500, Loss: 0.2794\n",
            "Epoch 888/1500, Loss: 0.2792\n",
            "Epoch 889/1500, Loss: 0.2790\n",
            "Epoch 890/1500, Loss: 0.2787\n",
            "Epoch 891/1500, Loss: 0.2785\n",
            "Epoch 892/1500, Loss: 0.2783\n",
            "Epoch 893/1500, Loss: 0.2780\n",
            "Epoch 894/1500, Loss: 0.2778\n",
            "Epoch 895/1500, Loss: 0.2776\n",
            "Epoch 896/1500, Loss: 0.2773\n",
            "Epoch 897/1500, Loss: 0.2771\n",
            "Epoch 898/1500, Loss: 0.2769\n",
            "Epoch 899/1500, Loss: 0.2767\n",
            "Epoch 900/1500, Loss: 0.2764\n",
            "Epoch 901/1500, Loss: 0.2762\n",
            "Epoch 902/1500, Loss: 0.2760\n",
            "Epoch 903/1500, Loss: 0.2758\n",
            "Epoch 904/1500, Loss: 0.2755\n",
            "Epoch 905/1500, Loss: 0.2753\n",
            "Epoch 906/1500, Loss: 0.2751\n",
            "Epoch 907/1500, Loss: 0.2749\n",
            "Epoch 908/1500, Loss: 0.2747\n",
            "Epoch 909/1500, Loss: 0.2744\n",
            "Epoch 910/1500, Loss: 0.2742\n",
            "Epoch 911/1500, Loss: 0.2740\n",
            "Epoch 912/1500, Loss: 0.2738\n",
            "Epoch 913/1500, Loss: 0.2736\n",
            "Epoch 914/1500, Loss: 0.2733\n",
            "Epoch 915/1500, Loss: 0.2731\n",
            "Epoch 916/1500, Loss: 0.2729\n",
            "Epoch 917/1500, Loss: 0.2727\n",
            "Epoch 918/1500, Loss: 0.2725\n",
            "Epoch 919/1500, Loss: 0.2723\n",
            "Epoch 920/1500, Loss: 0.2720\n",
            "Epoch 921/1500, Loss: 0.2718\n",
            "Epoch 922/1500, Loss: 0.2716\n",
            "Epoch 923/1500, Loss: 0.2714\n",
            "Epoch 924/1500, Loss: 0.2712\n",
            "Epoch 925/1500, Loss: 0.2710\n",
            "Epoch 926/1500, Loss: 0.2708\n",
            "Epoch 927/1500, Loss: 0.2706\n",
            "Epoch 928/1500, Loss: 0.2704\n",
            "Epoch 929/1500, Loss: 0.2701\n",
            "Epoch 930/1500, Loss: 0.2699\n",
            "Epoch 931/1500, Loss: 0.2697\n",
            "Epoch 932/1500, Loss: 0.2695\n",
            "Epoch 933/1500, Loss: 0.2693\n",
            "Epoch 934/1500, Loss: 0.2691\n",
            "Epoch 935/1500, Loss: 0.2689\n",
            "Epoch 936/1500, Loss: 0.2687\n",
            "Epoch 937/1500, Loss: 0.2685\n",
            "Epoch 938/1500, Loss: 0.2683\n",
            "Epoch 939/1500, Loss: 0.2681\n",
            "Epoch 940/1500, Loss: 0.2679\n",
            "Epoch 941/1500, Loss: 0.2677\n",
            "Epoch 942/1500, Loss: 0.2675\n",
            "Epoch 943/1500, Loss: 0.2673\n",
            "Epoch 944/1500, Loss: 0.2671\n",
            "Epoch 945/1500, Loss: 0.2669\n",
            "Epoch 946/1500, Loss: 0.2667\n",
            "Epoch 947/1500, Loss: 0.2665\n",
            "Epoch 948/1500, Loss: 0.2663\n",
            "Epoch 949/1500, Loss: 0.2661\n",
            "Epoch 950/1500, Loss: 0.2659\n",
            "Epoch 951/1500, Loss: 0.2657\n",
            "Epoch 952/1500, Loss: 0.2655\n",
            "Epoch 953/1500, Loss: 0.2653\n",
            "Epoch 954/1500, Loss: 0.2651\n",
            "Epoch 955/1500, Loss: 0.2649\n",
            "Epoch 956/1500, Loss: 0.2647\n",
            "Epoch 957/1500, Loss: 0.2645\n",
            "Epoch 958/1500, Loss: 0.2643\n",
            "Epoch 959/1500, Loss: 0.2641\n",
            "Epoch 960/1500, Loss: 0.2639\n",
            "Epoch 961/1500, Loss: 0.2637\n",
            "Epoch 962/1500, Loss: 0.2636\n",
            "Epoch 963/1500, Loss: 0.2634\n",
            "Epoch 964/1500, Loss: 0.2632\n",
            "Epoch 965/1500, Loss: 0.2630\n",
            "Epoch 966/1500, Loss: 0.2628\n",
            "Epoch 967/1500, Loss: 0.2626\n",
            "Epoch 968/1500, Loss: 0.2624\n",
            "Epoch 969/1500, Loss: 0.2622\n",
            "Epoch 970/1500, Loss: 0.2620\n",
            "Epoch 971/1500, Loss: 0.2618\n",
            "Epoch 972/1500, Loss: 0.2617\n",
            "Epoch 973/1500, Loss: 0.2615\n",
            "Epoch 974/1500, Loss: 0.2613\n",
            "Epoch 975/1500, Loss: 0.2611\n",
            "Epoch 976/1500, Loss: 0.2609\n",
            "Epoch 977/1500, Loss: 0.2607\n",
            "Epoch 978/1500, Loss: 0.2605\n",
            "Epoch 979/1500, Loss: 0.2604\n",
            "Epoch 980/1500, Loss: 0.2602\n",
            "Epoch 981/1500, Loss: 0.2600\n",
            "Epoch 982/1500, Loss: 0.2598\n",
            "Epoch 983/1500, Loss: 0.2596\n",
            "Epoch 984/1500, Loss: 0.2594\n",
            "Epoch 985/1500, Loss: 0.2593\n",
            "Epoch 986/1500, Loss: 0.2591\n",
            "Epoch 987/1500, Loss: 0.2589\n",
            "Epoch 988/1500, Loss: 0.2587\n",
            "Epoch 989/1500, Loss: 0.2585\n",
            "Epoch 990/1500, Loss: 0.2584\n",
            "Epoch 991/1500, Loss: 0.2582\n",
            "Epoch 992/1500, Loss: 0.2580\n",
            "Epoch 993/1500, Loss: 0.2578\n",
            "Epoch 994/1500, Loss: 0.2576\n",
            "Epoch 995/1500, Loss: 0.2575\n",
            "Epoch 996/1500, Loss: 0.2573\n",
            "Epoch 997/1500, Loss: 0.2571\n",
            "Epoch 998/1500, Loss: 0.2569\n",
            "Epoch 999/1500, Loss: 0.2568\n",
            "Epoch 1000/1500, Loss: 0.2566\n",
            "Epoch 1001/1500, Loss: 0.2564\n",
            "Epoch 1002/1500, Loss: 0.2562\n",
            "Epoch 1003/1500, Loss: 0.2561\n",
            "Epoch 1004/1500, Loss: 0.2559\n",
            "Epoch 1005/1500, Loss: 0.2557\n",
            "Epoch 1006/1500, Loss: 0.2555\n",
            "Epoch 1007/1500, Loss: 0.2554\n",
            "Epoch 1008/1500, Loss: 0.2552\n",
            "Epoch 1009/1500, Loss: 0.2550\n",
            "Epoch 1010/1500, Loss: 0.2548\n",
            "Epoch 1011/1500, Loss: 0.2547\n",
            "Epoch 1012/1500, Loss: 0.2545\n",
            "Epoch 1013/1500, Loss: 0.2543\n",
            "Epoch 1014/1500, Loss: 0.2542\n",
            "Epoch 1015/1500, Loss: 0.2540\n",
            "Epoch 1016/1500, Loss: 0.2538\n",
            "Epoch 1017/1500, Loss: 0.2537\n",
            "Epoch 1018/1500, Loss: 0.2535\n",
            "Epoch 1019/1500, Loss: 0.2533\n",
            "Epoch 1020/1500, Loss: 0.2532\n",
            "Epoch 1021/1500, Loss: 0.2530\n",
            "Epoch 1022/1500, Loss: 0.2528\n",
            "Epoch 1023/1500, Loss: 0.2526\n",
            "Epoch 1024/1500, Loss: 0.2525\n",
            "Epoch 1025/1500, Loss: 0.2523\n",
            "Epoch 1026/1500, Loss: 0.2521\n",
            "Epoch 1027/1500, Loss: 0.2520\n",
            "Epoch 1028/1500, Loss: 0.2518\n",
            "Epoch 1029/1500, Loss: 0.2517\n",
            "Epoch 1030/1500, Loss: 0.2515\n",
            "Epoch 1031/1500, Loss: 0.2513\n",
            "Epoch 1032/1500, Loss: 0.2512\n",
            "Epoch 1033/1500, Loss: 0.2510\n",
            "Epoch 1034/1500, Loss: 0.2508\n",
            "Epoch 1035/1500, Loss: 0.2507\n",
            "Epoch 1036/1500, Loss: 0.2505\n",
            "Epoch 1037/1500, Loss: 0.2503\n",
            "Epoch 1038/1500, Loss: 0.2502\n",
            "Epoch 1039/1500, Loss: 0.2500\n",
            "Epoch 1040/1500, Loss: 0.2499\n",
            "Epoch 1041/1500, Loss: 0.2497\n",
            "Epoch 1042/1500, Loss: 0.2495\n",
            "Epoch 1043/1500, Loss: 0.2494\n",
            "Epoch 1044/1500, Loss: 0.2492\n",
            "Epoch 1045/1500, Loss: 0.2491\n",
            "Epoch 1046/1500, Loss: 0.2489\n",
            "Epoch 1047/1500, Loss: 0.2487\n",
            "Epoch 1048/1500, Loss: 0.2486\n",
            "Epoch 1049/1500, Loss: 0.2484\n",
            "Epoch 1050/1500, Loss: 0.2483\n",
            "Epoch 1051/1500, Loss: 0.2481\n",
            "Epoch 1052/1500, Loss: 0.2480\n",
            "Epoch 1053/1500, Loss: 0.2478\n",
            "Epoch 1054/1500, Loss: 0.2476\n",
            "Epoch 1055/1500, Loss: 0.2475\n",
            "Epoch 1056/1500, Loss: 0.2473\n",
            "Epoch 1057/1500, Loss: 0.2472\n",
            "Epoch 1058/1500, Loss: 0.2470\n",
            "Epoch 1059/1500, Loss: 0.2469\n",
            "Epoch 1060/1500, Loss: 0.2467\n",
            "Epoch 1061/1500, Loss: 0.2466\n",
            "Epoch 1062/1500, Loss: 0.2464\n",
            "Epoch 1063/1500, Loss: 0.2462\n",
            "Epoch 1064/1500, Loss: 0.2461\n",
            "Epoch 1065/1500, Loss: 0.2459\n",
            "Epoch 1066/1500, Loss: 0.2458\n",
            "Epoch 1067/1500, Loss: 0.2456\n",
            "Epoch 1068/1500, Loss: 0.2455\n",
            "Epoch 1069/1500, Loss: 0.2453\n",
            "Epoch 1070/1500, Loss: 0.2452\n",
            "Epoch 1071/1500, Loss: 0.2450\n",
            "Epoch 1072/1500, Loss: 0.2449\n",
            "Epoch 1073/1500, Loss: 0.2447\n",
            "Epoch 1074/1500, Loss: 0.2446\n",
            "Epoch 1075/1500, Loss: 0.2444\n",
            "Epoch 1076/1500, Loss: 0.2443\n",
            "Epoch 1077/1500, Loss: 0.2441\n",
            "Epoch 1078/1500, Loss: 0.2440\n",
            "Epoch 1079/1500, Loss: 0.2438\n",
            "Epoch 1080/1500, Loss: 0.2437\n",
            "Epoch 1081/1500, Loss: 0.2435\n",
            "Epoch 1082/1500, Loss: 0.2434\n",
            "Epoch 1083/1500, Loss: 0.2432\n",
            "Epoch 1084/1500, Loss: 0.2431\n",
            "Epoch 1085/1500, Loss: 0.2429\n",
            "Epoch 1086/1500, Loss: 0.2428\n",
            "Epoch 1087/1500, Loss: 0.2426\n",
            "Epoch 1088/1500, Loss: 0.2425\n",
            "Epoch 1089/1500, Loss: 0.2424\n",
            "Epoch 1090/1500, Loss: 0.2422\n",
            "Epoch 1091/1500, Loss: 0.2421\n",
            "Epoch 1092/1500, Loss: 0.2419\n",
            "Epoch 1093/1500, Loss: 0.2418\n",
            "Epoch 1094/1500, Loss: 0.2416\n",
            "Epoch 1095/1500, Loss: 0.2415\n",
            "Epoch 1096/1500, Loss: 0.2413\n",
            "Epoch 1097/1500, Loss: 0.2412\n",
            "Epoch 1098/1500, Loss: 0.2411\n",
            "Epoch 1099/1500, Loss: 0.2409\n",
            "Epoch 1100/1500, Loss: 0.2408\n",
            "Epoch 1101/1500, Loss: 0.2406\n",
            "Epoch 1102/1500, Loss: 0.2405\n",
            "Epoch 1103/1500, Loss: 0.2403\n",
            "Epoch 1104/1500, Loss: 0.2402\n",
            "Epoch 1105/1500, Loss: 0.2401\n",
            "Epoch 1106/1500, Loss: 0.2399\n",
            "Epoch 1107/1500, Loss: 0.2398\n",
            "Epoch 1108/1500, Loss: 0.2396\n",
            "Epoch 1109/1500, Loss: 0.2395\n",
            "Epoch 1110/1500, Loss: 0.2394\n",
            "Epoch 1111/1500, Loss: 0.2392\n",
            "Epoch 1112/1500, Loss: 0.2391\n",
            "Epoch 1113/1500, Loss: 0.2389\n",
            "Epoch 1114/1500, Loss: 0.2388\n",
            "Epoch 1115/1500, Loss: 0.2387\n",
            "Epoch 1116/1500, Loss: 0.2385\n",
            "Epoch 1117/1500, Loss: 0.2384\n",
            "Epoch 1118/1500, Loss: 0.2382\n",
            "Epoch 1119/1500, Loss: 0.2381\n",
            "Epoch 1120/1500, Loss: 0.2380\n",
            "Epoch 1121/1500, Loss: 0.2378\n",
            "Epoch 1122/1500, Loss: 0.2377\n",
            "Epoch 1123/1500, Loss: 0.2376\n",
            "Epoch 1124/1500, Loss: 0.2374\n",
            "Epoch 1125/1500, Loss: 0.2373\n",
            "Epoch 1126/1500, Loss: 0.2371\n",
            "Epoch 1127/1500, Loss: 0.2370\n",
            "Epoch 1128/1500, Loss: 0.2369\n",
            "Epoch 1129/1500, Loss: 0.2367\n",
            "Epoch 1130/1500, Loss: 0.2366\n",
            "Epoch 1131/1500, Loss: 0.2365\n",
            "Epoch 1132/1500, Loss: 0.2363\n",
            "Epoch 1133/1500, Loss: 0.2362\n",
            "Epoch 1134/1500, Loss: 0.2361\n",
            "Epoch 1135/1500, Loss: 0.2359\n",
            "Epoch 1136/1500, Loss: 0.2358\n",
            "Epoch 1137/1500, Loss: 0.2357\n",
            "Epoch 1138/1500, Loss: 0.2355\n",
            "Epoch 1139/1500, Loss: 0.2354\n",
            "Epoch 1140/1500, Loss: 0.2353\n",
            "Epoch 1141/1500, Loss: 0.2351\n",
            "Epoch 1142/1500, Loss: 0.2350\n",
            "Epoch 1143/1500, Loss: 0.2349\n",
            "Epoch 1144/1500, Loss: 0.2347\n",
            "Epoch 1145/1500, Loss: 0.2346\n",
            "Epoch 1146/1500, Loss: 0.2345\n",
            "Epoch 1147/1500, Loss: 0.2343\n",
            "Epoch 1148/1500, Loss: 0.2342\n",
            "Epoch 1149/1500, Loss: 0.2341\n",
            "Epoch 1150/1500, Loss: 0.2340\n",
            "Epoch 1151/1500, Loss: 0.2338\n",
            "Epoch 1152/1500, Loss: 0.2337\n",
            "Epoch 1153/1500, Loss: 0.2336\n",
            "Epoch 1154/1500, Loss: 0.2334\n",
            "Epoch 1155/1500, Loss: 0.2333\n",
            "Epoch 1156/1500, Loss: 0.2332\n",
            "Epoch 1157/1500, Loss: 0.2331\n",
            "Epoch 1158/1500, Loss: 0.2329\n",
            "Epoch 1159/1500, Loss: 0.2328\n",
            "Epoch 1160/1500, Loss: 0.2327\n",
            "Epoch 1161/1500, Loss: 0.2325\n",
            "Epoch 1162/1500, Loss: 0.2324\n",
            "Epoch 1163/1500, Loss: 0.2323\n",
            "Epoch 1164/1500, Loss: 0.2322\n",
            "Epoch 1165/1500, Loss: 0.2320\n",
            "Epoch 1166/1500, Loss: 0.2319\n",
            "Epoch 1167/1500, Loss: 0.2318\n",
            "Epoch 1168/1500, Loss: 0.2317\n",
            "Epoch 1169/1500, Loss: 0.2315\n",
            "Epoch 1170/1500, Loss: 0.2314\n",
            "Epoch 1171/1500, Loss: 0.2313\n",
            "Epoch 1172/1500, Loss: 0.2311\n",
            "Epoch 1173/1500, Loss: 0.2310\n",
            "Epoch 1174/1500, Loss: 0.2309\n",
            "Epoch 1175/1500, Loss: 0.2308\n",
            "Epoch 1176/1500, Loss: 0.2306\n",
            "Epoch 1177/1500, Loss: 0.2305\n",
            "Epoch 1178/1500, Loss: 0.2304\n",
            "Epoch 1179/1500, Loss: 0.2303\n",
            "Epoch 1180/1500, Loss: 0.2302\n",
            "Epoch 1181/1500, Loss: 0.2300\n",
            "Epoch 1182/1500, Loss: 0.2299\n",
            "Epoch 1183/1500, Loss: 0.2298\n",
            "Epoch 1184/1500, Loss: 0.2297\n",
            "Epoch 1185/1500, Loss: 0.2295\n",
            "Epoch 1186/1500, Loss: 0.2294\n",
            "Epoch 1187/1500, Loss: 0.2293\n",
            "Epoch 1188/1500, Loss: 0.2292\n",
            "Epoch 1189/1500, Loss: 0.2291\n",
            "Epoch 1190/1500, Loss: 0.2289\n",
            "Epoch 1191/1500, Loss: 0.2288\n",
            "Epoch 1192/1500, Loss: 0.2287\n",
            "Epoch 1193/1500, Loss: 0.2286\n",
            "Epoch 1194/1500, Loss: 0.2284\n",
            "Epoch 1195/1500, Loss: 0.2283\n",
            "Epoch 1196/1500, Loss: 0.2282\n",
            "Epoch 1197/1500, Loss: 0.2281\n",
            "Epoch 1198/1500, Loss: 0.2280\n",
            "Epoch 1199/1500, Loss: 0.2278\n",
            "Epoch 1200/1500, Loss: 0.2277\n",
            "Epoch 1201/1500, Loss: 0.2276\n",
            "Epoch 1202/1500, Loss: 0.2275\n",
            "Epoch 1203/1500, Loss: 0.2274\n",
            "Epoch 1204/1500, Loss: 0.2273\n",
            "Epoch 1205/1500, Loss: 0.2271\n",
            "Epoch 1206/1500, Loss: 0.2270\n",
            "Epoch 1207/1500, Loss: 0.2269\n",
            "Epoch 1208/1500, Loss: 0.2268\n",
            "Epoch 1209/1500, Loss: 0.2267\n",
            "Epoch 1210/1500, Loss: 0.2265\n",
            "Epoch 1211/1500, Loss: 0.2264\n",
            "Epoch 1212/1500, Loss: 0.2263\n",
            "Epoch 1213/1500, Loss: 0.2262\n",
            "Epoch 1214/1500, Loss: 0.2261\n",
            "Epoch 1215/1500, Loss: 0.2260\n",
            "Epoch 1216/1500, Loss: 0.2258\n",
            "Epoch 1217/1500, Loss: 0.2257\n",
            "Epoch 1218/1500, Loss: 0.2256\n",
            "Epoch 1219/1500, Loss: 0.2255\n",
            "Epoch 1220/1500, Loss: 0.2254\n",
            "Epoch 1221/1500, Loss: 0.2253\n",
            "Epoch 1222/1500, Loss: 0.2252\n",
            "Epoch 1223/1500, Loss: 0.2250\n",
            "Epoch 1224/1500, Loss: 0.2249\n",
            "Epoch 1225/1500, Loss: 0.2248\n",
            "Epoch 1226/1500, Loss: 0.2247\n",
            "Epoch 1227/1500, Loss: 0.2246\n",
            "Epoch 1228/1500, Loss: 0.2245\n",
            "Epoch 1229/1500, Loss: 0.2244\n",
            "Epoch 1230/1500, Loss: 0.2242\n",
            "Epoch 1231/1500, Loss: 0.2241\n",
            "Epoch 1232/1500, Loss: 0.2240\n",
            "Epoch 1233/1500, Loss: 0.2239\n",
            "Epoch 1234/1500, Loss: 0.2238\n",
            "Epoch 1235/1500, Loss: 0.2237\n",
            "Epoch 1236/1500, Loss: 0.2236\n",
            "Epoch 1237/1500, Loss: 0.2235\n",
            "Epoch 1238/1500, Loss: 0.2233\n",
            "Epoch 1239/1500, Loss: 0.2232\n",
            "Epoch 1240/1500, Loss: 0.2231\n",
            "Epoch 1241/1500, Loss: 0.2230\n",
            "Epoch 1242/1500, Loss: 0.2229\n",
            "Epoch 1243/1500, Loss: 0.2228\n",
            "Epoch 1244/1500, Loss: 0.2227\n",
            "Epoch 1245/1500, Loss: 0.2226\n",
            "Epoch 1246/1500, Loss: 0.2225\n",
            "Epoch 1247/1500, Loss: 0.2223\n",
            "Epoch 1248/1500, Loss: 0.2222\n",
            "Epoch 1249/1500, Loss: 0.2221\n",
            "Epoch 1250/1500, Loss: 0.2220\n",
            "Epoch 1251/1500, Loss: 0.2219\n",
            "Epoch 1252/1500, Loss: 0.2218\n",
            "Epoch 1253/1500, Loss: 0.2217\n",
            "Epoch 1254/1500, Loss: 0.2216\n",
            "Epoch 1255/1500, Loss: 0.2215\n",
            "Epoch 1256/1500, Loss: 0.2214\n",
            "Epoch 1257/1500, Loss: 0.2213\n",
            "Epoch 1258/1500, Loss: 0.2211\n",
            "Epoch 1259/1500, Loss: 0.2210\n",
            "Epoch 1260/1500, Loss: 0.2209\n",
            "Epoch 1261/1500, Loss: 0.2208\n",
            "Epoch 1262/1500, Loss: 0.2207\n",
            "Epoch 1263/1500, Loss: 0.2206\n",
            "Epoch 1264/1500, Loss: 0.2205\n",
            "Epoch 1265/1500, Loss: 0.2204\n",
            "Epoch 1266/1500, Loss: 0.2203\n",
            "Epoch 1267/1500, Loss: 0.2202\n",
            "Epoch 1268/1500, Loss: 0.2201\n",
            "Epoch 1269/1500, Loss: 0.2200\n",
            "Epoch 1270/1500, Loss: 0.2199\n",
            "Epoch 1271/1500, Loss: 0.2198\n",
            "Epoch 1272/1500, Loss: 0.2197\n",
            "Epoch 1273/1500, Loss: 0.2195\n",
            "Epoch 1274/1500, Loss: 0.2194\n",
            "Epoch 1275/1500, Loss: 0.2193\n",
            "Epoch 1276/1500, Loss: 0.2192\n",
            "Epoch 1277/1500, Loss: 0.2191\n",
            "Epoch 1278/1500, Loss: 0.2190\n",
            "Epoch 1279/1500, Loss: 0.2189\n",
            "Epoch 1280/1500, Loss: 0.2188\n",
            "Epoch 1281/1500, Loss: 0.2187\n",
            "Epoch 1282/1500, Loss: 0.2186\n",
            "Epoch 1283/1500, Loss: 0.2185\n",
            "Epoch 1284/1500, Loss: 0.2184\n",
            "Epoch 1285/1500, Loss: 0.2183\n",
            "Epoch 1286/1500, Loss: 0.2182\n",
            "Epoch 1287/1500, Loss: 0.2181\n",
            "Epoch 1288/1500, Loss: 0.2180\n",
            "Epoch 1289/1500, Loss: 0.2179\n",
            "Epoch 1290/1500, Loss: 0.2178\n",
            "Epoch 1291/1500, Loss: 0.2177\n",
            "Epoch 1292/1500, Loss: 0.2176\n",
            "Epoch 1293/1500, Loss: 0.2175\n",
            "Epoch 1294/1500, Loss: 0.2174\n",
            "Epoch 1295/1500, Loss: 0.2173\n",
            "Epoch 1296/1500, Loss: 0.2172\n",
            "Epoch 1297/1500, Loss: 0.2171\n",
            "Epoch 1298/1500, Loss: 0.2170\n",
            "Epoch 1299/1500, Loss: 0.2169\n",
            "Epoch 1300/1500, Loss: 0.2168\n",
            "Epoch 1301/1500, Loss: 0.2167\n",
            "Epoch 1302/1500, Loss: 0.2166\n",
            "Epoch 1303/1500, Loss: 0.2165\n",
            "Epoch 1304/1500, Loss: 0.2164\n",
            "Epoch 1305/1500, Loss: 0.2163\n",
            "Epoch 1306/1500, Loss: 0.2162\n",
            "Epoch 1307/1500, Loss: 0.2161\n",
            "Epoch 1308/1500, Loss: 0.2160\n",
            "Epoch 1309/1500, Loss: 0.2159\n",
            "Epoch 1310/1500, Loss: 0.2158\n",
            "Epoch 1311/1500, Loss: 0.2157\n",
            "Epoch 1312/1500, Loss: 0.2156\n",
            "Epoch 1313/1500, Loss: 0.2155\n",
            "Epoch 1314/1500, Loss: 0.2154\n",
            "Epoch 1315/1500, Loss: 0.2153\n",
            "Epoch 1316/1500, Loss: 0.2152\n",
            "Epoch 1317/1500, Loss: 0.2151\n",
            "Epoch 1318/1500, Loss: 0.2150\n",
            "Epoch 1319/1500, Loss: 0.2149\n",
            "Epoch 1320/1500, Loss: 0.2148\n",
            "Epoch 1321/1500, Loss: 0.2147\n",
            "Epoch 1322/1500, Loss: 0.2146\n",
            "Epoch 1323/1500, Loss: 0.2145\n",
            "Epoch 1324/1500, Loss: 0.2144\n",
            "Epoch 1325/1500, Loss: 0.2143\n",
            "Epoch 1326/1500, Loss: 0.2142\n",
            "Epoch 1327/1500, Loss: 0.2141\n",
            "Epoch 1328/1500, Loss: 0.2140\n",
            "Epoch 1329/1500, Loss: 0.2139\n",
            "Epoch 1330/1500, Loss: 0.2138\n",
            "Epoch 1331/1500, Loss: 0.2137\n",
            "Epoch 1332/1500, Loss: 0.2136\n",
            "Epoch 1333/1500, Loss: 0.2135\n",
            "Epoch 1334/1500, Loss: 0.2134\n",
            "Epoch 1335/1500, Loss: 0.2133\n",
            "Epoch 1336/1500, Loss: 0.2132\n",
            "Epoch 1337/1500, Loss: 0.2131\n",
            "Epoch 1338/1500, Loss: 0.2130\n",
            "Epoch 1339/1500, Loss: 0.2129\n",
            "Epoch 1340/1500, Loss: 0.2128\n",
            "Epoch 1341/1500, Loss: 0.2127\n",
            "Epoch 1342/1500, Loss: 0.2126\n",
            "Epoch 1343/1500, Loss: 0.2126\n",
            "Epoch 1344/1500, Loss: 0.2125\n",
            "Epoch 1345/1500, Loss: 0.2124\n",
            "Epoch 1346/1500, Loss: 0.2123\n",
            "Epoch 1347/1500, Loss: 0.2122\n",
            "Epoch 1348/1500, Loss: 0.2121\n",
            "Epoch 1349/1500, Loss: 0.2120\n",
            "Epoch 1350/1500, Loss: 0.2119\n",
            "Epoch 1351/1500, Loss: 0.2118\n",
            "Epoch 1352/1500, Loss: 0.2117\n",
            "Epoch 1353/1500, Loss: 0.2116\n",
            "Epoch 1354/1500, Loss: 0.2115\n",
            "Epoch 1355/1500, Loss: 0.2114\n",
            "Epoch 1356/1500, Loss: 0.2113\n",
            "Epoch 1357/1500, Loss: 0.2112\n",
            "Epoch 1358/1500, Loss: 0.2111\n",
            "Epoch 1359/1500, Loss: 0.2111\n",
            "Epoch 1360/1500, Loss: 0.2110\n",
            "Epoch 1361/1500, Loss: 0.2109\n",
            "Epoch 1362/1500, Loss: 0.2108\n",
            "Epoch 1363/1500, Loss: 0.2107\n",
            "Epoch 1364/1500, Loss: 0.2106\n",
            "Epoch 1365/1500, Loss: 0.2105\n",
            "Epoch 1366/1500, Loss: 0.2104\n",
            "Epoch 1367/1500, Loss: 0.2103\n",
            "Epoch 1368/1500, Loss: 0.2102\n",
            "Epoch 1369/1500, Loss: 0.2101\n",
            "Epoch 1370/1500, Loss: 0.2101\n",
            "Epoch 1371/1500, Loss: 0.2100\n",
            "Epoch 1372/1500, Loss: 0.2099\n",
            "Epoch 1373/1500, Loss: 0.2098\n",
            "Epoch 1374/1500, Loss: 0.2097\n",
            "Epoch 1375/1500, Loss: 0.2096\n",
            "Epoch 1376/1500, Loss: 0.2095\n",
            "Epoch 1377/1500, Loss: 0.2094\n",
            "Epoch 1378/1500, Loss: 0.2093\n",
            "Epoch 1379/1500, Loss: 0.2092\n",
            "Epoch 1380/1500, Loss: 0.2091\n",
            "Epoch 1381/1500, Loss: 0.2091\n",
            "Epoch 1382/1500, Loss: 0.2090\n",
            "Epoch 1383/1500, Loss: 0.2089\n",
            "Epoch 1384/1500, Loss: 0.2088\n",
            "Epoch 1385/1500, Loss: 0.2087\n",
            "Epoch 1386/1500, Loss: 0.2086\n",
            "Epoch 1387/1500, Loss: 0.2085\n",
            "Epoch 1388/1500, Loss: 0.2084\n",
            "Epoch 1389/1500, Loss: 0.2083\n",
            "Epoch 1390/1500, Loss: 0.2083\n",
            "Epoch 1391/1500, Loss: 0.2082\n",
            "Epoch 1392/1500, Loss: 0.2081\n",
            "Epoch 1393/1500, Loss: 0.2080\n",
            "Epoch 1394/1500, Loss: 0.2079\n",
            "Epoch 1395/1500, Loss: 0.2078\n",
            "Epoch 1396/1500, Loss: 0.2077\n",
            "Epoch 1397/1500, Loss: 0.2076\n",
            "Epoch 1398/1500, Loss: 0.2076\n",
            "Epoch 1399/1500, Loss: 0.2075\n",
            "Epoch 1400/1500, Loss: 0.2074\n",
            "Epoch 1401/1500, Loss: 0.2073\n",
            "Epoch 1402/1500, Loss: 0.2072\n",
            "Epoch 1403/1500, Loss: 0.2071\n",
            "Epoch 1404/1500, Loss: 0.2070\n",
            "Epoch 1405/1500, Loss: 0.2070\n",
            "Epoch 1406/1500, Loss: 0.2069\n",
            "Epoch 1407/1500, Loss: 0.2068\n",
            "Epoch 1408/1500, Loss: 0.2067\n",
            "Epoch 1409/1500, Loss: 0.2066\n",
            "Epoch 1410/1500, Loss: 0.2065\n",
            "Epoch 1411/1500, Loss: 0.2064\n",
            "Epoch 1412/1500, Loss: 0.2064\n",
            "Epoch 1413/1500, Loss: 0.2063\n",
            "Epoch 1414/1500, Loss: 0.2062\n",
            "Epoch 1415/1500, Loss: 0.2061\n",
            "Epoch 1416/1500, Loss: 0.2060\n",
            "Epoch 1417/1500, Loss: 0.2059\n",
            "Epoch 1418/1500, Loss: 0.2058\n",
            "Epoch 1419/1500, Loss: 0.2058\n",
            "Epoch 1420/1500, Loss: 0.2057\n",
            "Epoch 1421/1500, Loss: 0.2056\n",
            "Epoch 1422/1500, Loss: 0.2055\n",
            "Epoch 1423/1500, Loss: 0.2054\n",
            "Epoch 1424/1500, Loss: 0.2053\n",
            "Epoch 1425/1500, Loss: 0.2053\n",
            "Epoch 1426/1500, Loss: 0.2052\n",
            "Epoch 1427/1500, Loss: 0.2051\n",
            "Epoch 1428/1500, Loss: 0.2050\n",
            "Epoch 1429/1500, Loss: 0.2049\n",
            "Epoch 1430/1500, Loss: 0.2048\n",
            "Epoch 1431/1500, Loss: 0.2048\n",
            "Epoch 1432/1500, Loss: 0.2047\n",
            "Epoch 1433/1500, Loss: 0.2046\n",
            "Epoch 1434/1500, Loss: 0.2045\n",
            "Epoch 1435/1500, Loss: 0.2044\n",
            "Epoch 1436/1500, Loss: 0.2043\n",
            "Epoch 1437/1500, Loss: 0.2043\n",
            "Epoch 1438/1500, Loss: 0.2042\n",
            "Epoch 1439/1500, Loss: 0.2041\n",
            "Epoch 1440/1500, Loss: 0.2040\n",
            "Epoch 1441/1500, Loss: 0.2039\n",
            "Epoch 1442/1500, Loss: 0.2038\n",
            "Epoch 1443/1500, Loss: 0.2038\n",
            "Epoch 1444/1500, Loss: 0.2037\n",
            "Epoch 1445/1500, Loss: 0.2036\n",
            "Epoch 1446/1500, Loss: 0.2035\n",
            "Epoch 1447/1500, Loss: 0.2034\n",
            "Epoch 1448/1500, Loss: 0.2034\n",
            "Epoch 1449/1500, Loss: 0.2033\n",
            "Epoch 1450/1500, Loss: 0.2032\n",
            "Epoch 1451/1500, Loss: 0.2031\n",
            "Epoch 1452/1500, Loss: 0.2030\n",
            "Epoch 1453/1500, Loss: 0.2030\n",
            "Epoch 1454/1500, Loss: 0.2029\n",
            "Epoch 1455/1500, Loss: 0.2028\n",
            "Epoch 1456/1500, Loss: 0.2027\n",
            "Epoch 1457/1500, Loss: 0.2026\n",
            "Epoch 1458/1500, Loss: 0.2026\n",
            "Epoch 1459/1500, Loss: 0.2025\n",
            "Epoch 1460/1500, Loss: 0.2024\n",
            "Epoch 1461/1500, Loss: 0.2023\n",
            "Epoch 1462/1500, Loss: 0.2022\n",
            "Epoch 1463/1500, Loss: 0.2022\n",
            "Epoch 1464/1500, Loss: 0.2021\n",
            "Epoch 1465/1500, Loss: 0.2020\n",
            "Epoch 1466/1500, Loss: 0.2019\n",
            "Epoch 1467/1500, Loss: 0.2018\n",
            "Epoch 1468/1500, Loss: 0.2018\n",
            "Epoch 1469/1500, Loss: 0.2017\n",
            "Epoch 1470/1500, Loss: 0.2016\n",
            "Epoch 1471/1500, Loss: 0.2015\n",
            "Epoch 1472/1500, Loss: 0.2014\n",
            "Epoch 1473/1500, Loss: 0.2014\n",
            "Epoch 1474/1500, Loss: 0.2013\n",
            "Epoch 1475/1500, Loss: 0.2012\n",
            "Epoch 1476/1500, Loss: 0.2011\n",
            "Epoch 1477/1500, Loss: 0.2011\n",
            "Epoch 1478/1500, Loss: 0.2010\n",
            "Epoch 1479/1500, Loss: 0.2009\n",
            "Epoch 1480/1500, Loss: 0.2008\n",
            "Epoch 1481/1500, Loss: 0.2007\n",
            "Epoch 1482/1500, Loss: 0.2007\n",
            "Epoch 1483/1500, Loss: 0.2006\n",
            "Epoch 1484/1500, Loss: 0.2005\n",
            "Epoch 1485/1500, Loss: 0.2004\n",
            "Epoch 1486/1500, Loss: 0.2004\n",
            "Epoch 1487/1500, Loss: 0.2003\n",
            "Epoch 1488/1500, Loss: 0.2002\n",
            "Epoch 1489/1500, Loss: 0.2001\n",
            "Epoch 1490/1500, Loss: 0.2001\n",
            "Epoch 1491/1500, Loss: 0.2000\n",
            "Epoch 1492/1500, Loss: 0.1999\n",
            "Epoch 1493/1500, Loss: 0.1998\n",
            "Epoch 1494/1500, Loss: 0.1998\n",
            "Epoch 1495/1500, Loss: 0.1997\n",
            "Epoch 1496/1500, Loss: 0.1996\n",
            "Epoch 1497/1500, Loss: 0.1995\n",
            "Epoch 1498/1500, Loss: 0.1995\n",
            "Epoch 1499/1500, Loss: 0.1994\n",
            "Test Accuracy: 76.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ortK4xkkhVZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}