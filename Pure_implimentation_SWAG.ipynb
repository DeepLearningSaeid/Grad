{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMr6XbAEJNeXW2C38Ve4Tf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningSaeid/Grad/blob/main/Pure_implimentation_SWAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrjnEKdTVr4G",
        "outputId": "42dd3e4b-ec26-41fa-a4cb-0a92c4ae22f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 5ms/step\n",
            "3/3 [==============================] - 0s 4ms/step\n",
            "3/3 [==============================] - 0s 4ms/step\n",
            "3/3 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f333c71bf40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3328970160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f33289725f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3328972ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step\n",
            "3/3 [==============================] - 0s 5ms/step\n",
            "3/3 [==============================] - 0s 4ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "3/3 [==============================] - 0s 5ms/step\n",
            "Average Loss: 0.16\n",
            "Average MSE: 0.31\n",
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_10 (InputLayer)       [(None, 8)]                  0         []                            \n",
            "                                                                                                  \n",
            " dense_45 (Dense)            (None, 50)                   450       ['input_10[0][0]']            \n",
            "                                                                                                  \n",
            " dense_46 (Dense)            (None, 50)                   450       ['input_10[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenat  (None, 100)                  0         ['dense_45[0][0]',            \n",
            " e)                                                                  'dense_46[0][0]']            \n",
            "                                                                                                  \n",
            " dense_47 (Dense)            (None, 50)                   5050      ['concatenate_18[0][0]']      \n",
            "                                                                                                  \n",
            " dense_48 (Dense)            (None, 50)                   2550      ['dense_47[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenat  (None, 200)                  0         ['dense_45[0][0]',            \n",
            " e)                                                                  'dense_46[0][0]',            \n",
            "                                                                     'dense_47[0][0]',            \n",
            "                                                                     'dense_48[0][0]']            \n",
            "                                                                                                  \n",
            " dense_49 (Dense)            (None, 1)                    201       ['concatenate_19[0][0]']      \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8701 (33.99 KB)\n",
            "Trainable params: 8701 (33.99 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import Input, Dense, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import Activation\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "\n",
        "# Load Pima Indians Diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop('class', axis=1).values\n",
        "Y = data['class'].values\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "def define_activation_functions():\n",
        "    activations = [\n",
        "        ('X_1', lambda x: K.pow(x, 1)),\n",
        "        ('X_2', lambda x: K.pow(x, 2) / 2),\n",
        "        ('X_2_', lambda x: K.pow(x, 2) / 24),\n",
        "        ('X_2__', lambda x: K.pow(x, 2) / 720),\n",
        "    ]\n",
        "    for name, func in activations:\n",
        "        get_custom_objects().update({name: Activation(func)})\n",
        "\n",
        "define_activation_functions()\n",
        "\n",
        "def create_optimized_model(input_dim, hidden_dim, output_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    x1 = Dense(hidden_dim, activation='X_1')(input_layer)\n",
        "    x2 = Dense(hidden_dim, activation='X_2')(input_layer)\n",
        "    merged1 = concatenate([x1, x2])\n",
        "    x3 = Dense(hidden_dim, activation='X_2_')(merged1)\n",
        "    x4 = Dense(hidden_dim, activation='X_2__')(x3)\n",
        "    merged2 = concatenate([x1, x2, x3, x4])\n",
        "    output = Dense(output_dim, activation='X_1')(merged2)\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "    return model\n",
        "\n",
        "input_dim = 8\n",
        "hidden_dim = 50\n",
        "output_dim = 1\n",
        "n_folds = 10\n",
        "\n",
        "# Initialize Stratified K-Fold\n",
        "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store performance metrics for each fold\n",
        "accuracy_scores = []\n",
        "mse_scores = []\n",
        "\n",
        "# Perform cross-validation\n",
        "for train_index, val_index in kf.split(X, Y):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = create_optimized_model(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss, val_mse = model.evaluate(X_val, Y_val, verbose=0)\n",
        "\n",
        "    # Calculate Mean Squared Error (MSE)\n",
        "    Y_val_pred = model.predict(X_val)\n",
        "    mse = np.mean((Y_val - Y_val_pred)**2)\n",
        "\n",
        "    # Append accuracy and MSE to lists\n",
        "    accuracy_scores.append(val_loss)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Calculate and print the average performance metrics over all folds\n",
        "average_loss = np.mean(accuracy_scores)\n",
        "average_mse = np.mean(mse_scores)\n",
        "\n",
        "print(f'Average Loss: {average_loss:.2f}')\n",
        "print(f'Average MSE: {average_mse:.2f}')\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, concatenate, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class CustomNeuralNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.model = self._create_model()\n",
        "\n",
        "    def _define_activation_functions(self):\n",
        "        activations = [\n",
        "            ('X_1', lambda x: x),  # Identity function for X_1\n",
        "            ('X_2', lambda x: tf.pow(x, 2) / 2),\n",
        "            ('X_2_', lambda x: tf.pow(x, 2) / 24),\n",
        "            ('X_2__', lambda x: tf.pow(x, 2) / 720),\n",
        "        ]\n",
        "        for name, func in activations:\n",
        "            tf.keras.utils.get_custom_objects()[name] = Activation(func)\n",
        "\n",
        "    def _create_model(self):\n",
        "        self._define_activation_functions()\n",
        "\n",
        "        input_layer = Input(shape=(self.input_dim,))\n",
        "        x1 = Dense(self.hidden_dim, activation='X_1')(input_layer)\n",
        "        x2 = Dense(self.hidden_dim, activation='X_2')(input_layer)\n",
        "        merged1 = concatenate([x1, x2])\n",
        "        x3 = Dense(self.hidden_dim, activation='X_2_')(merged1)\n",
        "        x4 = Dense(self.hidden_dim, activation='X_2__')(x3)\n",
        "        merged2 = concatenate([x1, x2, x3, x4])\n",
        "        output = Dense(self.output_dim, activation='X_1')(merged2)\n",
        "\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        return self.model.summary()\n",
        "\n",
        "    def fit(self, x, y, **kwargs):\n",
        "        return self.model.fit(x, y, **kwargs)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.model.predict(x)\n",
        "\n",
        "# Example usage:\n",
        "# custom_nn = CustomNeuralNetwork(input_dim, hidden_dim, output_dim)\n",
        "# custom_nn.summary()  # Display model summary\n",
        "# custom_nn.fit(x_train, y_train, epochs=100, batch_size=32)  # Train the model\n",
        "# predictions = custom_nn.predict(x_test)  # Make predictions\n"
      ],
      "metadata": {
        "id": "wtUHab74e5v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsELz_UncFCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LYJgY32OJcwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fph6lAJCJcrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the architecture\n",
        "input_size = 2\n",
        "hidden1_size = 4\n",
        "hidden2_size = 3\n",
        "hidden3_size = 2\n",
        "output_size = 1\n",
        "\n",
        "# Define activation functions (sigmoid in this case)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Define training parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Generate random input data and corresponding target values\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, input_size)\n",
        "y = np.random.randint(2, size=(100, output_size))\n",
        "\n",
        "# Initialize weights and biases\n",
        "weights = {\n",
        "    'W1': np.random.randn(input_size, hidden1_size),\n",
        "    'W2': np.random.randn(hidden1_size, hidden2_size),\n",
        "    'W3': np.random.randn(hidden2_size, hidden3_size),\n",
        "    'W4': np.random.randn(hidden3_size, output_size)\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b1': np.zeros((1, hidden1_size)),\n",
        "    'b2': np.zeros((1, hidden2_size)),\n",
        "    'b3': np.zeros((1, hidden3_size)),\n",
        "    'b4': np.zeros((1, output_size))\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, weights['W1']) + biases['b1']\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    z2 = np.dot(a1, weights['W2']) + biases['b2']\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    z3 = np.dot(a2, weights['W3']) + biases['b3']\n",
        "    a3 = sigmoid(z3)\n",
        "\n",
        "    z4 = np.dot(a3, weights['W4']) + biases['b4']\n",
        "    output = sigmoid(z4)\n",
        "\n",
        "    # Calculate the loss (mean squared error)\n",
        "    loss = np.mean((output - y) ** 2)\n",
        "\n",
        "    # Backward pass\n",
        "    delta4 = 2 * (output - y) * sigmoid_derivative(output)\n",
        "    dW4 = np.dot(a3.T, delta4)\n",
        "    db4 = np.sum(delta4, axis=0, keepdims=True)\n",
        "\n",
        "    delta3 = np.dot(delta4, weights['W4'].T) * sigmoid_derivative(a3)\n",
        "    dW3 = np.dot(a2.T, delta3)\n",
        "    db3 = np.sum(delta3, axis=0, keepdims=True)\n",
        "\n",
        "    delta2 = np.dot(delta3, weights['W3'].T) * sigmoid_derivative(a2)\n",
        "    dW2 = np.dot(a1.T, delta2)\n",
        "    db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "\n",
        "    delta1 = np.dot(delta2, weights['W2'].T) * sigmoid_derivative(a1)\n",
        "    dW1 = np.dot(X.T, delta1)\n",
        "    db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights and biases\n",
        "    weights['W4'] -= learning_rate * dW4\n",
        "    biases['b4'] -= learning_rate * db4\n",
        "    weights['W3'] -= learning_rate * dW3\n",
        "    biases['b3'] -= learning_rate * db3\n",
        "    weights['W2'] -= learning_rate * dW2\n",
        "    biases['b2'] -= learning_rate * db2\n",
        "    weights['W1'] -= learning_rate * dW1\n",
        "    biases['b1'] -= learning_rate * db1\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "# After training, you can use the trained neural network for predictions.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jURuNJqhJcjF",
        "outputId": "4ba98a3f-1d7a-4cfc-e486-9b6e40416ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10000, Loss: 0.2882\n",
            "Epoch 1000/10000, Loss: 0.2419\n",
            "Epoch 2000/10000, Loss: 0.2379\n",
            "Epoch 3000/10000, Loss: 0.2251\n",
            "Epoch 4000/10000, Loss: 0.2086\n",
            "Epoch 5000/10000, Loss: 0.2032\n",
            "Epoch 6000/10000, Loss: 0.2003\n",
            "Epoch 7000/10000, Loss: 0.1998\n",
            "Epoch 8000/10000, Loss: 0.2076\n",
            "Epoch 9000/10000, Loss: 0.1965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, output_size):\n",
        "        # Define the architecture\n",
        "        self.input_size = input_size\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "        self.hidden3_size = hidden3_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = {\n",
        "            'W1': np.random.randn(input_size, hidden1_size),\n",
        "            'W2': np.random.randn(input_size, hidden2_size),\n",
        "            'W3': np.random.randn(input_size, hidden3_size),\n",
        "            'W4': np.random.randn(hidden1_size + hidden2_size + hidden3_size, output_size)\n",
        "        }\n",
        "\n",
        "        self.biases = {\n",
        "            'b1': np.zeros((1, hidden1_size)),\n",
        "            'b2': np.zeros((1, hidden2_size)),\n",
        "            'b3': np.zeros((1, hidden3_size)),\n",
        "            'b4': np.zeros((1, output_size))\n",
        "        }\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Layer 1\n",
        "        self.z1 = np.dot(X, self.weights['W1']) + self.biases['b1']\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "\n",
        "        # Layer 2\n",
        "        self.z2 = np.dot(X, self.weights['W2']) + self.biases['b2']\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.z3 = np.dot(X, self.weights['W3']) + self.biases['b3']\n",
        "        self.a3 = self.sigmoid(self.z3)\n",
        "\n",
        "        # Concatenate the outputs of layers 1, 2, and 3\n",
        "        self.concatenated_output = np.concatenate((self.a1, self.a2, self.a3), axis=1)\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        self.z4 = np.dot(self.concatenated_output, self.weights['W4']) + self.biases['b4']\n",
        "        self.output = self.sigmoid(self.z4)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Backpropagation\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        delta4 = 2 * (self.output - y) * self.sigmoid_derivative(self.output)\n",
        "        dW4 = np.dot(self.concatenated_output.T, delta4)\n",
        "        db4 = np.sum(delta4, axis=0, keepdims=True)\n",
        "\n",
        "        # Split the delta for the concatenation in layer 4\n",
        "        delta4_split = np.dot(delta4, self.weights['W4'].T)\n",
        "\n",
        "        # Separate the deltas for layers 1, 2, and 3\n",
        "        delta3 = delta4_split[:, -self.hidden3_size:] * self.sigmoid_derivative(self.a3)\n",
        "        delta2 = delta4_split[:, -self.hidden3_size-self.hidden2_size:-self.hidden3_size] * self.sigmoid_derivative(self.a2)\n",
        "        delta1 = delta4_split[:, :-self.hidden3_size-self.hidden2_size] * self.sigmoid_derivative(self.a1)\n",
        "\n",
        "        # Calculate gradients for layers 1, 2, and 3\n",
        "        dW3 = np.dot(X.T, delta3)\n",
        "        db3 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        dW2 = np.dot(X.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights['W4'] -= learning_rate * dW4\n",
        "        self.biases['b4'] -= learning_rate * db4\n",
        "        self.weights['W3'] -= learning_rate * dW3\n",
        "        self.biases['b3'] -= learning_rate * db3\n",
        "        self.weights['W2'] -= learning_rate * dW2\n",
        "        self.biases['b2'] -= learning_rate * db2\n",
        "        self.weights['W1'] -= learning_rate * dW1\n",
        "        self.biases['b1'] -= learning_rate * db1\n",
        "\n",
        "        return dW4, db4, dW3, db3, dW2, db2, dW1, db1\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            self.feedforward(X)\n",
        "\n",
        "            # Backpropagation\n",
        "            self.backpropagation(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean((self.output - y) ** 2)\n",
        "                print(f'Epoch {epoch}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "# Example usage:\n",
        "input_size = 2\n",
        "hidden1_size = 4\n",
        "hidden2_size = 3\n",
        "hidden3_size = 2\n",
        "output_size = 1\n",
        "learning_rate = 0.1\n",
        "epochs = 10000000\n",
        "\n",
        "# Generate random input data and corresponding target values\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, input_size)\n",
        "y = np.random.randint(2, size=(100, output_size))\n",
        "\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size, hidden3_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "nn.train(X, y, learning_rate, epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CCE1A3nMLEbE",
        "outputId": "1e65805e-f344-42c4-94e9-e77bf1d00688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10000000, Loss: 0.2618\n",
            "Epoch 1000/10000000, Loss: 0.2438\n",
            "Epoch 2000/10000000, Loss: 0.2437\n",
            "Epoch 3000/10000000, Loss: 0.2435\n",
            "Epoch 4000/10000000, Loss: 0.2431\n",
            "Epoch 5000/10000000, Loss: 0.2424\n",
            "Epoch 6000/10000000, Loss: 0.2417\n",
            "Epoch 7000/10000000, Loss: 0.2417\n",
            "Epoch 8000/10000000, Loss: 0.2404\n",
            "Epoch 9000/10000000, Loss: 0.2383\n",
            "Epoch 10000/10000000, Loss: 0.2362\n",
            "Epoch 11000/10000000, Loss: 0.2342\n",
            "Epoch 12000/10000000, Loss: 0.2322\n",
            "Epoch 13000/10000000, Loss: 0.2305\n",
            "Epoch 14000/10000000, Loss: 0.2290\n",
            "Epoch 15000/10000000, Loss: 0.2277\n",
            "Epoch 16000/10000000, Loss: 0.2267\n",
            "Epoch 17000/10000000, Loss: 0.2258\n",
            "Epoch 18000/10000000, Loss: 0.2251\n",
            "Epoch 19000/10000000, Loss: 0.2244\n",
            "Epoch 20000/10000000, Loss: 0.2238\n",
            "Epoch 21000/10000000, Loss: 0.2233\n",
            "Epoch 22000/10000000, Loss: 0.2229\n",
            "Epoch 23000/10000000, Loss: 0.2225\n",
            "Epoch 24000/10000000, Loss: 0.2221\n",
            "Epoch 25000/10000000, Loss: 0.2218\n",
            "Epoch 26000/10000000, Loss: 0.2215\n",
            "Epoch 27000/10000000, Loss: 0.2212\n",
            "Epoch 28000/10000000, Loss: 0.2210\n",
            "Epoch 29000/10000000, Loss: 0.2208\n",
            "Epoch 30000/10000000, Loss: 0.2206\n",
            "Epoch 31000/10000000, Loss: 0.2204\n",
            "Epoch 32000/10000000, Loss: 0.2202\n",
            "Epoch 33000/10000000, Loss: 0.2200\n",
            "Epoch 34000/10000000, Loss: 0.2198\n",
            "Epoch 35000/10000000, Loss: 0.2196\n",
            "Epoch 36000/10000000, Loss: 0.2195\n",
            "Epoch 37000/10000000, Loss: 0.2193\n",
            "Epoch 38000/10000000, Loss: 0.2192\n",
            "Epoch 39000/10000000, Loss: 0.2191\n",
            "Epoch 40000/10000000, Loss: 0.2189\n",
            "Epoch 41000/10000000, Loss: 0.2188\n",
            "Epoch 42000/10000000, Loss: 0.2187\n",
            "Epoch 43000/10000000, Loss: 0.2186\n",
            "Epoch 44000/10000000, Loss: 0.2185\n",
            "Epoch 45000/10000000, Loss: 0.2184\n",
            "Epoch 46000/10000000, Loss: 0.2182\n",
            "Epoch 47000/10000000, Loss: 0.2181\n",
            "Epoch 48000/10000000, Loss: 0.2181\n",
            "Epoch 49000/10000000, Loss: 0.2180\n",
            "Epoch 50000/10000000, Loss: 0.2179\n",
            "Epoch 51000/10000000, Loss: 0.2178\n",
            "Epoch 52000/10000000, Loss: 0.2177\n",
            "Epoch 53000/10000000, Loss: 0.2176\n",
            "Epoch 54000/10000000, Loss: 0.2175\n",
            "Epoch 55000/10000000, Loss: 0.2175\n",
            "Epoch 56000/10000000, Loss: 0.2174\n",
            "Epoch 57000/10000000, Loss: 0.2173\n",
            "Epoch 58000/10000000, Loss: 0.2172\n",
            "Epoch 59000/10000000, Loss: 0.2172\n",
            "Epoch 60000/10000000, Loss: 0.2171\n",
            "Epoch 61000/10000000, Loss: 0.2170\n",
            "Epoch 62000/10000000, Loss: 0.2170\n",
            "Epoch 63000/10000000, Loss: 0.2169\n",
            "Epoch 64000/10000000, Loss: 0.2168\n",
            "Epoch 65000/10000000, Loss: 0.2168\n",
            "Epoch 66000/10000000, Loss: 0.2167\n",
            "Epoch 67000/10000000, Loss: 0.2166\n",
            "Epoch 68000/10000000, Loss: 0.2166\n",
            "Epoch 69000/10000000, Loss: 0.2165\n",
            "Epoch 70000/10000000, Loss: 0.2165\n",
            "Epoch 71000/10000000, Loss: 0.2164\n",
            "Epoch 72000/10000000, Loss: 0.2164\n",
            "Epoch 73000/10000000, Loss: 0.2163\n",
            "Epoch 74000/10000000, Loss: 0.2162\n",
            "Epoch 75000/10000000, Loss: 0.2162\n",
            "Epoch 76000/10000000, Loss: 0.2162\n",
            "Epoch 77000/10000000, Loss: 0.2163\n",
            "Epoch 78000/10000000, Loss: 0.2166\n",
            "Epoch 79000/10000000, Loss: 0.2167\n",
            "Epoch 80000/10000000, Loss: 0.2168\n",
            "Epoch 81000/10000000, Loss: 0.2169\n",
            "Epoch 82000/10000000, Loss: 0.2168\n",
            "Epoch 83000/10000000, Loss: 0.2168\n",
            "Epoch 84000/10000000, Loss: 0.2167\n",
            "Epoch 85000/10000000, Loss: 0.2165\n",
            "Epoch 86000/10000000, Loss: 0.2164\n",
            "Epoch 87000/10000000, Loss: 0.2163\n",
            "Epoch 88000/10000000, Loss: 0.2161\n",
            "Epoch 89000/10000000, Loss: 0.2160\n",
            "Epoch 90000/10000000, Loss: 0.2158\n",
            "Epoch 91000/10000000, Loss: 0.2156\n",
            "Epoch 92000/10000000, Loss: 0.2155\n",
            "Epoch 93000/10000000, Loss: 0.2153\n",
            "Epoch 94000/10000000, Loss: 0.2151\n",
            "Epoch 95000/10000000, Loss: 0.2149\n",
            "Epoch 96000/10000000, Loss: 0.2148\n",
            "Epoch 97000/10000000, Loss: 0.2146\n",
            "Epoch 98000/10000000, Loss: 0.2144\n",
            "Epoch 99000/10000000, Loss: 0.2142\n",
            "Epoch 100000/10000000, Loss: 0.2140\n",
            "Epoch 101000/10000000, Loss: 0.2138\n",
            "Epoch 102000/10000000, Loss: 0.2137\n",
            "Epoch 103000/10000000, Loss: 0.2135\n",
            "Epoch 104000/10000000, Loss: 0.2133\n",
            "Epoch 105000/10000000, Loss: 0.2131\n",
            "Epoch 106000/10000000, Loss: 0.2129\n",
            "Epoch 107000/10000000, Loss: 0.2127\n",
            "Epoch 108000/10000000, Loss: 0.2125\n",
            "Epoch 109000/10000000, Loss: 0.2123\n",
            "Epoch 110000/10000000, Loss: 0.2121\n",
            "Epoch 111000/10000000, Loss: 0.2119\n",
            "Epoch 112000/10000000, Loss: 0.2117\n",
            "Epoch 113000/10000000, Loss: 0.2116\n",
            "Epoch 114000/10000000, Loss: 0.2114\n",
            "Epoch 115000/10000000, Loss: 0.2112\n",
            "Epoch 116000/10000000, Loss: 0.2111\n",
            "Epoch 117000/10000000, Loss: 0.2109\n",
            "Epoch 118000/10000000, Loss: 0.2108\n",
            "Epoch 119000/10000000, Loss: 0.2107\n",
            "Epoch 120000/10000000, Loss: 0.2105\n",
            "Epoch 121000/10000000, Loss: 0.2104\n",
            "Epoch 122000/10000000, Loss: 0.2103\n",
            "Epoch 123000/10000000, Loss: 0.2102\n",
            "Epoch 124000/10000000, Loss: 0.2101\n",
            "Epoch 125000/10000000, Loss: 0.2100\n",
            "Epoch 126000/10000000, Loss: 0.2099\n",
            "Epoch 127000/10000000, Loss: 0.2098\n",
            "Epoch 128000/10000000, Loss: 0.2098\n",
            "Epoch 129000/10000000, Loss: 0.2097\n",
            "Epoch 130000/10000000, Loss: 0.2096\n",
            "Epoch 131000/10000000, Loss: 0.2095\n",
            "Epoch 132000/10000000, Loss: 0.2094\n",
            "Epoch 133000/10000000, Loss: 0.2094\n",
            "Epoch 134000/10000000, Loss: 0.2093\n",
            "Epoch 135000/10000000, Loss: 0.2092\n",
            "Epoch 136000/10000000, Loss: 0.2092\n",
            "Epoch 137000/10000000, Loss: 0.2091\n",
            "Epoch 138000/10000000, Loss: 0.2091\n",
            "Epoch 139000/10000000, Loss: 0.2090\n",
            "Epoch 140000/10000000, Loss: 0.2089\n",
            "Epoch 141000/10000000, Loss: 0.2089\n",
            "Epoch 142000/10000000, Loss: 0.2085\n",
            "Epoch 143000/10000000, Loss: 0.2090\n",
            "Epoch 144000/10000000, Loss: 0.2083\n",
            "Epoch 145000/10000000, Loss: 0.2082\n",
            "Epoch 146000/10000000, Loss: 0.2104\n",
            "Epoch 147000/10000000, Loss: 0.2082\n",
            "Epoch 148000/10000000, Loss: 0.2081\n",
            "Epoch 149000/10000000, Loss: 0.2081\n",
            "Epoch 150000/10000000, Loss: 0.2080\n",
            "Epoch 151000/10000000, Loss: 0.2080\n",
            "Epoch 152000/10000000, Loss: 0.2080\n",
            "Epoch 153000/10000000, Loss: 0.2079\n",
            "Epoch 154000/10000000, Loss: 0.2079\n",
            "Epoch 155000/10000000, Loss: 0.2079\n",
            "Epoch 156000/10000000, Loss: 0.2078\n",
            "Epoch 157000/10000000, Loss: 0.2080\n",
            "Epoch 158000/10000000, Loss: 0.2078\n",
            "Epoch 159000/10000000, Loss: 0.2077\n",
            "Epoch 160000/10000000, Loss: 0.2077\n",
            "Epoch 161000/10000000, Loss: 0.2077\n",
            "Epoch 162000/10000000, Loss: 0.2077\n",
            "Epoch 163000/10000000, Loss: 0.2076\n",
            "Epoch 164000/10000000, Loss: 0.2076\n",
            "Epoch 165000/10000000, Loss: 0.2076\n",
            "Epoch 166000/10000000, Loss: 0.2075\n",
            "Epoch 167000/10000000, Loss: 0.2076\n",
            "Epoch 168000/10000000, Loss: 0.2075\n",
            "Epoch 169000/10000000, Loss: 0.2075\n",
            "Epoch 170000/10000000, Loss: 0.2074\n",
            "Epoch 171000/10000000, Loss: 0.2074\n",
            "Epoch 172000/10000000, Loss: 0.2100\n",
            "Epoch 173000/10000000, Loss: 0.2074\n",
            "Epoch 174000/10000000, Loss: 0.2076\n",
            "Epoch 175000/10000000, Loss: 0.2073\n",
            "Epoch 176000/10000000, Loss: 0.2073\n",
            "Epoch 177000/10000000, Loss: 0.2073\n",
            "Epoch 178000/10000000, Loss: 0.2073\n",
            "Epoch 179000/10000000, Loss: 0.2072\n",
            "Epoch 180000/10000000, Loss: 0.2072\n",
            "Epoch 181000/10000000, Loss: 0.2072\n",
            "Epoch 182000/10000000, Loss: 0.2072\n",
            "Epoch 183000/10000000, Loss: 0.2071\n",
            "Epoch 184000/10000000, Loss: 0.2071\n",
            "Epoch 185000/10000000, Loss: 0.2080\n",
            "Epoch 186000/10000000, Loss: 0.2071\n",
            "Epoch 187000/10000000, Loss: 0.2070\n",
            "Epoch 188000/10000000, Loss: 0.2070\n",
            "Epoch 189000/10000000, Loss: 0.2069\n",
            "Epoch 190000/10000000, Loss: 0.2070\n",
            "Epoch 191000/10000000, Loss: 0.2098\n",
            "Epoch 192000/10000000, Loss: 0.2105\n",
            "Epoch 193000/10000000, Loss: 0.2102\n",
            "Epoch 194000/10000000, Loss: 0.2099\n",
            "Epoch 195000/10000000, Loss: 0.2097\n",
            "Epoch 196000/10000000, Loss: 0.2096\n",
            "Epoch 197000/10000000, Loss: 0.2094\n",
            "Epoch 198000/10000000, Loss: 0.2093\n",
            "Epoch 199000/10000000, Loss: 0.2091\n",
            "Epoch 200000/10000000, Loss: 0.2090\n",
            "Epoch 201000/10000000, Loss: 0.2089\n",
            "Epoch 202000/10000000, Loss: 0.2087\n",
            "Epoch 203000/10000000, Loss: 0.2086\n",
            "Epoch 204000/10000000, Loss: 0.2085\n",
            "Epoch 205000/10000000, Loss: 0.2084\n",
            "Epoch 206000/10000000, Loss: 0.2083\n",
            "Epoch 207000/10000000, Loss: 0.2082\n",
            "Epoch 208000/10000000, Loss: 0.2081\n",
            "Epoch 209000/10000000, Loss: 0.2080\n",
            "Epoch 210000/10000000, Loss: 0.2079\n",
            "Epoch 211000/10000000, Loss: 0.2078\n",
            "Epoch 212000/10000000, Loss: 0.2077\n",
            "Epoch 213000/10000000, Loss: 0.2076\n",
            "Epoch 214000/10000000, Loss: 0.2076\n",
            "Epoch 215000/10000000, Loss: 0.2075\n",
            "Epoch 216000/10000000, Loss: 0.2074\n",
            "Epoch 217000/10000000, Loss: 0.2074\n",
            "Epoch 218000/10000000, Loss: 0.2073\n",
            "Epoch 219000/10000000, Loss: 0.2072\n",
            "Epoch 220000/10000000, Loss: 0.2072\n",
            "Epoch 221000/10000000, Loss: 0.2071\n",
            "Epoch 222000/10000000, Loss: 0.2071\n",
            "Epoch 223000/10000000, Loss: 0.2070\n",
            "Epoch 224000/10000000, Loss: 0.2070\n",
            "Epoch 225000/10000000, Loss: 0.2069\n",
            "Epoch 226000/10000000, Loss: 0.2069\n",
            "Epoch 227000/10000000, Loss: 0.2069\n",
            "Epoch 228000/10000000, Loss: 0.2068\n",
            "Epoch 229000/10000000, Loss: 0.2068\n",
            "Epoch 230000/10000000, Loss: 0.2068\n",
            "Epoch 231000/10000000, Loss: 0.2067\n",
            "Epoch 232000/10000000, Loss: 0.2067\n",
            "Epoch 233000/10000000, Loss: 0.2067\n",
            "Epoch 234000/10000000, Loss: 0.2066\n",
            "Epoch 235000/10000000, Loss: 0.2066\n",
            "Epoch 236000/10000000, Loss: 0.2066\n",
            "Epoch 237000/10000000, Loss: 0.2065\n",
            "Epoch 238000/10000000, Loss: 0.2065\n",
            "Epoch 239000/10000000, Loss: 0.2065\n",
            "Epoch 240000/10000000, Loss: 0.2065\n",
            "Epoch 241000/10000000, Loss: 0.2064\n",
            "Epoch 242000/10000000, Loss: 0.2064\n",
            "Epoch 243000/10000000, Loss: 0.2064\n",
            "Epoch 244000/10000000, Loss: 0.2064\n",
            "Epoch 245000/10000000, Loss: 0.2063\n",
            "Epoch 246000/10000000, Loss: 0.2063\n",
            "Epoch 247000/10000000, Loss: 0.2063\n",
            "Epoch 248000/10000000, Loss: 0.2063\n",
            "Epoch 249000/10000000, Loss: 0.2063\n",
            "Epoch 250000/10000000, Loss: 0.2062\n",
            "Epoch 251000/10000000, Loss: 0.2062\n",
            "Epoch 252000/10000000, Loss: 0.2062\n",
            "Epoch 253000/10000000, Loss: 0.2062\n",
            "Epoch 254000/10000000, Loss: 0.2062\n",
            "Epoch 255000/10000000, Loss: 0.2061\n",
            "Epoch 256000/10000000, Loss: 0.2061\n",
            "Epoch 257000/10000000, Loss: 0.2061\n",
            "Epoch 258000/10000000, Loss: 0.2061\n",
            "Epoch 259000/10000000, Loss: 0.2061\n",
            "Epoch 260000/10000000, Loss: 0.2060\n",
            "Epoch 261000/10000000, Loss: 0.2060\n",
            "Epoch 262000/10000000, Loss: 0.2060\n",
            "Epoch 263000/10000000, Loss: 0.2060\n",
            "Epoch 264000/10000000, Loss: 0.2060\n",
            "Epoch 265000/10000000, Loss: 0.2060\n",
            "Epoch 266000/10000000, Loss: 0.2059\n",
            "Epoch 267000/10000000, Loss: 0.2059\n",
            "Epoch 268000/10000000, Loss: 0.2059\n",
            "Epoch 269000/10000000, Loss: 0.2059\n",
            "Epoch 270000/10000000, Loss: 0.2059\n",
            "Epoch 271000/10000000, Loss: 0.2059\n",
            "Epoch 272000/10000000, Loss: 0.2058\n",
            "Epoch 273000/10000000, Loss: 0.2058\n",
            "Epoch 274000/10000000, Loss: 0.2058\n",
            "Epoch 275000/10000000, Loss: 0.2058\n",
            "Epoch 276000/10000000, Loss: 0.2058\n",
            "Epoch 277000/10000000, Loss: 0.2058\n",
            "Epoch 278000/10000000, Loss: 0.2057\n",
            "Epoch 279000/10000000, Loss: 0.2057\n",
            "Epoch 280000/10000000, Loss: 0.2057\n",
            "Epoch 281000/10000000, Loss: 0.2057\n",
            "Epoch 282000/10000000, Loss: 0.2057\n",
            "Epoch 283000/10000000, Loss: 0.2057\n",
            "Epoch 284000/10000000, Loss: 0.2057\n",
            "Epoch 285000/10000000, Loss: 0.2056\n",
            "Epoch 286000/10000000, Loss: 0.2056\n",
            "Epoch 287000/10000000, Loss: 0.2056\n",
            "Epoch 288000/10000000, Loss: 0.2056\n",
            "Epoch 289000/10000000, Loss: 0.2056\n",
            "Epoch 290000/10000000, Loss: 0.2056\n",
            "Epoch 291000/10000000, Loss: 0.2055\n",
            "Epoch 292000/10000000, Loss: 0.2055\n",
            "Epoch 293000/10000000, Loss: 0.2055\n",
            "Epoch 294000/10000000, Loss: 0.2055\n",
            "Epoch 295000/10000000, Loss: 0.2055\n",
            "Epoch 296000/10000000, Loss: 0.2055\n",
            "Epoch 297000/10000000, Loss: 0.2055\n",
            "Epoch 298000/10000000, Loss: 0.2055\n",
            "Epoch 299000/10000000, Loss: 0.2054\n",
            "Epoch 300000/10000000, Loss: 0.2054\n",
            "Epoch 301000/10000000, Loss: 0.2054\n",
            "Epoch 302000/10000000, Loss: 0.2054\n",
            "Epoch 303000/10000000, Loss: 0.2054\n",
            "Epoch 304000/10000000, Loss: 0.2054\n",
            "Epoch 305000/10000000, Loss: 0.2054\n",
            "Epoch 306000/10000000, Loss: 0.2053\n",
            "Epoch 307000/10000000, Loss: 0.2053\n",
            "Epoch 308000/10000000, Loss: 0.2053\n",
            "Epoch 309000/10000000, Loss: 0.2053\n",
            "Epoch 310000/10000000, Loss: 0.2053\n",
            "Epoch 311000/10000000, Loss: 0.2053\n",
            "Epoch 312000/10000000, Loss: 0.2053\n",
            "Epoch 313000/10000000, Loss: 0.2053\n",
            "Epoch 314000/10000000, Loss: 0.2052\n",
            "Epoch 315000/10000000, Loss: 0.2052\n",
            "Epoch 316000/10000000, Loss: 0.2052\n",
            "Epoch 317000/10000000, Loss: 0.2052\n",
            "Epoch 318000/10000000, Loss: 0.2052\n",
            "Epoch 319000/10000000, Loss: 0.2052\n",
            "Epoch 320000/10000000, Loss: 0.2052\n",
            "Epoch 321000/10000000, Loss: 0.2051\n",
            "Epoch 322000/10000000, Loss: 0.2051\n",
            "Epoch 323000/10000000, Loss: 0.2051\n",
            "Epoch 324000/10000000, Loss: 0.2051\n",
            "Epoch 325000/10000000, Loss: 0.2051\n",
            "Epoch 326000/10000000, Loss: 0.2051\n",
            "Epoch 327000/10000000, Loss: 0.2051\n",
            "Epoch 328000/10000000, Loss: 0.2051\n",
            "Epoch 329000/10000000, Loss: 0.2050\n",
            "Epoch 330000/10000000, Loss: 0.2050\n",
            "Epoch 331000/10000000, Loss: 0.2050\n",
            "Epoch 332000/10000000, Loss: 0.2050\n",
            "Epoch 333000/10000000, Loss: 0.2050\n",
            "Epoch 334000/10000000, Loss: 0.2050\n",
            "Epoch 335000/10000000, Loss: 0.2050\n",
            "Epoch 336000/10000000, Loss: 0.2050\n",
            "Epoch 337000/10000000, Loss: 0.2049\n",
            "Epoch 338000/10000000, Loss: 0.2049\n",
            "Epoch 339000/10000000, Loss: 0.2049\n",
            "Epoch 340000/10000000, Loss: 0.2049\n",
            "Epoch 341000/10000000, Loss: 0.2049\n",
            "Epoch 342000/10000000, Loss: 0.2049\n",
            "Epoch 343000/10000000, Loss: 0.2049\n",
            "Epoch 344000/10000000, Loss: 0.2048\n",
            "Epoch 345000/10000000, Loss: 0.2048\n",
            "Epoch 346000/10000000, Loss: 0.2048\n",
            "Epoch 347000/10000000, Loss: 0.2048\n",
            "Epoch 348000/10000000, Loss: 0.2048\n",
            "Epoch 349000/10000000, Loss: 0.2048\n",
            "Epoch 350000/10000000, Loss: 0.2048\n",
            "Epoch 351000/10000000, Loss: 0.2047\n",
            "Epoch 352000/10000000, Loss: 0.2047\n",
            "Epoch 353000/10000000, Loss: 0.2047\n",
            "Epoch 354000/10000000, Loss: 0.2047\n",
            "Epoch 355000/10000000, Loss: 0.2047\n",
            "Epoch 356000/10000000, Loss: 0.2047\n",
            "Epoch 357000/10000000, Loss: 0.2047\n",
            "Epoch 358000/10000000, Loss: 0.2047\n",
            "Epoch 359000/10000000, Loss: 0.2046\n",
            "Epoch 360000/10000000, Loss: 0.2046\n",
            "Epoch 361000/10000000, Loss: 0.2046\n",
            "Epoch 362000/10000000, Loss: 0.2046\n",
            "Epoch 363000/10000000, Loss: 0.2046\n",
            "Epoch 364000/10000000, Loss: 0.2046\n",
            "Epoch 365000/10000000, Loss: 0.2046\n",
            "Epoch 366000/10000000, Loss: 0.2046\n",
            "Epoch 367000/10000000, Loss: 0.2045\n",
            "Epoch 368000/10000000, Loss: 0.2045\n",
            "Epoch 369000/10000000, Loss: 0.2045\n",
            "Epoch 370000/10000000, Loss: 0.2045\n",
            "Epoch 371000/10000000, Loss: 0.2045\n",
            "Epoch 372000/10000000, Loss: 0.2045\n",
            "Epoch 373000/10000000, Loss: 0.2045\n",
            "Epoch 374000/10000000, Loss: 0.2045\n",
            "Epoch 375000/10000000, Loss: 0.2045\n",
            "Epoch 376000/10000000, Loss: 0.2044\n",
            "Epoch 377000/10000000, Loss: 0.2044\n",
            "Epoch 378000/10000000, Loss: 0.2044\n",
            "Epoch 379000/10000000, Loss: 0.2044\n",
            "Epoch 380000/10000000, Loss: 0.2044\n",
            "Epoch 381000/10000000, Loss: 0.2044\n",
            "Epoch 382000/10000000, Loss: 0.2044\n",
            "Epoch 383000/10000000, Loss: 0.2044\n",
            "Epoch 384000/10000000, Loss: 0.2044\n",
            "Epoch 385000/10000000, Loss: 0.2043\n",
            "Epoch 386000/10000000, Loss: 0.2043\n",
            "Epoch 387000/10000000, Loss: 0.2043\n",
            "Epoch 388000/10000000, Loss: 0.2043\n",
            "Epoch 389000/10000000, Loss: 0.2043\n",
            "Epoch 390000/10000000, Loss: 0.2043\n",
            "Epoch 391000/10000000, Loss: 0.2043\n",
            "Epoch 392000/10000000, Loss: 0.2043\n",
            "Epoch 393000/10000000, Loss: 0.2043\n",
            "Epoch 394000/10000000, Loss: 0.2043\n",
            "Epoch 395000/10000000, Loss: 0.2042\n",
            "Epoch 396000/10000000, Loss: 0.2042\n",
            "Epoch 397000/10000000, Loss: 0.2042\n",
            "Epoch 398000/10000000, Loss: 0.2042\n",
            "Epoch 399000/10000000, Loss: 0.2042\n",
            "Epoch 400000/10000000, Loss: 0.2042\n",
            "Epoch 401000/10000000, Loss: 0.2042\n",
            "Epoch 402000/10000000, Loss: 0.2042\n",
            "Epoch 403000/10000000, Loss: 0.2042\n",
            "Epoch 404000/10000000, Loss: 0.2042\n",
            "Epoch 405000/10000000, Loss: 0.2042\n",
            "Epoch 406000/10000000, Loss: 0.2041\n",
            "Epoch 407000/10000000, Loss: 0.2041\n",
            "Epoch 408000/10000000, Loss: 0.2041\n",
            "Epoch 409000/10000000, Loss: 0.2041\n",
            "Epoch 410000/10000000, Loss: 0.2041\n",
            "Epoch 411000/10000000, Loss: 0.2041\n",
            "Epoch 412000/10000000, Loss: 0.2041\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-71ba26645f21>\u001b[0m in \u001b[0;36m<cell line: 121>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Train the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-71ba26645f21>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def X_activation(x):\n",
        "    return x\n",
        "\n",
        "def X_activation_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "\n",
        "# Define the custom activation functions and their derivatives\n",
        "def X_2_activation(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "def X_2_activation_derivative(x):\n",
        "    return (x / 4)\n",
        "\n",
        "def X_3_activation(x):\n",
        "    return (x**3) / 24\n",
        "\n",
        "def X_3_activation_derivative(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, output_size):\n",
        "        # Define the architecture\n",
        "        self.input_size = input_size\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "        self.hidden3_size = hidden3_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = {\n",
        "            'W1': np.random.randn(input_size, hidden1_size),\n",
        "            'W2': np.random.randn(input_size, hidden2_size),\n",
        "            'W3': np.random.randn(input_size, hidden3_size),\n",
        "            'W4': np.random.randn(hidden1_size + hidden2_size + hidden3_size, output_size)\n",
        "        }\n",
        "\n",
        "        self.biases = {\n",
        "            'b1': np.random.randn(1, hidden1_size),\n",
        "            'b2': np.random.randn(1, hidden2_size),\n",
        "            'b3': np.random.randn(1, hidden3_size),\n",
        "            'b4': np.random.randn(1, output_size)\n",
        "        }\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "\n",
        "        # Define the X activation function and its derivative\n",
        "    def X_activation(self, x):\n",
        "        return X_activation(x)\n",
        "\n",
        "    def X_activation_derivative(self, x):\n",
        "        return X_activation_derivative(x)\n",
        "\n",
        "    def X_2_activation(self, x):\n",
        "        return X_2_activation(x)\n",
        "\n",
        "    def X_2_activation_derivative(self, x):\n",
        "        return X_2_activation_derivative(x)\n",
        "\n",
        "    def X_3_activation(self, x):\n",
        "        return X_3_activation(x)\n",
        "\n",
        "    def X_3_activation_derivative(self, x):\n",
        "        return X_3_activation_derivative(x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "    # Layer 1 (X Activation)\n",
        "        self.z1 = np.dot(X, self.weights['W1']) + self.biases['b1']\n",
        "        self.a1 = X_activation(self.z1)  # Use X activation for layer\n",
        "\n",
        "        # Layer 2\n",
        "        self.z2 = np.dot(X, self.weights['W2']) + self.biases['b2']\n",
        "        self.a2 = X_2_activation(self.z2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.z3 = np.dot(X, self.weights['W3']) + self.biases['b3']\n",
        "        self.a3 = X_3_activation(self.z3)\n",
        "\n",
        "        # Concatenate the outputs of layers 1, 2, and 3\n",
        "        self.concatenated_output = np.concatenate((self.a1, self.a2, self.a3), axis=1)\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        self.z4 = np.dot(self.concatenated_output, self.weights['W4']) + self.biases['b4']\n",
        "        self.output = X_activation(self.z4)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Backpropagation\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        delta4 = 2 * (self.output - y) * self.X_activation_derivative(self.output)\n",
        "        dW4 = np.dot(self.concatenated_output.T, delta4)\n",
        "        db4 = np.sum(delta4, axis=0, keepdims=True)\n",
        "\n",
        "        # Split the delta for the concatenation in layer 4\n",
        "        delta4_split = np.dot(delta4, self.weights['W4'].T)\n",
        "\n",
        "        # Separate the deltas for layers 1, 2, and 3\n",
        "        delta3 = delta4_split[:, -self.hidden3_size:] * self.X_3_activation_derivative(self.a3)\n",
        "\n",
        "        delta2 = delta4_split[:, -self.hidden3_size-self.hidden2_size:-self.hidden3_size] * self.X_2_activation_derivative(self.a2)\n",
        "        delta1 = delta4_split[:, :-self.hidden3_size-self.hidden2_size] * self.X_activation_derivative(self.a1)\n",
        "\n",
        "        # Calculate gradients for layers 1, 2, and 3\n",
        "        dW3 = np.dot(X.T, delta3)\n",
        "        db3 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        dW2 = np.dot(X.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights['W4'] -= learning_rate * dW4\n",
        "        self.biases['b4'] -= learning_rate * db4\n",
        "        self.weights['W3'] -= learning_rate * dW3\n",
        "        self.biases['b3'] -= learning_rate * db3\n",
        "        self.weights['W2'] -= learning_rate * dW2\n",
        "        self.biases['b2'] -= learning_rate * db2\n",
        "        self.weights['W1'] -= learning_rate * dW1\n",
        "        self.biases['b1'] -= learning_rate * db1\n",
        "\n",
        "        return dW4, db4, dW3, db3, dW2, db2, dW1, db1\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            self.feedforward(X)\n",
        "\n",
        "            # Backpropagation\n",
        "            self.backpropagation(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 500 == 0:\n",
        "                loss = np.mean((self.output - y) ** 2)\n",
        "                print(f'Epoch {epoch}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "# Example usage:\n",
        "input_size = 1\n",
        "hidden1_size = 4\n",
        "hidden2_size = 3\n",
        "hidden3_size = 2\n",
        "output_size = 1\n",
        "learning_rate = 0.000001\n",
        "epochs = 20\n",
        "\n",
        "# Generate random input data and corresponding target values\n",
        "np.random.seed(0)\n",
        "# Generate random input data X between 0 and 1\n",
        "X = np.random.rand(1000)\n",
        "X=X.reshape((1000,1))\n",
        "# Calculate y as the sigmoid of X\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "y = sigmoid(X)\n",
        "\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size, hidden3_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "nn.train(X, y, learning_rate, epochs)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsvsZn7OPB77",
        "outputId": "e7230b1b-6d83-4d3a-b5c9-6e9a8c29ab13"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/20, Loss: 0.8188\n",
            "(1000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X, y, label='Actual Data', s=5)\n",
        "plt.scatter(X, nn.output, label='Predicted Data', s=5)\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "q2l_ns4YZ8ID",
        "outputId": "beb34492-6603-41eb-a830-30943499d3ee"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load Iris dataset and preprocess\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "learning_rate = 0.00001\n",
        "epochs = 2500\n",
        "\n",
        "\n",
        "input_size = 4\n",
        "hidden1_size = 8\n",
        "hidden2_size = 6\n",
        "hidden3_size = 4\n",
        "output_size = 3\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size,hidden3_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "start_time = time.time()\n",
        "\n",
        "nn.train(X_train, y_train, learning_rate, epochs)\n",
        "\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training Execution Time: {execution_time:.2f} seconds\")\n",
        "# Evaluate the trained model\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
        "\n",
        "y_pred = nn.feedforward(X_test)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "nxfkaiQm63is",
        "outputId": "b6ebc059-b2b0-46f0-cf9b-9d25d2651d8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/2500, Loss: 54.0207\n",
            "Epoch 500/2500, Loss: 0.2556\n",
            "Epoch 1000/2500, Loss: 0.1499\n",
            "Epoch 1500/2500, Loss: 0.1114\n",
            "Epoch 2000/2500, Loss: 0.0909\n",
            "Training Execution Time: 1.22 seconds\n",
            "Test Accuracy: 96.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import time\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "y_onehot = tf.keras.utils.to_categorical(y_encoded, num_classes=3)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=4, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Measure execution time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=1, verbose=0)\n",
        "\n",
        "# Calculate execution time\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Execution Time: {execution_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3w1TDhT0kql",
        "outputId": "c56fdfbb-ff81-4c84-b253-0e5c47e82d6e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 159ms/step - loss: 0.0755 - accuracy: 1.0000\n",
            "Test Loss: 0.0755, Test Accuracy: 1.0000\n",
            "Execution Time: 4.15 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5MnwJYU64LL-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}