{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg6gXevSj8CJKTWNZW9mvM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningSaeid/Grad/blob/main/Pure_implimentation_SWAG_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def X_activation(x):\n",
        "    return x\n",
        "\n",
        "def X_activation_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def X_2_activation(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "def X_2_activation_derivative(x):\n",
        "    return (x / 4)\n",
        "\n",
        "def X_3_activation(x):\n",
        "    return (x**3) / 24\n",
        "\n",
        "def X_3_activation_derivative(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, output_size):\n",
        "        # Define the architecture\n",
        "        self.input_size = input_size\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "        self.hidden3_size = hidden3_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = {\n",
        "            'W1': np.random.randn(input_size, hidden1_size),\n",
        "            'W2': np.random.randn(input_size, hidden2_size),\n",
        "            'W3': np.random.randn(input_size, hidden3_size),\n",
        "            'W4': np.random.randn(hidden1_size + hidden2_size + hidden3_size, output_size)\n",
        "        }\n",
        "\n",
        "        self.biases = {\n",
        "            'b1': np.random.randn(1, hidden1_size),\n",
        "            'b2': np.random.randn(1, hidden2_size),\n",
        "            'b3': np.random.randn(1, hidden3_size),\n",
        "            'b4': np.random.randn(1, output_size)\n",
        "        }\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def X_activation(self, x):\n",
        "        return X_activation(x)\n",
        "\n",
        "    def X_activation_derivative(self, x):\n",
        "        return X_activation_derivative(x)\n",
        "\n",
        "    def X_2_activation(self, x):\n",
        "        return X_2_activation(x)\n",
        "\n",
        "    def X_2_activation_derivative(self, x):\n",
        "        return X_2_activation_derivative(x)\n",
        "\n",
        "    def X_3_activation(self, x):\n",
        "        return X_3_activation(x)\n",
        "\n",
        "    def X_3_activation_derivative(self, x):\n",
        "        return X_3_activation_derivative(x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Layer 1 (X Activation)\n",
        "        self.z1 = np.dot(X, self.weights['W1']) + self.biases['b1']\n",
        "        self.a1 = self.X_activation(self.z1)  # Use X activation for layer\n",
        "\n",
        "        # Layer 2\n",
        "        self.z2 = np.dot(X, self.weights['W2']) + self.biases['b2']\n",
        "        self.a2 = self.X_2_activation(self.z2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.z3 = np.dot(X, self.weights['W3']) + self.biases['b3']\n",
        "        self.a3 = self.X_3_activation(self.z3)\n",
        "\n",
        "        # Concatenate the outputs of layers 1, 2, and 3\n",
        "        self.concatenated_output = np.concatenate((self.a1, self.a2, self.a3), axis=1)\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        self.z4 = np.dot(self.concatenated_output, self.weights['W4']) + self.biases['b4']\n",
        "        self.output = self.X_activation(self.z4)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Backpropagation\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        delta4 = 2 * (self.output - y) * self.X_activation_derivative(self.output)\n",
        "        dW4 = np.dot(self.concatenated_output.T, delta4)\n",
        "        db4 = np.sum(delta4, axis=0, keepdims=True)\n",
        "\n",
        "        # Split the delta for the concatenation in layer 4\n",
        "        delta4_split = np.dot(delta4, self.weights['W4'].T)\n",
        "\n",
        "        # Separate the deltas for layers 1, 2, and 3\n",
        "        delta3 = delta4_split[:, -self.hidden3_size:] * self.X_3_activation_derivative(self.a3)\n",
        "\n",
        "        delta2 = delta4_split[:, -self.hidden3_size-self.hidden2_size:-self.hidden3_size] * self.X_2_activation_derivative(self.a2)\n",
        "        delta1 = delta4_split[:, :-self.hidden3_size-self.hidden2_size] * self.X_activation_derivative(self.a1)\n",
        "\n",
        "        # Calculate gradients for layers 1, 2, and 3\n",
        "        dW3 = np.dot(X.T, delta3)\n",
        "        db3 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        dW2 = np.dot(X.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights['W4'] -= learning_rate * dW4\n",
        "        self.biases['b4'] -= learning_rate * db4\n",
        "        self.weights['W3'] -= learning_rate * dW3\n",
        "        self.biases['b3'] -= learning_rate * db3\n",
        "        self.weights['W2'] -= learning_rate * dW2\n",
        "        self.biases['b2'] -= learning_rate * db2\n",
        "        self.weights['W1'] -= learning_rate * dW1\n",
        "        self.biases['b1'] -= learning_rate * db1\n",
        "\n",
        "        return dW4, db4, dW3, db3, dW2, db2, dW1, db1\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            self.feedforward(X)\n",
        "\n",
        "            # Backpropagation\n",
        "            self.backpropagation(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 500 == 0:\n",
        "                loss = np.mean((self.output - y) ** 2)\n",
        "                print(f'Epoch {epoch}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "##########################################################\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import time\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Add a small epsilon value to avoid exactly zero or one values\n",
        "epsilon = 1e-4\n",
        "X = np.clip(X, epsilon, 1 - epsilon)  # Clip values to be in the range (epsilon, 1 - epsilon)\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "learning_rate = 0.00001\n",
        "epochs = 3000\n",
        "\n",
        "\n",
        "input_size = 4\n",
        "hidden1_size = 8\n",
        "hidden2_size = 8\n",
        "hidden3_size = 8\n",
        "output_size = 3\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size,hidden3_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "start_time = time.time()\n",
        "\n",
        "nn.train(X_train, y_train, learning_rate, epochs)\n",
        "\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training Execution Time: {execution_time:.2f} seconds\")\n",
        "# Evaluate the trained model\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
        "\n",
        "y_pred = nn.feedforward(X_test)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "WUK1OWKhB-WJ",
        "outputId": "1ccd3d6b-7ddf-4b4d-d4df-12ceefeb6bfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/3000, Loss: 18.2705\n",
            "Epoch 500/3000, Loss: 0.2485\n",
            "Epoch 1000/3000, Loss: 0.1756\n",
            "Epoch 1500/3000, Loss: 0.1599\n",
            "Epoch 2000/3000, Loss: 0.1498\n",
            "Epoch 2500/3000, Loss: 0.1420\n",
            "Training Execution Time: 1.45 seconds\n",
            "Test Accuracy: 73.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def X_activation(x):\n",
        "    return x\n",
        "\n",
        "def X_activation_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def X_2_activation(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "def X_2_activation_derivative(x):\n",
        "    return (x / 4)\n",
        "\n",
        "def X_3_activation(x):\n",
        "    return (x**3) / 24\n",
        "\n",
        "def X_3_activation_derivative(x):\n",
        "    return (x**2) / 8\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, output_size):\n",
        "        # Define the architecture\n",
        "        self.input_size = input_size\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "        self.hidden3_size = hidden3_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = {\n",
        "            'W1': np.random.randn(input_size, hidden1_size),\n",
        "            'W2': np.random.randn(input_size, hidden2_size),\n",
        "            'W3': np.random.randn(input_size, hidden3_size),\n",
        "            'W4': np.random.randn(hidden1_size + hidden2_size + hidden3_size, output_size)\n",
        "        }\n",
        "\n",
        "        self.biases = {\n",
        "            'b1': np.random.randn(1, hidden1_size),\n",
        "            'b2': np.random.randn(1, hidden2_size),\n",
        "            'b3': np.random.randn(1, hidden3_size),\n",
        "            'b4': np.random.randn(1, output_size)\n",
        "        }\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def X_activation(self, x):\n",
        "        return X_activation(x)\n",
        "\n",
        "    def X_activation_derivative(self, x):\n",
        "        return X_activation_derivative(x)\n",
        "\n",
        "    def X_2_activation(self, x):\n",
        "        return X_2_activation(x)\n",
        "\n",
        "    def X_2_activation_derivative(self, x):\n",
        "        return X_2_activation_derivative(x)\n",
        "\n",
        "    def X_3_activation(self, x):\n",
        "        return X_3_activation(x)\n",
        "\n",
        "    def X_3_activation_derivative(self, x):\n",
        "        return X_3_activation_derivative(x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Layer 1 (X Activation)\n",
        "        self.z1 = np.dot(X, self.weights['W1']) + self.biases['b1']\n",
        "        self.a1 = self.X_activation(self.z1)  # Use X activation for layer\n",
        "\n",
        "        # Layer 2\n",
        "        self.z2 = np.dot(X, self.weights['W2']) + self.biases['b2']\n",
        "        self.a2 = self.X_2_activation(self.z2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.z3 = np.dot(X, self.weights['W3']) + self.biases['b3']\n",
        "        self.a3 = self.X_3_activation(self.z3)\n",
        "\n",
        "        # Concatenate the outputs of layers 1, 2, and 3\n",
        "        self.concatenated_output = np.concatenate((self.a1, self.a2, self.a3), axis=1)\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        self.z4 = np.dot(self.concatenated_output, self.weights['W4']) + self.biases['b4']\n",
        "        self.output = self.X_activation(self.z4)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Backpropagation\n",
        "\n",
        "        # Layer 4 (Output Layer)\n",
        "        delta4 = 2 * (self.output - y) * self.X_activation_derivative(self.output)\n",
        "        dW4 = np.dot(self.concatenated_output.T, delta4)\n",
        "        db4 = np.sum(delta4, axis=0, keepdims=True)\n",
        "\n",
        "        # Split the delta for the concatenation in layer 4\n",
        "        delta4_split = np.dot(delta4, self.weights['W4'].T)\n",
        "\n",
        "        # Separate the deltas for layers 1, 2, and 3\n",
        "        delta3 = delta4_split[:, -self.hidden3_size:] * self.X_3_activation_derivative(self.a3)\n",
        "\n",
        "        delta2 = delta4_split[:, -self.hidden3_size-self.hidden2_size:-self.hidden3_size] * self.X_2_activation_derivative(self.a2)\n",
        "        delta1 = delta4_split[:, :-self.hidden3_size-self.hidden2_size] * self.X_activation_derivative(self.a1)\n",
        "\n",
        "        # Calculate gradients for layers 1, 2, and 3\n",
        "        dW3 = np.dot(X.T, delta3)\n",
        "        db3 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        dW2 = np.dot(X.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights['W4'] -= learning_rate * dW4\n",
        "        self.biases['b4'] -= learning_rate * db4\n",
        "        self.weights['W3'] -= learning_rate * dW3\n",
        "        self.biases['b3'] -= learning_rate * db3\n",
        "        self.weights['W2'] -= learning_rate * dW2\n",
        "        self.biases['b2'] -= learning_rate * db2\n",
        "        self.weights['W1'] -= learning_rate * dW1\n",
        "        self.biases['b1'] -= learning_rate * db1\n",
        "\n",
        "        return dW4, db4, dW3, db3, dW2, db2, dW1, db1\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        batch_size = 10\n",
        "        n_batches = len(X) // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0  # Reset epoch loss\n",
        "\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                X_batch = X[i:i + batch_size]\n",
        "                y_batch = y[i:i + batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                self.feedforward(X_batch)\n",
        "\n",
        "                # Backpropagation\n",
        "                self.backpropagation(X_batch, y_batch, learning_rate)\n",
        "\n",
        "                # Calculate batch loss and add it to epoch loss\n",
        "                batch_loss = np.mean((self.output - y_batch) ** 2)\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            # Calculate average epoch loss\n",
        "            epoch_loss /= n_batches\n",
        "\n",
        "            if epoch % 500 == 0:\n",
        "                print(f'Epoch {epoch}/{epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "##########################################################\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import time\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Add a small epsilon value to avoid exactly zero or one values\n",
        "epsilon = 1e-4\n",
        "X = np.clip(X, epsilon, 1 - epsilon)  # Clip values to be in the range (epsilon, 1 - epsilon)\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "learning_rate = 0.0001\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "input_size = 4\n",
        "hidden1_size = 8\n",
        "hidden2_size = 8\n",
        "hidden3_size = 8\n",
        "output_size = 3\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size,hidden3_size, output_size)\n",
        "\n",
        "# Train the neural network\n",
        "start_time = time.time()\n",
        "\n",
        "nn.train(X_train, y_train, learning_rate, epochs)\n",
        "\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training Execution Time: {execution_time:.2f} seconds\")\n",
        "# Evaluate the trained model\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
        "\n",
        "y_pred = nn.feedforward(X_test)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f'Test Accuracy: {acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "49mz28zO1ee4",
        "outputId": "43c27909-efea-49e4-bd72-d1a01d899505",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/100, Loss: 11.0058\n",
            "Training Execution Time: 0.19 seconds\n",
            "Test Accuracy: 70.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b3': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square(Z3)\n",
        "\n",
        "    return A3, (X, Z1, A1, Z2, A2, Z3, A3, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated = cache\n",
        "\n",
        "    dA3 = 2 * (A3 - y_true)\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated = np.dot(dZ3, params['W3'].T)\n",
        "    dA1, dA2 = d_concatenated[:, :hidden_size1], d_concatenated[:, hidden_size1:]\n",
        "\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = dA2 * square_derivative(Z2)\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]  # Convert to one-hot encoding\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "# Convert predictions to one-hot encoded format\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "\n",
        "# Compute accuracy using one-hot encoded predictions\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "lkkAcYvnRl4V",
        "outputId": "113f8997-9503-4135-9e88-b52f3d4250ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3331\n",
            "Epoch 20, Loss: 0.2046\n",
            "Epoch 40, Loss: 0.0684\n",
            "Epoch 60, Loss: 0.0364\n",
            "Epoch 80, Loss: 0.0379\n",
            "Epoch 100, Loss: 0.0347\n",
            "Epoch 120, Loss: 0.0330\n",
            "Epoch 140, Loss: 0.0318\n",
            "Epoch 160, Loss: 0.0309\n",
            "Epoch 180, Loss: 0.0303\n",
            "Test Loss: 0.0728\n",
            "Test Accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square(Z3)\n",
        "\n",
        "\n",
        "    Z4 = np.dot(A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    dA3 = np.dot(dZ4, params['W4'].T)\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated = np.dot(dZ3, params['W3'].T)\n",
        "    dA1, dA2 = d_concatenated[:, :hidden_size1], d_concatenated[:, hidden_size1:]\n",
        "\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = dA2 * square_derivative(Z2)\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]  # Convert to one-hot encoding\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "# Convert predictions to one-hot encoded format\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "\n",
        "# Compute accuracy using one-hot encoded predictions\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "ct7XnKWhcBTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c1d7ef-e882-4f12-d87c-b52669a81ff8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3337\n",
            "Epoch 20, Loss: 0.2220\n",
            "Epoch 40, Loss: 0.2210\n",
            "Epoch 60, Loss: 0.2001\n",
            "Epoch 80, Loss: 0.1148\n",
            "Epoch 100, Loss: 0.1125\n",
            "Epoch 120, Loss: 0.1042\n",
            "Epoch 140, Loss: 0.0653\n",
            "Epoch 160, Loss: 0.0712\n",
            "Epoch 180, Loss: 0.0715\n",
            "Test Loss: 0.0688\n",
            "Test Accuracy: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3, output_size) * 0.1,  # Corrected output size\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)  # Concatenate A3 and concatenated\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)  # Update using concatenated A3\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "    d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_Z2 = dA2 * square_derivative(Z2)\n",
        "    dA1, dA2 = d_concatenated[:, :hidden_size1], d_concatenated[:, hidden_size1:]\n",
        "\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = d_concatenated_Z2\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]  # Convert to one-hot encoding\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "# Convert predictions to one-hot encoded format\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "\n",
        "# Compute accuracy using one-hot encoded predictions\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "UGVaHyh_eS4r",
        "outputId": "0768b12b-9fab-4ad8-9c5e-6273ce51af74"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (120,15) and (5,3) not aligned: 15 (dim 1) != 5 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-866342005de0>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-866342005de0>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(X, params)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mconcatenated_A3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Concatenate A3 and concatenated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mZ4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated_A3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mA4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Linear activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (120,15) and (5,3) not aligned: 15 (dim 1) != 5 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "    d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_Z2 = dA2 * square_derivative(Z2)\n",
        "    dA1, dA2 = d_concatenated[:, :hidden_size1], d_concatenated[:, hidden_size1:]\n",
        "\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = d_concatenated_Z2[:, hidden_size1:]  # Update dZ2 calculation\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "kBeosOoOhGoy",
        "outputId": "5da7fad2-700d-4b0b-e747-2c13593da487"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'dA2' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-7bc2605f96a5>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-7bc2605f96a5>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(y_true, cache, params)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mdb3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0md_concatenated_Z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msquare_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mdA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_concatenated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mhidden_size1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_concatenated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'dA2' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)/4\n",
        "\n",
        "def square_(x):\n",
        "    return np.power(x, 2)/24\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square_(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "    d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = d_concatenated[:, hidden_size1:]  # Corrected dA2 calculation\n",
        "    d_concatenated_Z2 = dA2 * square_derivative(Z2)\n",
        "\n",
        "    dA1 = d_concatenated[:, :hidden_size1]\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = d_concatenated_Z2\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_onehot = np.eye(3)[y]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 5\n",
        "hidden_size2 = 5\n",
        "hidden_size3 = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "predictions = np.argmax(output_test, axis=1)\n",
        "predictions_onehot = np.eye(output_size)[predictions]\n",
        "accuracy = np.mean(np.all(predictions_onehot == Y_test, axis=1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxIbYV68iOcK",
        "outputId": "7563d00c-9061-483c-b49c-7b3e0a471d4a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3355\n",
            "Epoch 20, Loss: 0.1032\n",
            "Epoch 40, Loss: 0.0946\n",
            "Epoch 60, Loss: 0.0688\n",
            "Epoch 80, Loss: 0.0483\n",
            "Epoch 100, Loss: 0.0393\n",
            "Epoch 120, Loss: 0.0355\n",
            "Epoch 140, Loss: 0.0335\n",
            "Epoch 160, Loss: 0.0317\n",
            "Epoch 180, Loss: 0.0355\n",
            "Test Loss: 0.0398\n",
            "Test Accuracy: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(input_size, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(X, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)  # Derivative of linear activation is 1\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :hidden_size3]\n",
        "    d_concatenated = d_concatenated_A3[:, hidden_size3:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = d_concatenated[:, hidden_size1:]  # Corrected dA2 calculation\n",
        "    d_concatenated_Z2 = dA2 * square_derivative(Z2)\n",
        "\n",
        "    dA1 = d_concatenated[:, :hidden_size1]\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = d_concatenated_Z2\n",
        "    dW2 = np.dot(X.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y = y.reshape(-1, 1)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 10\n",
        "hidden_size2 = 10\n",
        "hidden_size3 = 10\n",
        "output_size = 1\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sOG4uyvif0A",
        "outputId": "333313d2-e24f-42e8-814a-39328a09c944"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 29709.8532\n",
            "Epoch 20, Loss: nan\n",
            "Epoch 40, Loss: nan\n",
            "Epoch 60, Loss: nan\n",
            "Epoch 80, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss: nan\n",
            "Epoch 120, Loss: nan\n",
            "Epoch 140, Loss: nan\n",
            "Epoch 160, Loss: nan\n",
            "Epoch 180, Loss: nan\n",
            "Test Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2)/4\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return  x/2\n",
        "\n",
        "\n",
        "def square_2(x):\n",
        "    return np.power(x, 2)/24\n",
        "\n",
        "def square_2_derivative(x):\n",
        "    return  x/12\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(hidden_size1, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(A1, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square_2(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :params['W3'].shape[1]]\n",
        "    d_concatenated = d_concatenated_A3[:, params['W3'].shape[1]:]\n",
        "\n",
        "    dZ3 = dA3 * square_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = d_concatenated[:, hidden_size1:]  # Corrected dA2 calculation\n",
        "    dZ2 = dA2 * square_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    dA1 = d_concatenated[:, :hidden_size1]\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y = y.reshape(-1, 1)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 10\n",
        "hidden_size2 = 10\n",
        "hidden_size3 = 10\n",
        "output_size = 1\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcSSARqwpVSe",
        "outputId": "dcf5e14c-b178-4505-a77c-3c78e7187216"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 29721.1327\n",
            "Epoch 20, Loss: nan\n",
            "Epoch 40, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-14750e32b4c0>:11: RuntimeWarning: overflow encountered in power\n",
            "  return np.power(x, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60, Loss: nan\n",
            "Epoch 80, Loss: nan\n",
            "Epoch 100, Loss: nan\n",
            "Epoch 120, Loss: nan\n",
            "Epoch 140, Loss: nan\n",
            "Epoch 160, Loss: nan\n",
            "Epoch 180, Loss: nan\n",
            "Test Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2) / 4\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return x / 2\n",
        "\n",
        "def square_2(x):\n",
        "    return np.power(x, 2) / 24\n",
        "\n",
        "def square_2_derivative(x):\n",
        "    return x / 12\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(hidden_size1, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(A1, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square_2(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params, hidden_size1, hidden_size2):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :params['W3'].shape[1]]\n",
        "    d_concatenated = d_concatenated_A3[:, params['W3'].shape[1]:]\n",
        "\n",
        "    dZ3 = dA3 * square_2_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = d_concatenated[:, hidden_size1:]\n",
        "    dZ2 = dA2 * square_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    dA1 = d_concatenated[:, :hidden_size1]\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y = y.reshape(-1, 1)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 10\n",
        "hidden_size2 = 10\n",
        "hidden_size3 = 10\n",
        "output_size = 1\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params, hidden_size1, hidden_size2)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd7SiD6btVv-",
        "outputId": "444d6109-be64-4c9c-a37d-b44d4cb994b6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 29728.5256\n",
            "Epoch 20, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-66be0136e3cf>:11: RuntimeWarning: overflow encountered in power\n",
            "  return np.power(x, 2) / 4\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Loss: nan\n",
            "Epoch 60, Loss: nan\n",
            "Epoch 80, Loss: nan\n",
            "Epoch 100, Loss: nan\n",
            "Epoch 120, Loss: nan\n",
            "Epoch 140, Loss: nan\n",
            "Epoch 160, Loss: nan\n",
            "Epoch 180, Loss: nan\n",
            "Test Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x/2\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2) / 4\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return x / 2\n",
        "\n",
        "def square_2(x):\n",
        "    return np.power(x, 2) / 24\n",
        "\n",
        "def square_2_derivative(x):\n",
        "    return x / 12\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * 0.1,\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(hidden_size1, hidden_size2) * 0.1,\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * 0.1,\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * 0.1,\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(A1, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square_2(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)  # Linear activation\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params, hidden_size1, hidden_size2):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :params['W3'].shape[1]]\n",
        "    d_concatenated = d_concatenated_A3[:, params['W3'].shape[1]:]\n",
        "\n",
        "    dZ3 = dA3 * square_2_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = d_concatenated[:, hidden_size1:]\n",
        "    dZ2 = dA2 * square_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    dA1 = d_concatenated[:, :hidden_size1]\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Apply MinMaxScaler to transform X to be in the range [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y = y.reshape(-1, 1)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 10\n",
        "hidden_size2 = 10\n",
        "hidden_size3 = 10\n",
        "output_size = 1\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params, hidden_size1, hidden_size2)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2hvtBbxuFkX",
        "outputId": "1705280a-2063-4051-cbb3-2a14f60afde6"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 29711.0636\n",
            "Epoch 20, Loss: nan\n",
            "Epoch 40, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-006922dac669>:20: RuntimeWarning: overflow encountered in power\n",
            "  return np.power(x, 2) / 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60, Loss: nan\n",
            "Epoch 80, Loss: nan\n",
            "Epoch 100, Loss: nan\n",
            "Epoch 120, Loss: nan\n",
            "Epoch 140, Loss: nan\n",
            "Epoch 160, Loss: nan\n",
            "Epoch 180, Loss: nan\n",
            "Test Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Activation functions\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def square(x):\n",
        "    return np.power(x, 2) / 4\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def square_derivative(x):\n",
        "    return x / 2\n",
        "\n",
        "def square_2(x):\n",
        "    return np.power(x, 2) / 24\n",
        "\n",
        "def square_2_derivative(x):\n",
        "    return x / 12\n",
        "\n",
        "# Initialize network parameters\n",
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    return {\n",
        "        'W1': np.random.randn(input_size, hidden_size1) * np.sqrt(2. / input_size),\n",
        "        'b1': np.zeros((1, hidden_size1)),\n",
        "        'W2': np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2. / hidden_size1),\n",
        "        'b2': np.zeros((1, hidden_size2)),\n",
        "        'W3': np.random.randn(hidden_size1 + hidden_size2, hidden_size3) * np.sqrt(2. / (hidden_size1 + hidden_size2)),\n",
        "        'b3': np.zeros((1, hidden_size3)),\n",
        "        'W4': np.random.randn(hidden_size3 + hidden_size1 + hidden_size2, output_size) * np.sqrt(2. / (hidden_size3 + hidden_size1 + hidden_size2)),\n",
        "        'b4': np.zeros((1, output_size))\n",
        "    }\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = identity(Z1)\n",
        "\n",
        "    Z2 = np.dot(A1, params['W2']) + params['b2']\n",
        "    A2 = square(Z2)\n",
        "\n",
        "    concatenated = np.concatenate((A1, A2), axis=1)\n",
        "\n",
        "    Z3 = np.dot(concatenated, params['W3']) + params['b3']\n",
        "    A3 = square_2(Z3)\n",
        "\n",
        "    concatenated_A3 = np.concatenate((A3, concatenated), axis=1)\n",
        "\n",
        "    Z4 = np.dot(concatenated_A3, params['W4']) + params['b4']\n",
        "    A4 = identity(Z4)\n",
        "\n",
        "    return A4, (X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated)\n",
        "\n",
        "# Compute loss (Mean Squared Error)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(y_true, cache, params, hidden_size1, hidden_size2):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3, concatenated_A3, Z4, A4, concatenated = cache\n",
        "\n",
        "    dA4 = 2 * (A4 - y_true)\n",
        "    dZ4 = dA4 * identity_derivative(Z4)\n",
        "    dW4 = np.dot(concatenated_A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    d_concatenated_A3 = np.dot(dZ4, params['W4'].T)\n",
        "    dA3 = d_concatenated_A3[:, :params['W3'].shape[1]]\n",
        "    d_concatenated = d_concatenated_A3[:, params['W3'].shape[1]:]\n",
        "\n",
        "    dZ3 = dA3 * square_2_derivative(Z3)\n",
        "    dW3 = np.dot(concatenated.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = d_concatenated[:, hidden_size1:]\n",
        "    dZ2 = dA2 * square_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    dA1 = d_concatenated[:, :hidden_size1]\n",
        "    dZ1 = dA1 * identity_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dW3': dW3, 'db3': db3, 'dW4': dW4, 'db4': db4}\n",
        "    return grads\n",
        "\n",
        "# Update network parameters\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "    for key in params.keys():\n",
        "        params[key] -= learning_rate * grads['d' + key]\n",
        "    return params\n",
        "\n",
        "# Load and preprocess the Diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Apply MinMaxScaler to transform X to be in the range [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y = y.reshape(-1, 1)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 10\n",
        "hidden_size2 = 10\n",
        "hidden_size3 = 10\n",
        "output_size = 1\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
        "\n",
        "# Training settings\n",
        "epochs = 200\n",
        "learning_rate = 0.00001  # Adjusted learning rate\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    output, cache = forward_pass(X_train, params)\n",
        "    loss = compute_loss(Y_train, output)\n",
        "    grads = backward_pass(Y_train, cache, params, hidden_size1, hidden_size2)\n",
        "    params = update_parameters(params, grads, learning_rate)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "output_test, _ = forward_pass(X_test, params)\n",
        "test_loss = compute_loss(Y_test, output_test)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2YAssEPujBU",
        "outputId": "5116e78e-1e76-41d8-8f70-d21175915b19"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 29889.3644\n",
            "Epoch 20, Loss: nan\n",
            "Epoch 40, Loss: nan\n",
            "Epoch 60, Loss: nan\n",
            "Epoch 80, Loss: nan\n",
            "Epoch 100, Loss: nan\n",
            "Epoch 120, Loss: nan\n",
            "Epoch 140, Loss: nan\n",
            "Epoch 160, Loss: nan\n",
            "Epoch 180, Loss: nan\n",
            "Test Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Concatenate, Lambda\n",
        "\n",
        "# Custom activation function using TensorFlow operations\n",
        "def custom_square_activation(x):\n",
        "    return tf.math.pow(x, 2) / 4\n",
        "\n",
        "def custom_square_2_activation(x):\n",
        "    return tf.math.pow(x, 2) / 24\n",
        "\n",
        "# Load Iris dataset\n",
        "iris_data = load_iris()\n",
        "X, y = iris_data.data, iris_data.target.reshape(-1, 1)\n",
        "\n",
        "# Preprocess the data: Normalize X\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define the input size and hidden sizes\n",
        "input_size = X_scaled.shape[1]\n",
        "hidden_size1 = 10\n",
        "hidden_size2 = 10\n",
        "hidden_size3 = 10\n",
        "output_size = 1  # For regression, it's 1; for classification, it should match the number of classes\n",
        "\n",
        "# Define the model architecture\n",
        "input_layer = Input(shape=(input_size,))\n",
        "\n",
        "hidden_layer_1 = Dense(hidden_size1, activation='linear')(input_layer)\n",
        "hidden_layer_2 = Dense(hidden_size2)(input_layer)\n",
        "hidden_layer_2_activated = Lambda(custom_square_activation)(hidden_layer_2)\n",
        "\n",
        "concatenated_layer_1 = Concatenate()([hidden_layer_1, hidden_layer_2_activated])\n",
        "\n",
        "hidden_layer_3 = Dense(hidden_size3)(concatenated_layer_1)\n",
        "hidden_layer_3_activated = Lambda(custom_square_2_activation)(hidden_layer_3)\n",
        "\n",
        "concatenated_layer_2 = Concatenate()([hidden_layer_3_activated, concatenated_layer_1])\n",
        "\n",
        "output_layer = Dense(output_size, activation='linear')(concatenated_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {test_loss}')\n"
      ],
      "metadata": {
        "id": "kBy8odvKxofD",
        "outputId": "49baf5b8-8944-422d-e13c-7bfa8f84f9ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 4)]                  0         []                            \n",
            "                                                                                                  \n",
            " dense_19 (Dense)            (None, 10)                   50        ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_18 (Dense)            (None, 10)                   50        ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)           (None, 10)                   0         ['dense_19[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 20)                   0         ['dense_18[0][0]',            \n",
            " )                                                                   'lambda_2[0][0]']            \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, 10)                   210       ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)           (None, 10)                   0         ['dense_20[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 30)                   0         ['lambda_3[0][0]',            \n",
            " )                                                                   'concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " dense_21 (Dense)            (None, 1)                    31        ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 341 (1.33 KB)\n",
            "Trainable params: 341 (1.33 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 5s 32ms/step - loss: 2.0958 - val_loss: 2.6762\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.8121 - val_loss: 2.3287\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 1.5583 - val_loss: 2.0224\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.3593 - val_loss: 1.7292\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.1516 - val_loss: 1.4909\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.9884 - val_loss: 1.2675\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.8363 - val_loss: 1.0833\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.7141 - val_loss: 0.9134\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.6034 - val_loss: 0.7647\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5019 - val_loss: 0.6464\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4199 - val_loss: 0.5368\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.3447 - val_loss: 0.4485\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.2888 - val_loss: 0.3669\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2383 - val_loss: 0.3100\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.2020 - val_loss: 0.2603\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1704 - val_loss: 0.2259\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1467 - val_loss: 0.2005\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.1299 - val_loss: 0.1810\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.1169 - val_loss: 0.1670\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1075 - val_loss: 0.1541\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0996 - val_loss: 0.1447\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0930 - val_loss: 0.1374\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0874 - val_loss: 0.1305\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0832 - val_loss: 0.1247\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0788 - val_loss: 0.1205\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0753 - val_loss: 0.1160\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0727 - val_loss: 0.1130\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0690 - val_loss: 0.1080\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 1s 89ms/step - loss: 0.0661 - val_loss: 0.1036\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0636 - val_loss: 0.1000\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0616 - val_loss: 0.0971\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0595 - val_loss: 0.0938\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0575 - val_loss: 0.0914\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.0555 - val_loss: 0.0889\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0540 - val_loss: 0.0866\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0523 - val_loss: 0.0848\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0517 - val_loss: 0.0837\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 27ms/step - loss: 0.0498 - val_loss: 0.0815\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 35ms/step - loss: 0.0487 - val_loss: 0.0796\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.0477 - val_loss: 0.0774\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0469 - val_loss: 0.0758\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.0460 - val_loss: 0.0745\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0453 - val_loss: 0.0732\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0446 - val_loss: 0.0719\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0440 - val_loss: 0.0703\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0433 - val_loss: 0.0701\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0430 - val_loss: 0.0701\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0424 - val_loss: 0.0688\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0420 - val_loss: 0.0671\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0418 - val_loss: 0.0666\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0413 - val_loss: 0.0661\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0410 - val_loss: 0.0651\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0407 - val_loss: 0.0649\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0405 - val_loss: 0.0638\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0402 - val_loss: 0.0634\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0398 - val_loss: 0.0626\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0397 - val_loss: 0.0615\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0394 - val_loss: 0.0617\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0392 - val_loss: 0.0613\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0391 - val_loss: 0.0607\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0388 - val_loss: 0.0605\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0386 - val_loss: 0.0600\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0385 - val_loss: 0.0600\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0384 - val_loss: 0.0595\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0384 - val_loss: 0.0585\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0383 - val_loss: 0.0596\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0380 - val_loss: 0.0592\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0379 - val_loss: 0.0579\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0376 - val_loss: 0.0578\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0377 - val_loss: 0.0580\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0374 - val_loss: 0.0575\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0373 - val_loss: 0.0571\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0372 - val_loss: 0.0563\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0370 - val_loss: 0.0565\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0370 - val_loss: 0.0566\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0369 - val_loss: 0.0563\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0369 - val_loss: 0.0561\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0369 - val_loss: 0.0551\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0367 - val_loss: 0.0557\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0366 - val_loss: 0.0553\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0364 - val_loss: 0.0551\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0364 - val_loss: 0.0557\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0365 - val_loss: 0.0551\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0365 - val_loss: 0.0554\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0361 - val_loss: 0.0544\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0361 - val_loss: 0.0538\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0361 - val_loss: 0.0542\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0361 - val_loss: 0.0542\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0360 - val_loss: 0.0535\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0359 - val_loss: 0.0542\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0358 - val_loss: 0.0540\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0358 - val_loss: 0.0534\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0357 - val_loss: 0.0541\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0357 - val_loss: 0.0532\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0356 - val_loss: 0.0529\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0355 - val_loss: 0.0530\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0355 - val_loss: 0.0529\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0359 - val_loss: 0.0518\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0354 - val_loss: 0.0523\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0354 - val_loss: 0.0517\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0433\n",
            "Test loss: 0.04333622381091118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Concatenate, Lambda\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Custom activation function using TensorFlow operations\n",
        "def custom_square_activation(x):\n",
        "    return tf.math.pow(x, 2) / 4\n",
        "\n",
        "def custom_square_2_activation(x):\n",
        "    return tf.math.pow(x, 2) / 24\n",
        "\n",
        "# Load Iris dataset\n",
        "iris_data = load_iris()\n",
        "X, y = iris_data.data, iris_data.target\n",
        "\n",
        "# Preprocess the data: Normalize X\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# One-hot encode y\n",
        "y_encoded = to_categorical(y)\n",
        "\n",
        "# Define the input size and hidden sizes\n",
        "input_size = X_scaled.shape[1]\n",
        "hidden_size1 = 10\n",
        "hidden_size2 = 10\n",
        "hidden_size3 = 10\n",
        "output_size = 3  # This should match the number of classes for classification\n",
        "\n",
        "# Define the model architecture\n",
        "input_layer = Input(shape=(input_size,))\n",
        "\n",
        "hidden_layer_1 = Dense(hidden_size1, activation='linear')(input_layer)\n",
        "hidden_layer_2 = Dense(hidden_size2)(input_layer)\n",
        "hidden_layer_2_activated = Lambda(custom_square_activation)(hidden_layer_2)\n",
        "\n",
        "concatenated_layer_1 = Concatenate()([hidden_layer_1, hidden_layer_2_activated])\n",
        "\n",
        "hidden_layer_3 = Dense(hidden_size3)(concatenated_layer_1)\n",
        "hidden_layer_3_activated = Lambda(custom_square_2_activation)(hidden_layer_3)\n",
        "\n",
        "concatenated_layer_2 = Concatenate()([hidden_layer_3_activated, concatenated_layer_1])\n",
        "\n",
        "output_layer = Dense(output_size, activation='softmax')(concatenated_layer_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {test_loss}')\n",
        "print(f'Test accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "id": "R0fu8CwryNoc",
        "outputId": "bb54ac79-eff2-4b3f-aa31-70df3042a617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, 4)]                  0         []                            \n",
            "                                                                                                  \n",
            " dense_23 (Dense)            (None, 10)                   50        ['input_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_22 (Dense)            (None, 10)                   50        ['input_4[0][0]']             \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)           (None, 10)                   0         ['dense_23[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate  (None, 20)                   0         ['dense_22[0][0]',            \n",
            " )                                                                   'lambda_4[0][0]']            \n",
            "                                                                                                  \n",
            " dense_24 (Dense)            (None, 10)                   210       ['concatenate_4[0][0]']       \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)           (None, 10)                   0         ['dense_24[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate  (None, 30)                   0         ['lambda_5[0][0]',            \n",
            " )                                                                   'concatenate_4[0][0]']       \n",
            "                                                                                                  \n",
            " dense_25 (Dense)            (None, 3)                    93        ['concatenate_5[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 403 (1.57 KB)\n",
            "Trainable params: 403 (1.57 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 2s 41ms/step - loss: 0.9354 - accuracy: 0.5000 - val_loss: 0.8358 - val_accuracy: 0.6667\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.8682 - accuracy: 0.5833 - val_loss: 0.7873 - val_accuracy: 0.7083\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.8092 - accuracy: 0.6146 - val_loss: 0.7424 - val_accuracy: 0.7083\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.7573 - accuracy: 0.6458 - val_loss: 0.7032 - val_accuracy: 0.7917\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.7136 - accuracy: 0.6458 - val_loss: 0.6660 - val_accuracy: 0.7917\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.6725 - accuracy: 0.6458 - val_loss: 0.6328 - val_accuracy: 0.7917\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6384 - accuracy: 0.6667 - val_loss: 0.6016 - val_accuracy: 0.8333\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6066 - accuracy: 0.6667 - val_loss: 0.5729 - val_accuracy: 0.8333\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.5795 - accuracy: 0.6667 - val_loss: 0.5455 - val_accuracy: 0.9167\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.5522 - accuracy: 0.6979 - val_loss: 0.5215 - val_accuracy: 0.9167\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5296 - accuracy: 0.7188 - val_loss: 0.4979 - val_accuracy: 0.9167\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5078 - accuracy: 0.7188 - val_loss: 0.4772 - val_accuracy: 0.9167\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.4880 - accuracy: 0.7604 - val_loss: 0.4582 - val_accuracy: 0.9167\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4695 - accuracy: 0.7917 - val_loss: 0.4405 - val_accuracy: 0.9167\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4526 - accuracy: 0.8125 - val_loss: 0.4227 - val_accuracy: 0.9167\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4361 - accuracy: 0.8125 - val_loss: 0.4046 - val_accuracy: 0.9167\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4208 - accuracy: 0.8333 - val_loss: 0.3891 - val_accuracy: 0.9583\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4064 - accuracy: 0.8333 - val_loss: 0.3758 - val_accuracy: 0.9583\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3930 - accuracy: 0.8333 - val_loss: 0.3610 - val_accuracy: 0.9583\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3808 - accuracy: 0.8438 - val_loss: 0.3475 - val_accuracy: 0.9583\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3687 - accuracy: 0.8438 - val_loss: 0.3358 - val_accuracy: 0.9583\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.3583 - accuracy: 0.8333 - val_loss: 0.3231 - val_accuracy: 0.9583\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3482 - accuracy: 0.8542 - val_loss: 0.3129 - val_accuracy: 0.9167\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3380 - accuracy: 0.8542 - val_loss: 0.3036 - val_accuracy: 0.9167\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3293 - accuracy: 0.8750 - val_loss: 0.2949 - val_accuracy: 0.9167\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3205 - accuracy: 0.8750 - val_loss: 0.2853 - val_accuracy: 0.9167\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.3130 - accuracy: 0.8854 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3047 - accuracy: 0.8854 - val_loss: 0.2690 - val_accuracy: 0.9167\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.2978 - accuracy: 0.8854 - val_loss: 0.2621 - val_accuracy: 0.9167\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.2909 - accuracy: 0.8854 - val_loss: 0.2550 - val_accuracy: 0.9167\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2841 - accuracy: 0.8854 - val_loss: 0.2481 - val_accuracy: 0.9167\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2777 - accuracy: 0.9167 - val_loss: 0.2430 - val_accuracy: 0.9167\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.2714 - accuracy: 0.9062 - val_loss: 0.2370 - val_accuracy: 0.9167\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2656 - accuracy: 0.9062 - val_loss: 0.2329 - val_accuracy: 0.9167\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2594 - accuracy: 0.9062 - val_loss: 0.2248 - val_accuracy: 0.9167\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2537 - accuracy: 0.9062 - val_loss: 0.2179 - val_accuracy: 0.9167\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2480 - accuracy: 0.9271 - val_loss: 0.2111 - val_accuracy: 0.9167\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.2430 - accuracy: 0.9271 - val_loss: 0.2072 - val_accuracy: 0.9167\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2369 - accuracy: 0.9271 - val_loss: 0.1989 - val_accuracy: 0.9167\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.2309 - accuracy: 0.9271 - val_loss: 0.1937 - val_accuracy: 0.9167\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.2256 - accuracy: 0.9271 - val_loss: 0.1896 - val_accuracy: 0.9583\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.2204 - accuracy: 0.9271 - val_loss: 0.1821 - val_accuracy: 0.9583\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.2148 - accuracy: 0.9271 - val_loss: 0.1791 - val_accuracy: 0.9583\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2094 - accuracy: 0.9271 - val_loss: 0.1756 - val_accuracy: 0.9583\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2038 - accuracy: 0.9271 - val_loss: 0.1704 - val_accuracy: 0.9583\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1981 - accuracy: 0.9271 - val_loss: 0.1633 - val_accuracy: 0.9583\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 20ms/step - loss: 0.1930 - accuracy: 0.9375 - val_loss: 0.1598 - val_accuracy: 0.9583\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 21ms/step - loss: 0.1877 - accuracy: 0.9375 - val_loss: 0.1533 - val_accuracy: 0.9583\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1824 - accuracy: 0.9375 - val_loss: 0.1479 - val_accuracy: 0.9583\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.1774 - accuracy: 0.9375 - val_loss: 0.1457 - val_accuracy: 0.9583\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1721 - accuracy: 0.9375 - val_loss: 0.1430 - val_accuracy: 0.9583\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.1680 - accuracy: 0.9375 - val_loss: 0.1390 - val_accuracy: 0.9583\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1623 - accuracy: 0.9375 - val_loss: 0.1356 - val_accuracy: 0.9583\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1582 - accuracy: 0.9375 - val_loss: 0.1287 - val_accuracy: 0.9583\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1531 - accuracy: 0.9375 - val_loss: 0.1266 - val_accuracy: 0.9583\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1492 - accuracy: 0.9375 - val_loss: 0.1207 - val_accuracy: 0.9583\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1442 - accuracy: 0.9375 - val_loss: 0.1200 - val_accuracy: 0.9583\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.1404 - accuracy: 0.9479 - val_loss: 0.1165 - val_accuracy: 0.9583\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1353 - accuracy: 0.9479 - val_loss: 0.1134 - val_accuracy: 0.9583\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1317 - accuracy: 0.9479 - val_loss: 0.1106 - val_accuracy: 0.9583\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1305 - accuracy: 0.9479 - val_loss: 0.1025 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1244 - accuracy: 0.9583 - val_loss: 0.1035 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1209 - accuracy: 0.9583 - val_loss: 0.1037 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1173 - accuracy: 0.9583 - val_loss: 0.1032 - val_accuracy: 0.9583\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1147 - accuracy: 0.9583 - val_loss: 0.0980 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1114 - accuracy: 0.9583 - val_loss: 0.0991 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1088 - accuracy: 0.9583 - val_loss: 0.0991 - val_accuracy: 0.9583\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1069 - accuracy: 0.9583 - val_loss: 0.0974 - val_accuracy: 0.9583\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1046 - accuracy: 0.9688 - val_loss: 0.0855 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1009 - accuracy: 0.9583 - val_loss: 0.0835 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0988 - accuracy: 0.9583 - val_loss: 0.0830 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0974 - accuracy: 0.9688 - val_loss: 0.0870 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0949 - accuracy: 0.9792 - val_loss: 0.0846 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0931 - accuracy: 0.9792 - val_loss: 0.0816 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0913 - accuracy: 0.9688 - val_loss: 0.0759 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0908 - accuracy: 0.9688 - val_loss: 0.0723 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0886 - accuracy: 0.9688 - val_loss: 0.0767 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0866 - accuracy: 0.9792 - val_loss: 0.0762 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0853 - accuracy: 0.9792 - val_loss: 0.0735 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0840 - accuracy: 0.9792 - val_loss: 0.0739 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0826 - accuracy: 0.9792 - val_loss: 0.0720 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0837 - accuracy: 0.9688 - val_loss: 0.0657 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0807 - accuracy: 0.9688 - val_loss: 0.0689 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0796 - accuracy: 0.9792 - val_loss: 0.0705 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0798 - accuracy: 0.9792 - val_loss: 0.0721 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0773 - accuracy: 0.9792 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0789 - accuracy: 0.9583 - val_loss: 0.0560 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0760 - accuracy: 0.9688 - val_loss: 0.0607 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0752 - accuracy: 0.9792 - val_loss: 0.0687 - val_accuracy: 0.9583\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0755 - accuracy: 0.9792 - val_loss: 0.0682 - val_accuracy: 0.9583\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0737 - accuracy: 0.9792 - val_loss: 0.0632 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0733 - accuracy: 0.9792 - val_loss: 0.0619 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0725 - accuracy: 0.9792 - val_loss: 0.0592 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0720 - accuracy: 0.9792 - val_loss: 0.0568 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0714 - accuracy: 0.9792 - val_loss: 0.0570 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0709 - accuracy: 0.9792 - val_loss: 0.0595 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0710 - accuracy: 0.9792 - val_loss: 0.0579 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0697 - accuracy: 0.9792 - val_loss: 0.0587 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0701 - accuracy: 0.9792 - val_loss: 0.0619 - val_accuracy: 0.9583\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0693 - accuracy: 0.9792 - val_loss: 0.0523 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0523 - accuracy: 1.0000\n",
            "Test loss: 0.05231952294707298\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3uMehfguyjDV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}