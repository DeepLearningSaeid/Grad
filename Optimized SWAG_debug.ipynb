{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningSaeid/Grad/blob/main/Optimized%20SWAG_debug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import timeit\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, concatenate, Dropout, Flatten, Activation, BatchNormalization\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import get_custom_objects, to_categorical, plot_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.datasets import load_iris, load_digits, load_wine, load_diabetes, load_breast_cancer, fetch_olivetti_faces, load_linnerud\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "from sklearn.datasets import load_digits\n"
      ],
      "metadata": {
        "id": "5FIWRupRrL-v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Custom Activation Functions:\n",
        "# We create a function define_activation_functions to define a series of polynomial-based custom activation functions.\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(110)\n",
        "\n",
        "# Define custom activation functions and update the custom objects dictionary\n",
        "def define_activation_functions():\n",
        "    \"\"\"\n",
        "    Define custom activation functions and update the custom objects dictionary.\n",
        "    \"\"\"\n",
        "    # Define activation functions and their respective names\n",
        "    activation_functions = [\n",
        "        ('X_1', lambda x: tf.pow(x, 1)),\n",
        "        ('X_2', lambda x: tf.pow(x, 2) / 2),\n",
        "        ('X_2_', lambda x: tf.pow(x, 2) / 24),\n",
        "        ('X_2__', lambda x: tf.pow(x, 2) / 720),\n",
        "        ('X_2___', lambda x: tf.pow(x, 2) / 40320),\n",
        "    ]\n",
        "\n",
        "    # Update the custom objects dictionary with the defined activation functions\n",
        "    for name, function in activation_functions:\n",
        "        get_custom_objects().update({name: Activation(function)})\n",
        "\n",
        "# Define and register the custom activation functions\n",
        "define_activation_functions()\n",
        "\n",
        "def create_optimized_model(input_dim, hidden_dim, output_dim, metrics='accuracy', learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Create an optimized SWAG model with a custom architecture.\n",
        "\n",
        "    :param input_dim: int, dimension of the input data\n",
        "    :param output_dim: int, dimension of the output data\n",
        "    :param hidden_dim: int, hidden layer dimension, default is 50\n",
        "    :return: Model, a compiled Keras model\n",
        "    \"\"\"\n",
        "\n",
        "    # Define input layer\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "    # First layer with custom activations\n",
        "    layer_1_x1 = Dense(hidden_dim, activation='X_1')(input_layer)\n",
        "    layer_1_x2 = Dense(hidden_dim, activation='X_2')(input_layer)\n",
        "    concat_first_layer = concatenate([layer_1_x1, layer_1_x2])\n",
        "\n",
        "    # Second layer with custom activations\n",
        "    layer_x3_x4 = Dense(hidden_dim, activation='X_2_')(concat_first_layer)\n",
        "\n",
        "    # Third layer with custom activations\n",
        "    layer_x5_x6 = Dense(hidden_dim, activation='X_2__')(layer_x3_x4)\n",
        "\n",
        "    # Concatenate all layers\n",
        "    concat_second_layer = concatenate([layer_1_x1, layer_1_x2, concat_first_layer,\n",
        "                                       layer_x3_x4, layer_x5_x6])\n",
        "\n",
        "    # Output layer for the concatenated layers\n",
        "    output_first_layer = Dense(hidden_dim, activation='linear')(concat_second_layer)\n",
        "\n",
        "    # Final output layer\n",
        "    output_layer = Dense(output_dim, activation='linear')(output_first_layer)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model(input_layer, output_layer)\n",
        "\n",
        "    # Define the optimizer\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Compile the model with specified loss and metrics\n",
        "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=[metrics])\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "xID7aH81K83q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create an instance of the optimized model\n",
        "# Input dimension is 784 (28x28 pixels, flattened),\n",
        "# 500 hidden units in each layer,\n",
        "# 10 output units (one for each digit)\n",
        "optimized_model = create_optimized_model(784, 500, 10, 'accuracy')\n",
        "\n",
        "# Load the MNIST dataset\n",
        "# MNIST dataset contains 28x28 pixel grayscale images of handwritten digits (0-9)\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "# Normalize the pixel values of the images by adding 10 and dividing by 300\n",
        "# This changes the range of pixel values and can help with model training\n",
        "x_train, x_test = (x_train + 10) / 300.0, (x_test + 10) / 300.0\n",
        "\n",
        "# Reshape the data\n",
        "# Flatten the 28x28 images into 1D arrays of 784 elements for model input\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# One-hot encode the labels\n",
        "# Convert class vectors (integers) to binary class matrices for use with categorical crossentropy\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Training the model\n",
        "# Specify batch size and number of epochs for training\n",
        "batch_size = 100\n",
        "epochs = 2\n",
        "\n",
        "# Start a timer to measure training time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Fit the model on the training data\n",
        "# x_train and y_train are the training data and labels\n",
        "# Batch size determines the number of samples processed before the model is updated\n",
        "# Epochs is the number of complete passes through the training dataset\n",
        "# Verbose=1 shows a progress bar during training\n",
        "# Validation data is used to evaluate the loss and any model metrics at the end of each epoch\n",
        "history = optimized_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
        "\n",
        "# Stop the timer\n",
        "end = timeit.default_timer()\n",
        "\n",
        "# Output the training time\n",
        "# Print the time taken by the training process\n",
        "print(f\"Training time: {end - start} seconds\")\n"
      ],
      "metadata": {
        "id": "8aQhT2LoKZDH",
        "outputId": "86e6fcdf-8785-4351-a232-1f88a361926e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/2\n",
            "600/600 [==============================] - 14s 6ms/step - loss: 0.0420 - accuracy: 0.8375 - val_loss: 0.0268 - val_accuracy: 0.9026\n",
            "Epoch 2/2\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0207 - accuracy: 0.9354 - val_loss: 0.0201 - val_accuracy: 0.9283\n",
            "Training time: 27.255359362000007 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "# Fashion MNIST is a dataset of Zalando's article images, consisting of 60,000 training and 10,000 test examples.\n",
        "# Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "# Normalize the pixel values of the images by adding 10 and then dividing by 300.\n",
        "# This changes the range of pixel values and can help with model training.\n",
        "x_train, x_test = (x_train + 10) / 300.0, (x_test + 10) / 300.0\n",
        "\n",
        "# Reshape the data\n",
        "# Flatten the 28x28 images into 1D arrays of 784 elements for model input.\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# One-hot encode the labels\n",
        "# Convert class vectors (integers) to binary class matrices for use with categorical crossentropy.\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 100\n",
        "epochs = 5\n",
        "\n",
        "# # AutoKeras model\n",
        "# # AutoKeras is an AutoML system based on Keras. It tries different model architectures automatically.\n",
        "# # Here, StructuredDataClassifier is used, suitable for structured data classification.\n",
        "# # max_trials parameter indicates how many different models to try.\n",
        "# ak_model = ak.StructuredDataClassifier(max_trials=3, overwrite=True)\n",
        "\n",
        "# # Start a timer for the AutoKeras model training\n",
        "# start_ak = timeit.default_timer()\n",
        "\n",
        "# # Fit the AutoKeras model on the training data\n",
        "# ak_model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# # Stop the timer and print the training time\n",
        "# end_ak = timeit.default_timer()\n",
        "# print(f\"AutoKeras Training time: {end_ak - start_ak} seconds\")\n",
        "\n",
        "# # Evaluate the AutoKeras model\n",
        "# # The evaluate function returns the loss value & metrics values for the model in test mode.\n",
        "# _, ak_accuracy = ak_model.evaluate(x_test, y_test)\n",
        "# print(f\"AutoKeras Model Accuracy: {ak_accuracy}\")\n",
        "\n",
        "# # Print separator for clarity in output\n",
        "print('******************************************************************************************')\n",
        "\n",
        "# Custom optimized model\n",
        "# Create an instance of a custom optimized model with specific parameters.\n",
        "optimized_model = create_optimized_model(784, 500, 10, 'accuracy')\n",
        "\n",
        "# Start a timer for the custom model training\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Fit the custom model on the training data\n",
        "history = optimized_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
        "\n",
        "# Stop the timer and print the training time\n",
        "end = timeit.default_timer()\n",
        "print(f\"Training time: {end - start} seconds\")\n"
      ],
      "metadata": {
        "id": "lVNTPp71EwH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc4aed32-4a91-4bc1-f239-489609a4eff5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "******************************************************************************************\n",
            "Epoch 1/5\n",
            "600/600 [==============================] - 5s 5ms/step - loss: 0.1036 - accuracy: 0.7780 - val_loss: 0.0334 - val_accuracy: 0.8323\n",
            "Epoch 2/5\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.0290 - accuracy: 0.8553 - val_loss: 0.0291 - val_accuracy: 0.8465\n",
            "Epoch 3/5\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.0261 - accuracy: 0.8723 - val_loss: 0.0270 - val_accuracy: 0.8592\n",
            "Epoch 4/5\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0244 - accuracy: 0.8830 - val_loss: 0.0282 - val_accuracy: 0.8623\n",
            "Epoch 5/5\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.0233 - accuracy: 0.8909 - val_loss: 0.0265 - val_accuracy: 0.8642\n",
            "Training time: 22.49565139100001 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "# The Iris dataset is a classic dataset for classification, containing 3 types of Iris flowers\n",
        "iris = load_iris()\n",
        "X = iris.data  # Feature matrix (measurements of the flowers)\n",
        "y = iris.target  # Target vector (types of Iris flowers)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# 80% of data is used for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "# StandardScaler standardizes features by removing the mean and scaling to unit variance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit to data and then transform it\n",
        "X_test = scaler.transform(X_test)        # Perform standardization by centering and scaling\n",
        "\n",
        "# One-hot encode the labels\n",
        "# Convert class vectors (integers) to binary class matrices\n",
        "y_train = to_categorical(y_train, 3)\n",
        "y_test = to_categorical(y_test, 3)\n",
        "\n",
        "# Determine the number of features and classes\n",
        "num_features = X_train.shape[1]  # Number of features\n",
        "num_classes = y_train.shape[1]   # Number of classes\n",
        "\n",
        "# AutoKeras model\n",
        "# AutoKeras is an AutoML system based on Keras. It automatically tries different model architectures.\n",
        "# max_trials is the number of different models to try.\n",
        "ak_model = ak.StructuredDataClassifier(max_trials=3, overwrite=True)\n",
        "\n",
        "# Start timing the AutoKeras training\n",
        "start_ak = timeit.default_timer()\n",
        "\n",
        "# Train the AutoKeras model\n",
        "ak_model.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "# Stop timing and print the training duration\n",
        "end_ak = timeit.default_timer()\n",
        "print(f\"AutoKeras Training time: {end_ak - start_ak} seconds\")\n",
        "\n",
        "# Evaluate the AutoKeras model\n",
        "# Evaluates the model on the testing dataset\n",
        "_, ak_accuracy = ak_model.evaluate(X_test, y_test)\n",
        "print(f\"AutoKeras Model Accuracy: {ak_accuracy}\")\n",
        "\n",
        "# Print separators for output clarity\n",
        "print('******************************************************************************************')\n",
        "\n",
        "# Custom model\n",
        "# Create an instance of a custom optimized model with specific parameters\n",
        "model = create_optimized_model(num_features, 10, num_classes, 'accuracy')\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "\n",
        "# Start timing the custom model training\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Train the custom model\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Stop timing and print the training duration\n",
        "end = timeit.default_timer()\n",
        "print(f\"Training time: {end - start} seconds\")\n",
        "\n",
        "# Evaluate the custom model\n",
        "# Evaluates the model on the testing dataset\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"SWAG Model Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "KXztQpoaiuYu",
        "outputId": "f6b653b7-eade-4237-b1fd-f70ab4f9b9d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ak' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c9a55a513268>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# AutoKeras is an AutoML system based on Keras. It automatically tries different model architectures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# max_trials is the number of different models to try.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mak_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructuredDataClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Start timing the AutoKeras training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ak' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "# The 'digits' dataset is a collection of 8x8 pixel images of handwritten digits\n",
        "digits = load_digits()\n",
        "X = digits.data  # Feature matrix (pixel values of images)\n",
        "y = to_categorical(digits.target)  # One-hot encode the target labels\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "# 80% of data is used for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using MinMaxScaler to scale the data\n",
        "# MinMaxScaler scales each feature to a given range, here between 0 and 1.\n",
        "# This is important for neural network models which are sensitive to input scale.\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit to data, then transform it\n",
        "X_test = scaler.transform(X_test)        # Apply the same transformation to the test data\n",
        "\n",
        "# Determine the number of features and classes from the data\n",
        "num_features = X_train.shape[1]  # Number of features (64 pixels per image in this case)\n",
        "num_classes = y_train.shape[1]   # Number of classes (digits 0-9, so 10 classes)\n",
        "\n",
        "# Print the number of features for confirmation\n",
        "print(num_features)\n",
        "\n",
        "# AutoKeras model\n",
        "# AutoKeras is an AutoML system based on Keras, automating the process of model selection and training.\n",
        "# max_trials indicates how many different models to try.\n",
        "ak_model = ak.StructuredDataClassifier(max_trials=5, overwrite=True)\n",
        "\n",
        "# Define the number of epochs for training\n",
        "epochs = 5\n",
        "\n",
        "# Train the AutoKeras model\n",
        "# Fit the model to the training data\n",
        "ak_model.fit(X_train, y_train, epochs=epochs)\n",
        "\n",
        "# Evaluate the AutoKeras model on the test set\n",
        "# Evaluates the model's performance in terms of loss and accuracy\n",
        "loss, ak_accuracy = ak_model.evaluate(X_test, y_test)\n",
        "print(f\"AutoKeras Model Accuracy: {ak_accuracy}\")\n",
        "\n",
        "# Print separator for clarity in output\n",
        "print('******************************************************************************************')\n",
        "\n",
        "# Custom optimized model\n",
        "# Creating an instance of a custom optimized model with specific parameters\n",
        "model = create_optimized_model(num_features, 200, num_classes, 'accuracy')\n",
        "\n",
        "# Training parameters for the custom model\n",
        "batch_size = 10  # Number of samples per gradient update\n",
        "\n",
        "# Train the custom model\n",
        "# Fit the model to the training data using defined batch size and epochs\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the custom model on the test set\n",
        "# Evaluates the model's performance in terms of loss and accuracy\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"SWAG Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "CCEhk7EEBPb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "# The Wine dataset is a classic multiclass classification dataset\n",
        "wine = load_wine()\n",
        "X = wine.data  # Feature matrix (chemical constituents of wines)\n",
        "y = to_categorical(wine.target)  # One-hot encode the target labels\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "# 80% of data is used for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using MinMaxScaler to scale the data\n",
        "# MinMaxScaler scales each feature to a given range, here between 0 and 1\n",
        "# This is important for neural network models which are sensitive to input scale\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit to data, then transform it\n",
        "X_test = scaler.transform(X_test)        # Apply the same transformation to the test data\n",
        "\n",
        "# Determine the number of features and classes from the data\n",
        "num_features = X_train.shape[1]  # Number of features\n",
        "num_classes = y_train.shape[1]   # Number of classes\n",
        "\n",
        "# Print the number of features for confirmation\n",
        "print(num_features)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 10\n",
        "epochs = 5\n",
        "\n",
        "# AutoKeras model\n",
        "# AutoKeras is an AutoML system based on Keras, automating the process of model selection and training\n",
        "# max_trials indicates how many different models to try\n",
        "ak_model = ak.StructuredDataClassifier(max_trials=5, overwrite=True)\n",
        "\n",
        "# Train the AutoKeras model\n",
        "# Fit the model to the training data\n",
        "ak_model.fit(X_train, y_train, epochs=epochs)\n",
        "\n",
        "# Evaluate the AutoKeras model on the test set\n",
        "# Evaluates the model's performance in terms of loss and accuracy\n",
        "loss, ak_accuracy = ak_model.evaluate(X_test, y_test)\n",
        "print(f\"AutoKeras Model Accuracy: {ak_accuracy}\")\n",
        "\n",
        "# Print separators for clarity in output\n",
        "print('******************************************************************************************')\n",
        "\n",
        "# Custom optimized model\n",
        "# Creating an instance of a custom optimized model with specific parameters\n",
        "model = create_optimized_model(num_features, 20, num_classes, 'accuracy')\n",
        "\n",
        "# Train the custom model\n",
        "# Fit the model to the training data using defined batch size and epochs\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the custom model on the test set\n",
        "# Evaluates the model's performance in terms of loss and accuracy\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"SWAG Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "u6pEsuuoBPYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "# The Breast Cancer dataset is a binary classification dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data  # Feature matrix (characteristics of the cell nuclei)\n",
        "y = cancer.target  # Target vector (cancer diagnosis: malignant or benign)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "# 80% of data is used for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using MinMaxScaler to scale the data\n",
        "# MinMaxScaler scales each feature to a given range, here between 0 and 1\n",
        "# This is important for neural network models which are sensitive to input scale\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit to data, then transform it\n",
        "X_test = scaler.transform(X_test)        # Apply the same transformation to the test data\n",
        "\n",
        "# Determine the number of features from the data\n",
        "num_features = X_train.shape[1]  # Number of features\n",
        "\n",
        "# Print the number of features for confirmation\n",
        "print(num_features)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 10\n",
        "epochs = 5\n",
        "\n",
        "# AutoKeras model\n",
        "# AutoKeras is an AutoML system based on Keras, automating the process of model selection and training\n",
        "# max_trials indicates how many different models to try\n",
        "ak_model = ak.StructuredDataClassifier(max_trials=5, overwrite=True)\n",
        "\n",
        "# Train the AutoKeras model\n",
        "# Fit the model to the training data\n",
        "ak_model.fit(X_train, y_train, epochs=epochs)\n",
        "\n",
        "# Evaluate the AutoKeras model on the test set\n",
        "# Evaluates the model's performance in terms of loss and accuracy\n",
        "loss, ak_accuracy = ak_model.evaluate(X_test, y_test)\n",
        "print(f\"AutoKeras Model Accuracy: {ak_accuracy}\")\n",
        "\n",
        "# Print separators for clarity in output\n",
        "print('******************************************************************************************')\n",
        "\n",
        "# Custom optimized model\n",
        "# Creating an instance of a custom optimized model with specific parameters\n",
        "# Here, the output layer has 1 unit because it's a binary classification problem\n",
        "model = create_optimized_model(num_features, 20, 1, 'accuracy')\n",
        "\n",
        "# Train the custom model\n",
        "# Fit the model to the training data using defined batch size and epochs\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the custom model on the test set\n",
        "# Evaluates the model's performance in terms of loss and accuracy\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "g7N_ZWiPBPS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "import autokeras as ak\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load Boston Housing data\n",
        "# The Boston Housing dataset contains information about various houses in Boston\n",
        "# It's used for regression tasks to predict the value of houses based on different attributes\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "    path=\"boston_housing.npz\", test_split=0.2, seed=113\n",
        ")\n",
        "\n",
        "# Using MinMaxScaler to scale the data\n",
        "# MinMaxScaler scales each feature to a given range, here between 0 and 1\n",
        "# This scaling can improve the performance of neural network models\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit to data, then transform it\n",
        "X_test = scaler.transform(X_test)        # Apply the same transformation to the test data\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 5\n",
        "epochs = 30\n",
        "\n",
        "# AutoKeras model for regression\n",
        "# AutoKeras StructuredDataRegressor automatically tries different model architectures\n",
        "# max_trials indicates how many different models to try\n",
        "ak_model = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\n",
        "\n",
        "# Start timing the AutoKeras model training\n",
        "start_ak = timeit.default_timer()\n",
        "\n",
        "# Train the AutoKeras model\n",
        "ak_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Stop timing and print the training duration\n",
        "end_ak = timeit.default_timer()\n",
        "print(f\"AutoKeras Training time: {end_ak - start_ak} seconds\")\n",
        "\n",
        "# Evaluate the AutoKeras model\n",
        "# Evaluates the model's performance in terms of loss and mean absolute error (MAE)\n",
        "loss, ak_mae = ak_model.evaluate(X_test, y_test)\n",
        "print(f\"AutoKeras Model MAE: {ak_mae}\")\n",
        "\n",
        "# Print separators for output clarity\n",
        "print('******************************************************************************************')\n",
        "\n",
        "# Custom optimized model for regression\n",
        "# Create an instance of a custom optimized model with specific parameters\n",
        "# The model is designed for regression with Mean Absolute Error (MAE) as the metric\n",
        "input_dim = X_train.shape[1]  # Number of input features\n",
        "optimized_model = create_optimized_model(input_dim, input_dim, 1, 'mae')  # Using input_dim as hidden_dim and 1 output unit\n",
        "\n",
        "# Start timing the custom model training\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Train the custom model\n",
        "# Fit the model to the training data using defined batch size and epochs\n",
        "history = optimized_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Stop timing and print the training duration\n",
        "end = timeit.default_timer()\n",
        "print(f\"Training time: {end - start} seconds\")\n",
        "# Evaluate the AutoKeras model\n",
        "# Evaluates the model's performance in terms of loss and mean absolute error (MAE)\n",
        "loss, _mae = optimized_model.evaluate(X_test, y_test)\n",
        "print(f\"AutoKeras Model MAE: {_mae}\")\n",
        "\n",
        "# Print separators for output clarity\n"
      ],
      "metadata": {
        "id": "Bh72fFHYpQ3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the dataset\n",
        "# The Diabetes dataset is commonly used for regression analysis\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data  # Feature matrix (medical measurements)\n",
        "y = diabetes.target  # Target variable (quantitative measure of disease progression)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# test_size=0.2 allocates 20% of the data for testing and the rest for training\n",
        "# random_state=42 ensures reproducibility of the results\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features using MinMaxScaler\n",
        "# MinMaxScaler scales each feature to a range between 0 and 1\n",
        "# This scaling is beneficial for neural network models\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit to the training data and transform it\n",
        "X_test = scaler.transform(X_test)        # Transform the test data using the same scale\n",
        "\n",
        "# Determine the number of features from the data\n",
        "num_features = X_train.shape[1]  # Number of features in the dataset\n",
        "print(num_features)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 10\n",
        "epochs = 30\n",
        "\n",
        "# AutoKeras StructuredDataRegressor model\n",
        "# This is an AutoML model that automatically searches for the best model architecture\n",
        "# max_trials=3 sets the number of different models AutoKeras will try\n",
        "ak_model = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\n",
        "\n",
        "# Train the AutoKeras model\n",
        "# Fit the model on the training data\n",
        "ak_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluate the AutoKeras model\n",
        "# Evaluates the model's performance in terms of loss and mean absolute error (MAE)\n",
        "loss_ak, ak_mae = ak_model.evaluate(X_test, y_test)\n",
        "print(f\"AutoKeras Model MAE: {ak_mae}\")\n",
        "\n",
        "# Print separators for clarity in output\n",
        "print('******************************************************************************************')\n",
        "\n",
        "# Custom optimized model\n",
        "# Create an instance of a custom optimized model for regression\n",
        "# This model uses the mean absolute error (MAE) as a performance metric\n",
        "model = create_optimized_model(num_features, 20, 1, 'mae')  # The model predicts a single output\n",
        "\n",
        "# Train the custom model\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the custom model\n",
        "# Evaluates the model's performance in terms of loss and MAE\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"SWAG Model MAE: {mae}\")\n"
      ],
      "metadata": {
        "id": "KyPriNtHq-OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "# The Olivetti Faces dataset contains a set of face images\n",
        "faces = fetch_olivetti_faces()\n",
        "\n",
        "X = faces.images  # Images of faces\n",
        "y = to_categorical(faces.target)  # One-hot encode the target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# test_size=0.2 allocates 20% of the data for testing, rest for training\n",
        "# random_state=42 ensures reproducibility of the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the image data to flatten the image arrays (64x64 pixels)\n",
        "X_train = X_train.reshape((-1, 64*64))\n",
        "X_test = X_test.reshape((-1, 64*64))\n",
        "\n",
        "# Initialize a MinMaxScaler\n",
        "# This scales each feature to a given range, here between 0.01 and 0.99\n",
        "# Scaling images can improve the performance of neural network models\n",
        "min_max_scaler = MinMaxScaler(feature_range=(0.01, 0.99))\n",
        "\n",
        "# Fit and transform the training data using the scaler\n",
        "X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the same scaler\n",
        "X_test_scaled = min_max_scaler.transform(X_test)\n",
        "\n",
        "# Determine the number of features and classes from the data\n",
        "num_features = X_train.shape[1]  # Number of features (flattened image size)\n",
        "num_classes = y.shape[1]         # Number of classes (distinct faces)\n",
        "\n",
        "# Print the number of classes for confirmation\n",
        "print(num_classes)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 10\n",
        "epochs = 15\n",
        "\n",
        "# AutoKeras model for classification\n",
        "# AutoKeras StructuredDataClassifier automatically tries different model architectures\n",
        "# max_trials=5 sets the number of different models AutoKeras will try\n",
        "ak_model = ak.StructuredDataClassifier(max_trials=5, overwrite=True)\n",
        "\n",
        "# Train the AutoKeras model\n",
        "# Fit the model on the training data\n",
        "ak_model.fit(X_train, y_train, epochs=epochs)\n",
        "\n",
        "# Evaluate the AutoKeras model\n",
        "# Evaluates the model's performance in terms of loss and accuracy\n",
        "loss, ak_accuracy = ak_model.evaluate(X_test, y_test)\n",
        "print(f\"AutoKeras Model Accuracy: {ak_accuracy}\")\n",
        "\n",
        "# Print separators for clarity in output\n",
        "print('******************************************************************************************')\n",
        "\n",
        "# Custom optimized model for classification\n",
        "# Creating an instance of a custom optimized model with specific parameters\n",
        "# The model uses accuracy as the performance metric\n",
        "model = create_optimized_model(num_features, 600, num_classes, 'accuracy', 0.0001)  # Learning rate is set to 0.0001\n",
        "\n",
        "# Train the custom model\n",
        "# Fit the model on the training data using defined batch size and epochs\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the custom model\n",
        "# Evaluates the model's performance in terms of loss and accuracy\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"SWAG Model Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "c-zaXKibDx7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the Linnerud dataset\n",
        "# The Linnerud dataset is a multivariate dataset containing physical measurements and exercise data\n",
        "linnerud = load_linnerud()\n",
        "X = linnerud.data  # Physical measurements (independent variables)\n",
        "y = linnerud.target  # Exercise data (dependent variables)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# test_size=0.2 allocates 20% of the data for testing and the rest for training\n",
        "# random_state=42 ensures reproducibility of the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using MinMaxScaler to scale the data\n",
        "# MinMaxScaler scales each feature to a given range, here between 0 and 1\n",
        "# This scaling can improve the performance of neural network models\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit to the training data and transform it\n",
        "X_test = scaler.transform(X_test)        # Transform the test data using the same scale\n",
        "\n",
        "# Determine the number of features from the data\n",
        "num_features = X_train.shape[1]  # Number of features in the dataset\n",
        "print(num_features)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 10\n",
        "epochs = 30\n",
        "\n",
        "# AutoKeras StructuredDataRegressor model\n",
        "# This is an AutoML model that automatically searches for the best model architecture\n",
        "# max_trials=3 sets the number of different models AutoKeras will try\n",
        "ak_model = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\n",
        "\n",
        "# Train the AutoKeras model\n",
        "# Fit the model on the training data\n",
        "ak_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluate the AutoKeras model\n",
        "# Evaluates the model's performance in terms of loss and mean absolute error (MAE)\n",
        "loss_ak, ak_mae = ak_model.evaluate(X_test, y_test)\n",
        "print(f\"AutoKeras Model MAE: {ak_mae}\")\n",
        "\n",
        "# Custom optimized model for regression\n",
        "# Creating an instance of a custom optimized model with specific parameters\n",
        "# This model uses mean absolute error (MAE) as a performance metric\n",
        "model = create_optimized_model(num_features, 20, 1, 'mae')  # The model predicts a single output\n",
        "\n",
        "# Train the custom model\n",
        "# Fit the model on the training data using defined batch size and epochs\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the custom model\n",
        "# Evaluates the model's performance in terms of loss and MAE\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"SWAG Model MAE: {mae}\")\n"
      ],
      "metadata": {
        "id": "BHA1-SM7FDDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def generate_random_nonlinear_function():\n",
        "    \"\"\"\n",
        "    Generate a random nonlinear function based on a mix of sine, cosine, exponential,\n",
        "    and logarithmic operations.\n",
        "    \"\"\"\n",
        "    operations = [np.sin, np.cos, np.log]\n",
        "    coeffs = np.random.uniform(-1, 5, 4)  # Random coefficients\n",
        "\n",
        "    def random_function(x):\n",
        "        result = 0\n",
        "        for coeff, operation in zip(coeffs, operations):\n",
        "            # Safeguard for the logarithmic function to avoid log(0) and negative values\n",
        "            if operation == np.log:\n",
        "                result += coeff * operation(np.abs(x) + 1)\n",
        "            else:\n",
        "                result += coeff * operation(x)\n",
        "        return result\n",
        "\n",
        "    return random_function\n",
        "\n",
        "# Generate a random nonlinear function\n",
        "random_func = generate_random_nonlinear_function()\n",
        "\n",
        "# Test the function\n",
        "x_values = np.linspace(-10, 10, 400)  # Generate 400 points between -10 and 10\n",
        "y_values = random_func(x_values)\n",
        "\n",
        "# You can plot this function using matplotlib to visualize it\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(x_values, y_values)\n",
        "plt.title(\"Random Nonlinear Function\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JXIlXrDnG-PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import autokeras as ak\n",
        "# from your_module import generate_random_nonlinear_function, create_optimized_model\n",
        "\n",
        "# Generate the dataset\n",
        "random_func = generate_random_nonlinear_function()  # Define this function\n",
        "x_values = np.linspace(-10, 10, 1000)\n",
        "y_values = random_func(x_values)\n",
        "x_values = x_values.reshape(-1, 1)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0.01, 0.99))\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# AutoKeras Model\n",
        "ak_model = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\n",
        "ak_model.fit(X_train_scaled, y_train, epochs=10, batch_size=10)\n",
        "loss_ak, ak_mae = ak_model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"AutoKeras Model MAE: {ak_mae}\")\n",
        "\n",
        "# Custom Optimized Model\n",
        "model = create_optimized_model(1, 50, 1, 'mae')  # Define this function\n",
        "model.fit(X_train_scaled, y_train, epochs=10, batch_size=10, verbose=1)\n",
        "loss, mae = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Custom Model MAE: {mae}\")\n",
        "\n",
        "# Predictions\n",
        "ak_predictions = ak_model.predict(X_test_scaled)\n",
        "custom_model_predictions = model.predict(X_test_scaled)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train, y_train, label='Training data', alpha=0.5)\n",
        "plt.scatter(X_test, ak_predictions, label='AutoKeras Predictions', alpha=0.5)\n",
        "plt.scatter(X_test, custom_model_predictions, label='Custom Model Predictions', alpha=0.5)\n",
        "plt.title(\"Model Predictions vs Actual Data\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nxfhpjFe2Eel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZDg_EHg3kxe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}