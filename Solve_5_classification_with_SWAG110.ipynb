{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningSaeid/Grad/blob/main/Solve_5_classification_with_SWAG110.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "18993f35214f6d02bb40d8f7b5660171a3d9384a",
        "collapsed": true,
        "id": "rraZNltGZHsM"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3YC01S2UddSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install autokeras\n",
        "\n",
        "!pip install keras==2.9.0\n",
        "!pip install SWAG-DNN==0.1.31\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import autokeras as ak"
      ],
      "metadata": {
        "id": "tUF4enr0JAzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3a265a00b0b7f1012da45001f83a0d75598ab96b",
        "id": "lcfca3UeZHsk"
      },
      "source": [
        "from SWAG_DNN.taylor import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs=5\n",
        "batch_size=1\n",
        "\n",
        "# define 10-fold cross validation test harness\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "cvscores = []\n",
        "\n",
        "list_fun=['Ch_First','Ch_second','SWAG','Hermite','Legendre','Sin']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python code defines a function called train_fun that takes three arguments: X, Y, and Input_size. The purpose of this function is to train various models using different activation functions and perform 10-fold cross-validation to evaluate their performance. The activation functions used are stored in the list_fun list, which includes Chebyshev of the First Kind, Chebyshev of the Second Kind, SWAG, Hermite, Legendre, and Sin functions.\n",
        "\n",
        "# The function iterates through each activation function, creating a model for each one using the globals()[i] function. The model's architecture is then printed with model_1.summary(). A temporary array a is created to store the accuracy scores for each fold.\n",
        "\n",
        "# The 10-fold cross-validation process is performed using the kfold.split() method from the scikit-learn library. The model is trained with the training data, and the test data is used to evaluate its performance. The accuracy score for each fold is printed and stored in the cvscores list and the a array.\n",
        "\n",
        "# After all the folds are completed, the mean accuracy and standard deviation are calculated and printed for the current model. This process is repeated for all activation functions in the list_fun list.\n",
        "\n",
        "# Once all models have been evaluated, the function returns the temp list, which contains the accuracy scores for all models and their respective folds, along with the mean accuracy for each model.\n",
        "\n",
        "def train_fun(X,Y,Input_size):\n",
        "    list_fun=list_fun=['Ch_First','Ch_second','SWAG','Hermite','Legendre','Sin']\n",
        "    temp = list()\n",
        "    for i in list_fun:\n",
        "        print('############################################################################################################################')\n",
        "        print('############################################################################################################################')\n",
        "        print('############################################################################################################################')\n",
        "        cvscores = []\n",
        "        print(i)\n",
        "        model_1=globals()[i](Input_size,Input_size,1)\n",
        "        print(model_1.summary())\n",
        "        a = np.zeros(shape=(11,))\n",
        "        index1=0\n",
        "\n",
        "        for train, test in kfold.split(X, Y):\n",
        "            model_1.fit(X[train], Y[train], epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "            scores = model_1.evaluate(X[test], Y[test], verbose=0)\n",
        "            print(\"%s: %.2f%%\" % (model_1.metrics_names[1], scores[1]*100))\n",
        "            cvscores.append(scores[1] * 100)\n",
        "            a[index1]=scores[1]*100\n",
        "            index1=index1+1\n",
        "        a[index1]=np.mean(cvscores)\n",
        "        temp.append(a)\n",
        "        print(\"The Mean accuracy is %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "    print('**************************************************************************************************')\n",
        "    print('**************************************************************************************************')\n",
        "    print('**************************************************************************************************')\n",
        "    print('**************************************************************************************************')\n",
        "    return temp"
      ],
      "metadata": {
        "id": "MK7UurXBR9Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_keras(X_,Y_):\n",
        "  a = np.zeros(shape=(10,))\n",
        "  index1=0\n",
        "  cvscores=[]\n",
        "  for train, test in kfold.split(X_, Y_):\n",
        "      model_1 = ak.StructuredDataClassifier(max_trials=30)\n",
        "      model_1.fit(X_[train], Y_[train], epochs=epochs,verbose=0)\n",
        "      scores = model_1.evaluate(X_[test], Y_[test], verbose=0)\n",
        "      print(\" accuracy : %.2f%%\" % ( scores[1]*100))\n",
        "      cvscores.append(scores[1] * 100)\n",
        "      a[index1]=scores[1]*100\n",
        "      index1=index1+1\n",
        "  print(np.mean(cvscores))\n",
        "  print('**************************************************************************************************')\n",
        "  return a"
      ],
      "metadata": {
        "id": "a8x9E5z19roh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "51659646ab1c7e1c87283d427e7bab1c9ddb58a6",
        "id": "g48shk8hZHsl"
      },
      "source": [
        "#  ========================================================\n",
        "# First experiment\n",
        "##  Load data of   ionosphere data set\n",
        "\n",
        "\n",
        "This is a small dataset that you can <a href=\"https://www.kaggle.com/creepyghost/uci-ionosphere/version/1\">download from the kaggle </a>.\n",
        "\n",
        "\n",
        "### About this file:\n",
        "\n",
        "The Ionosphere dataset is a widely-used benchmark dataset in machine learning and pattern recognition. It was originally collected by researchers at the Johns Hopkins University Applied Physics Laboratory for the purpose of studying the ionosphere's structure and electron content. The dataset consists of radar data collected from a phased array of 16 high-frequency antennas, with a total of 34 attributes and a binary target variable.\n",
        "\n",
        "The dataset contains 351 instances, each representing a radar return from the ionosphere. The 34 attributes correspond to various features extracted from the radar signals, such as the phase and amplitude of the received signal. These features have been pre-processed and scaled to make them suitable for machine learning algorithms. The binary target variable indicates whether the radar return shows evidence of some structure in the ionosphere (labeled \"good\") or not (labeled \"bad\").\n",
        "\n",
        "Source: Donated to UCI Machine Learning Repository by\n",
        "\n",
        "Donor:\n",
        "\n",
        "Vince Sigillito (vgs '@' aplcen.apl.jhu.edu)\n",
        "\n",
        "Source:\n",
        "\n",
        "Space Physics Group Applied Physics Laboratory Johns Hopkins University Johns Hopkins Road Laurel, MD 20723"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/ionosphere_data_kaggle.csv', header=1)\n",
        "dataset = dataframe.values\n",
        "\n",
        "#Number of Rows and columns\n",
        "Number_rows,Input_size=dataset.shape\n",
        "\n",
        "X = dataset[:,0:Input_size-1].astype(float)\n",
        "Y = dataset[:,Input_size-1]\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "# ionosphere=SWAG_per(X,Y,Input_size-1,2*Input_size-1,1)\n",
        "re=auto_keras(X,Y)"
      ],
      "metadata": {
        "id": "cfbvQ1kk-CaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(re)"
      ],
      "metadata": {
        "id": "JsQFNfdY-qR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9e664b718ffcf89c49865b7c16d56603692fe233",
        "id": "SLSQzg2dZHsl"
      },
      "source": [
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/ionosphere_data_kaggle.csv', header=1)\n",
        "dataset = dataframe.values\n",
        "\n",
        "#Number of Rows and columns\n",
        "Number_rows,Input_size=dataset.shape\n",
        "\n",
        "X = dataset[:,0:Input_size-1].astype(float)\n",
        "Y = dataset[:,Input_size-1]\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "ionosphere=train_fun(X,Y,Input_size-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "82656e0b85d7dba3468014dc7ed34ac6cf344c04",
        "id": "vW6TNFKoZHsl"
      },
      "source": [
        "### We use 10 k-fold Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja3FhqGCZHsm"
      },
      "source": [
        "\n",
        "for j in range(11):\n",
        "    str_='0'\n",
        "    for i in range(6):\n",
        "        if i>0:\n",
        "            str_=str_+' & '+str(round(ionosphere[i][j], 2))\n",
        "        else:\n",
        "            str_=str(round(ionosphere[i][j], 2))\n",
        "    str_=str_+r\" \\\\ \"\n",
        "    if j<10:\n",
        "        print(str_)\n",
        "    else:\n",
        "        print('************************************************')\n",
        "        print(str_)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "27eed7ff58cf82c8891be7a4897c3e6119787f15",
        "id": "-ZZFPLZXZHsn"
      },
      "source": [
        "#  ========================================================\n",
        "# Second experiment\n",
        "##  Load data of  banknote authentication Data Set \n",
        "\n",
        "This is a small dataset that you can <a href=\"https://www.kaggle.com/jacksonharper/data_banknote_authentication\">download from the kaggle </a>.\n",
        "\n",
        "\n",
        "### About this file:\n",
        "\n",
        "https://www.kaggle.com/jacksonharper/data_banknote_authentication\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "bdb386302802d49db72877cf44748d603cc549f5",
        "id": "khT76wZ8ZHso"
      },
      "source": [
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/data_banknote_authentication.csv' , header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "\n",
        "#Number of Rows and columns\n",
        "Number_rows,Input_size=dataset.shape\n",
        "\n",
        "X = dataset[:,0:Input_size-1].astype(float)\n",
        "Y = dataset[:,Input_size-1]\n",
        "X=normalize(X)\n",
        "X=rescale_range(X)\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "banknote=train_fun(X,Y,Input_size-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkhUyKW-ZHso"
      },
      "source": [
        "\n",
        "for j in range(11):\n",
        "    str_='0'\n",
        "    for i in range(6):\n",
        "        if i>0:\n",
        "            str_=str_+' & '+str(round(banknote[i][j], 2))\n",
        "        else:\n",
        "            str_=str(round(banknote[i][j], 2))\n",
        "    str_=str_+r\" \\\\ \"\n",
        "    if j<10:\n",
        "        print(str_)\n",
        "    else:\n",
        "        print('************************************************')\n",
        "        print(str_)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a4f3d1b35c0a260c3e5deb1d41278f4e1834e807",
        "id": "_JlzgL8OZHso"
      },
      "source": [
        "#  ========================================================\n",
        "# Third experiment\n",
        "##  Load data of  Connectionist Bench (Sonar)\n",
        "\n",
        "This is a small dataset that you can <a href=\"https://www.kaggle.com/adx891/sonar-data-set\">download from the kaggle </a>.\n",
        "\n",
        "\n",
        "### About this file:\n",
        "\n",
        "Connectionist Bench (Sonar, Mines vs. Rocks) Data Set Download: Data Folder, Data Set Description\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4994aeadde7ce8549562411b2762ec43746beecd",
        "id": "JK6MB5B6ZHsp"
      },
      "source": [
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/sonar.all-data.csv', header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "\n",
        "#Number of Rows and columns\n",
        "Number_rows,Input_size=dataset.shape\n",
        "\n",
        "X = dataset[:,0:Input_size-1].astype(float)\n",
        "Y = dataset[:,Input_size-1]\n",
        "X=normalize(X)\n",
        "X=rescale_range(X)\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "sonar=train_fun(X,Y,Input_size-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ePxKb_UZHsp"
      },
      "source": [
        "\n",
        "for j in range(11):\n",
        "    str_='0'\n",
        "    for i in range(6):\n",
        "        if i>0:\n",
        "            str_=str_+' & '+str(round(sonar[i][j], 2))\n",
        "        else:\n",
        "            str_=str(round(sonar[i][j], 2))\n",
        "    str_=str_+r\" \\\\ \"\n",
        "    if j<10:\n",
        "        print(str_)\n",
        "    else:\n",
        "        print('************************************************')\n",
        "        print(str_)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f829d47ace8dfe21048d5d12fea8a401237b518c",
        "collapsed": true,
        "id": "XOkqgdbGZHsp"
      },
      "source": [
        "#  ========================================================\n",
        "# Fourth  experiment    \n",
        "##  Load data of  Pima Indians Diabetes Database\n",
        "\n",
        "\n",
        "\n",
        "This is a small dataset that you can <a href=\"https://www.kaggle.com/uciml/pima-indians-diabetes-database\">download from the kaggle </a>.\n",
        "\n",
        "\n",
        "### About this file:\n",
        "\n",
        "The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ccf19230a88cfcb598b90cb0bf06a8f4695005a8",
        "id": "CtmFc6KzZHsq"
      },
      "source": [
        "dataframe = read_csv('https://raw.githubusercontent.com/DeepLearningSaeid/New-Type-of-Deep-Learning/master/diabetes.csv', header=1)\n",
        "dataset = dataframe.values\n",
        "\n",
        "\n",
        "#Number of Rows and columns\n",
        "Number_rows,Input_size=dataset.shape\n",
        "\n",
        "X = dataset[:,0:Input_size-1].astype(float)\n",
        "Y = dataset[:,Input_size-1]\n",
        "X=normalize(X)\n",
        "X=rescale_range(X)\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "diabetes=train_fun(X,Y,Input_size-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avrB5UwwZHsq"
      },
      "source": [
        "\n",
        "for j in range(11):\n",
        "    str_='0'\n",
        "    for i in range(6):\n",
        "        if i>0:\n",
        "            str_=str_+' & '+str(round(diabetes[i][j], 2))\n",
        "        else:\n",
        "            str_=str(round(diabetes[i][j], 2))\n",
        "    str_=str_+r\" \\\\ \"\n",
        "    if j<10:\n",
        "        print(str_)\n",
        "    else:\n",
        "        print('************************************************')\n",
        "        print(str_)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpG5eaWgZHss"
      },
      "source": [
        "fig, axs = plt.subplots(2, 2)\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "axs[0, 0].boxplot(ionosphere, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[0, 0].set_title('ionosphere')\n",
        "axs[1, 0].boxplot(banknote, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[1, 0].set_title('banknote')\n",
        "axs[1, 1].boxplot(sonar, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[1, 1].set_title('sonar')\n",
        "axs[0, 1].boxplot(diabetes, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[0, 1].set_title('diabetes')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNfKJgCJZHss"
      },
      "source": [
        "fig, axs = plt.subplots(2, 2)\n",
        "fig.set_size_inches(20, 20)\n",
        "axs[0, 0].boxplot(ionosphere, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[0, 0].set_title('ionosphere')\n",
        "axs[1, 0].boxplot(banknote, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[1, 0].set_title('banknote')\n",
        "axs[1, 1].boxplot(sonar, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[1, 1].set_title('sonar')\n",
        "axs[0, 1].boxplot(diabetes, labels=[str(r) for r in list_fun], showmeans=True)\n",
        "axs[0, 1].set_title('diabetes')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWzsKyazZHst"
      },
      "source": [
        "histories=[]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fd5da88ef403a8b98cc7e1296a2a560a20918346",
        "id": "ckJO8s0XZHsu"
      },
      "source": [
        "# Fifth experiment\n",
        "## Load data of THE MNIST DATABASE of handwritten digits\n",
        "\n",
        "## About this file:\n",
        "The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFPqsAWoZHsu"
      },
      "source": [
        "def plot_(histories):    \n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "    size=1\n",
        "    in_1=histories[0]\n",
        "    in_2=histories[1]\n",
        "    in_3=histories[2]\n",
        "    in_4=histories[3]\n",
        "    in_5=histories[4]\n",
        "    in_6=histories[5]\n",
        "    plt.figure(figsize=(15,15))\n",
        "    #['Ch_First','Ch_second','SWAG','Hermite','Legendre','Sin']\n",
        "###############################################\n",
        "    plt.subplot(321)\n",
        "    training_loss1 = in_1.history['accuracy']\n",
        "    test_loss1 = in_1.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('Ch_First')\n",
        "\n",
        "##################################################\n",
        "    plt.subplot(322)\n",
        "    training_loss1 = in_2.history['accuracy']\n",
        "    test_loss1 = in_2.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('Ch_second')\n",
        "##################################################\n",
        "    plt.subplot(323)\n",
        "    training_loss1 = in_3.history['accuracy']\n",
        "    test_loss1 = in_3.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('SWAG')    \n",
        "##################################################\n",
        "    plt.subplot(324)\n",
        "    training_loss1 = in_4.history['accuracy']\n",
        "    test_loss1 = in_4.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('Hermite') \n",
        "##################################################\n",
        "    plt.subplot(325)\n",
        "    training_loss1 = in_5.history['accuracy']\n",
        "    test_loss1 = in_5.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('Legendre') \n",
        "##################################################\n",
        "    plt.subplot(326)\n",
        "    training_loss1 = in_6.history['accuracy']\n",
        "    test_loss1 = in_6.history['val_accuracy']\n",
        "    epoch_count = range(1, len(training_loss1) + 1)\n",
        "    plt.plot(epoch_count, training_loss1, 'r--')\n",
        "    plt.plot(epoch_count, test_loss1, 'b-')\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.ylim(0, size)\n",
        "    plt.title('Sin(mx)') \n",
        "    \n",
        "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.0010, right=0.95, hspace=0.3,\n",
        "            wspace=0.18)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0H3o52jAVkpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JE01V2kZHsu"
      },
      "source": [
        "df_XY_train = pd.read_csv('https://raw.githubusercontent.com/sbussmann/kaggle-mnist/master/Data/train.csv')\n",
        "df_X_test  = pd.read_csv('https://raw.githubusercontent.com/sbussmann/kaggle-mnist/master/Data/test.csv')\n",
        "\n",
        "Y_train = df_XY_train['label'].values\n",
        "X_train = df_XY_train.drop('label', axis=1).values\n",
        "X_test  = df_X_test.values\n",
        "\n",
        "# Normalize the data\n",
        "\n",
        "# Normalize the data\n",
        "X_train = X_train +1\n",
        "X_test = X_test +1\n",
        "X_train = X_train / 3.0\n",
        "X_test = X_test / 3.0\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (784, 1) #tensorflow channels_last\n",
        "num_classes = 10\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0],img_rows*img_cols).astype('float32')/255\n",
        "Y_train = to_categorical(Y_train, num_classes)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJqLmPOZZHsu"
      },
      "source": [
        "# create model\n",
        "epochs=3\n",
        "batch_size=128\n",
        "\n",
        "for i in list_fun:\n",
        "    model_7=locals()[i](784,784,10)\n",
        "    model_7.summary()\n",
        "    history=model_7.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs,validation_data = (X_val, Y_val))\n",
        "    histories.append(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ce8d321b7d6a915e260f0ff3a31e820b7ed34fa3",
        "id": "CvA3LVU8ZHsu"
      },
      "source": [
        "plot_(histories)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}